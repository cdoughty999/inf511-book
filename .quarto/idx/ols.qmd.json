{"title":"Ordinary Least Squares","markdown":{"headingText":"Ordinary Least Squares","headingAttr":{"id":"sec-ols","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Lecture material\n\nPlease download and print the lecture materials from [Bblearn](https://bblearn.nau.edu/){target=\"_blank\"} or the [GitHub repo](https://github.com/joseph-mihaljevic/inf511-book/tree/main/lecture-material){target=\"_blank\"}. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section.\n\n## In-class Code {.unnumbered}\n\nRemember that our goal is to estimate the linear relationship between data observations of response variable, $y$, and its measured covariate, $x$, following: $Y = XB + \\epsilon$, where $\\epsilon \\sim N(0, \\sigma^2 I).$ Our coefficients to estimate are therefore $\\hat{B}$, which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), $\\hat{\\sigma}$. To estimate the coefficients, we are attempting to minimize the residual sum of squares, $|| \\epsilon || ^ 2$. See [Footnotes @sec-crossprod] for more information regarding this notation.\n\n## Generate the data\n\nWe'll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we'll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\n```{r}\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon\n```\n\n## Plot the relationship\n\n```{r}\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)\n```\n\n## Estimate the coefficients using R's `lm()` function {#sec-lm-output}\n\n```{r}\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n```\n\n## Estimate the coefficients manually\n\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, $Y$. Remember that we estimate the coefficient vector, $\\hat{B}$ from: $$X^TX \\hat{B} = X^T Y$$ $$\\hat{B} = (X^TX)^{-1} X^T Y$$ These equations include the multiplicative inverse matrix, $(X^TX)^{-1}$. See the [Footnotes @sec-solve] for more information about inverse matrices and the `solve()` function.\n\n```{r}\n\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?\n```\n\n## Plot the *estimated* relationships {#sec-est-plot}\n\n```{r}\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)\n```\n\n## Why are the $\\hat{B}$ different from true $B$?\n\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), $|| \\epsilon ||^2$.\n\n```{r}\n# True sum of squares:\nsum(epsilon)^2\n\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals)^2\n\n## From manual OLS\nsum( (y - xmat %*% bhat_solve) )^2\n\n```\nYou can see that the OLS strategy effectively minimized the SSE to zero.\n\n\n## Understanding Uncertainty in $\\hat{B}$\n\nWhile the OLS analysis estimates the regression coefficients, $\\hat{B}$, from the observed data $Y$, our estimates of these coefficients have error (i.e., uncertainty), such that the estimates are only as good as the data. Specifically, if we have fewer data points (i.e., $n$ is low), we have less certainty in $\\hat{B}$. In lecture, we showed, that:\n$$\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right), $$\nand we know that $\\hat{\\sigma}^2$ depends on sample size $n$, following:\n$$\\hat{\\sigma}^2 \\quad = \\quad \\frac{1}{n-p} (Y_{obs} - Y_{pred})^T (Y_{obs} - Y_{pred}) \\quad = \\quad \\frac{1}{n-p} \\hat{\\epsilon}^T \\hat{\\epsilon}$$\n\nUsing these equations, we showed then that $SE(\\beta_i) = \\sqrt{diag\\left( (X^TX)^{-1} \\right)_i \\hat{\\sigma}^2}$. Let's calculate this manually and compare to the output of the `lm()` function.\n\n```{r}\n# Extract the model summary, which has useful components\nlm_out_summary = summary(lm_out)\n# Extract the estimated residual standard deviation, sigma\nest_sigma = lm_out_summary$sigma\nest_sigma\n\n# We already calculated (X^T X)^{-1} as inv_xtx\nbeta_cov_mat = inv_xtx * est_sigma^2\nbeta_cov_mat\nse_beta = sqrt(diag(beta_cov_mat))\nse_beta\n```\nCompare these values to the output of the `summary()` of @sec-lm-output in the column labelled `Std. Error`.  \n\n## Confidence Intervals for $\\hat{B}$ {#sec-conf-beta}\n\nTo calculate confidence intervals for $\\hat{B}$, we first must understand the $t$ (a.k.a. Student's $t$) probability distribution. This distribution represents the case when we are estimating the mean of a normally distributed variable and either the sample size is small or the variable's standard deviation is unknown. Essentially, the $t$ distribution increases the uncertainty (i.e., variance) in cases of low sample size (i.e., small $n$). With low sample size (and/or high number of parameters), the degrees of freedom of the $t$-distribution, $\\nu$ is low, whereas with high sample size, $\\nu$ is large. As $\\nu$ approaches infinity, the $t$-distribution approximates the standard normal distribution (i.e., $N(\\mu, \\sigma)|\\mu=0,\\sigma=1$). \n```{r}\n#| echo: false\n\n# Generate x sequence\nn_seq = 1000\nx_seq = seq(-10, 10, length.out = n_seq)\n\nt_pdf_nu2 = dt(x_seq, df = 2)\nt_pdf_nu100 = dt(x_seq, df = 100)\nnorm_pdf = dnorm(x_seq, mean = 0, sd = 1)\n\n# compare to t-100\nplot(NA,NA, xlab = \"x\", ylab = \"P(x)\", \n     xlim = range(x_seq), ylim = c(0, 0.5))\nlines(norm_pdf~x_seq, col = \"red\", lwd = 2)\nlines(t_pdf_nu2~x_seq, col = \"blue\", lwd = 2)\nlines(t_pdf_nu100~x_seq, col = \"blue\", lwd = 1.2, lty = 2)\nlegend(x = 2.5, y = 0.4,\n       legend = c(\"Standard Normal\", expression(italic(t)[2]), expression(italic(t)[100])),\n       lty = c(1,1,2), col = c(\"red\", \"blue\", \"blue\"), bty = \"n\")\n```\nIt is the case for $\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right)$ that we do not know the mean ($B$), and we are estimating the variance, $\\hat{\\sigma}^2$. Specifically, we are estimating the true mean vector, $B$, as $\\hat{B}$, and we are estimating the variance of the residuals as $\\hat{\\sigma}^2$. We can therefore re-write the uncertainty in $\\hat{B}$ as a multivariate $t$ distribution: \n$$(\\hat{B} - B) \\sim t_{\\nu} \\left( 0, \\Sigma \\right),$$\nwhere the means are zero, $\\nu$ is the degrees of freedom (i.e., $n-p$), and $\\Sigma = (X^TX)^{-1} \\hat{\\sigma}^2$. $(\\hat{B} - B)$ represents the deviation of the estimated coefficients from the true coefficients, which is why the distribution is centered around zero. It is perhaps easier to separate the individual estimated coefficients, $\\beta_i$, into their separate $t$-distributions, estimated as the off-diagonals of $(X^TX)^{-1} \\hat{\\sigma}^2$:\n$$(\\hat{\\beta}_i - \\beta_i) \\sim t_{\\nu} \\hat{\\sigma},$$\nwhich shows that the $t$-distribution is scaled by the estimated $SE(\\hat{\\beta}_i)$. As shown in Dr. Barber's materials, using this information, we can derive the confidence interval (at the $\\alpha$ confidence level) calculation for $\\hat{\\beta}_i$ as: \n$$ \\hat{\\beta}_i \\pm t \\left(\\frac{1-\\alpha}{2}, \\nu \\right) SE(\\hat{\\beta}_i),$$\nwhere the $t()$ notation represents the *critical value* of the $t$-distribution with $\\nu$ degrees of freedom, $t_{crit}$, for which $P(z \\le t_{crit}) = \\frac{1-\\alpha}{2}$, and $z$ is a continuous, random variable.\n\n::: {.callout-note}\n## Covariance of $\\hat{\\beta}_i$\n\nAlthough it is convenient and easier to digest the confidence interval of individual $\\hat{\\beta}_i$, we must realize that the estimates of the $\\beta_i$ can covary (i.e., have non-zero covariance), which is quantified in the variance-covariance matrix of $\\hat{B}$, $(X^TX)^{-1} \\hat{\\sigma}^2$. We will show how this is important below.\n:::\n\nLet's manually calculate the 95% confidence intervals in $\\hat{B}$ and compare to R's internal function `confint()`.\n```{r}\n# Extract the degrees of freedom from the model (\\nu)\n# which can also be calculated as n - p\nt_df = lm_out$df.residual\n\n# Calculate t critical for alpha = 0.05\n# This will give us the 95% conf interval (CI)\nt_crit = qt(1-(0.05/2), df = t_df)\n\n# Calculate the upper and lower CI for both betas\nci_int = lm_coef[1] + c(-1,1)*t_crit*se_beta[1]\nci_slope = lm_coef[2] + c(-1,1)*t_crit*se_beta[2]\n\n# Construct a table of values\nci_mat = \n    rbind(c(lm_coef[1], ci_int),\n          c(lm_coef[2], ci_slope))\ncolnames(ci_mat) = c(\"coef\", \"lowCI\", \"highCI\")\nrownames(ci_mat) = c(\"intercept\", \"slope\")\nci_mat\n\n# Compare these manual calculations to built-in\n# function confint(), which by default extracts the \n# 95% CI for a lm() model's coefficients\nconfint(lm_out)\n```\n## Propagate uncertainty in $\\hat{B}$ for predictions of $Y$\n\nThere are several ways to calculate and visualize our uncertainty in model predictions of observed data $Y$ and unobserved data of the dependent variable (i.e., interpolation). The colored lines drawn on the figure in @sec-est-plot represent the expected values of $Y$ based on the OLS analysis' estimate of $\\hat{B}$, but this line does not include uncertainty in these coefficient values. \n\n### Multivariate $t$-distribution method\nFirst, we will calculate uncertainty by sampling from the multivariate $t$ distribution that represents error in regression coefficients, $\\hat{B}$. \n\n```{r}\n# We will \"bootstrap\" 1000 samples of intercept and slope\nset.seed(3)\nn_samp = 500\n\n# Draw from the multivariate t \n# which represents (\\hat{B} - B)\ntest_mat_deviates = \n  mnormt::rmt(n_samp, mean = c(0,0), S = beta_cov_mat, df = t_df)\n\n# Now calculate the realized intercept and slope\n# using the t-distributed deviates\ntest_mat_t = cbind(\n  lm_coef[1] + c(test_mat_deviates[,1]),\n  lm_coef[2] + c(test_mat_deviates[,2])\n)\n\n# Calculate the 95% quantiles and compare to the \n# calculated 95% confidence intervals from above\napply(test_mat_t, \n      MARGIN = 2, # applies function (FUN) to columns (dim 2)\n      FUN = quantile, probs = c(0.025, 0.5, 0.975))\n\n# Compare\nci_mat\n\n# Plot the relationship between intercept and slope\n# Notice the covariance\nplot(test_mat_t, xlab = \"Intercept\", ylab = \"Slope\")\n```\n\nNext, for each pair of intercept and slope randomly drawn above, we will calculate the expected values of $Y$ across the range of covariate $x$. We will then summarize the 95% quantile of expected $Y$ at each value of $x$ in this interpolation. To do this, we need a function to calculate the expected value of $Y$. This function will have the intercept and slope as inputs and will output the expected values of $Y$ across a range of $x$. Then, we will `apply()` this function using all of the values of intercept and slope, in a vectorized and therefore very efficient manner, rather than using any `for` loops. \n\n```{r}\n# Create a matrix that holds the values of x\n# over which we want to interpolate the expected\n# values of Y\nx_fake_mat = \n  cbind(\n    rep(1, times = 100),\n    seq(0,max(x1),length.out = 100)\n  )\n\n# Create a function that will calculate the expected values\ny_hat_fun = function(x, x_mat){\n  x_mat %*% x\n}\n\n# Apply this function to all intercepts and slopes that\n# we drew from the multivariate t\ny_pred_mt = apply(test_mat_t, 1, y_hat_fun, x_mat=x_fake_mat)\n\n# Summarize the 95% quantile of the expected value of Y\n# at each value of x \ny_pred_mt_summary = apply(y_pred_mt, 1, quantile, probs = c(0.025, 0.975))\nstr(y_pred_mt_summary)\n```\n\n### `predict()` function method\n\nR has a built-in function `predict()` (see specific variant `predict.lm()`) which calculates expected values of the dependent variable from a linear regression model estimated using the function `lm()`.\n\n```{r}\n# Note that 'newdata' must be a data frame that includes the ranges\n# of each covariate in the regression model for which you want \n# to generate interpolated or predicted values of the dependent variable\n\n# Here we are calculated the expected values as well as the \n# 95% confidence intervals for those expected values\ny_predict = predict(lm_out,\n                 newdata = data.frame(x1 = c(x_fake_mat[,2])),\n                 interval = \"confidence\", level = 0.95)\nstr(y_predict)\n```\n\n### Compare the two methods \n\nLet's visualize the output of the two methods to compare. \n\n```{r}\n# plot\nplot(x=NA,y=NA,xlab = \"x\", ylab = \"y\",\n     xlim = c(0,max(x1)), ylim = c(-5, 25), pch = 19)\n# Plot the expected values of Y for each pair of int/slope \nfor(i in 1:n_samp){\n  lines(y_pred_mt[,i] ~ x_fake_mat[,2],\n        # Reduce the opacity of each line\n        col = scales::alpha(\"black\", alpha = 0.1), lwd = 2)\n}\n# Add the data points\npoints(y ~ x1, col = 'orange', pch = 19, cex = 2)\n# Add the expected values of Y from \\hat{B}\nabline(coef = lm_coef, col = \"orange\", lwd = 3)\n# Add the conf int of expected Y using multivariate t\nlines(y_pred_mt_summary[1,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\nlines(y_pred_mt_summary[2,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\n# Add the conf int of expected Y using predict() function\nlines(y_predict[,\"lwr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\nlines(y_predict[,\"upr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\n```\nThere is yet a third option to calculate the uncertainty in predicted (i.e., interpolated or extrapolated) values of \n$Y$, which is to derive an exact calculation of the confidence interval using the $t$ distribution, similar to that shown in @sec-conf-beta. See Ch4.1 of Dr. Barber's book for this derivation. \n\n\n## Footnotes\n\n### Euclidean norm & cross product {#sec-crossprod}\n\nWe often see the syntax, $|| a ||$, which is the Euclidean norm of the $n$-sized vector $a$: $$|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,$$ so that when we see $|| a ||^2$, this results in the sum of squares of vector $a$, $\\sum_{i=1}^{n} a_i^2$.\n\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, $\\epsilon_i$, are in vector, $\\epsilon$. The sum of squares of vector $\\epsilon$ is therefore $|| \\epsilon ||^2$. Algebraically, we can find this value as the cross product of $\\epsilon$, which is $\\epsilon^{T}\\epsilon$. Let's do a coded example with vector $x$.\n\n```{r}\n\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n\n# Evaluated as cross-product\nt(x) %*% x\n## Or with crossprod()\ncrossprod(x,x)\n\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n\n```\n\n### `solve()` and Inverse of matrix {#sec-solve}\n\nSuppose we have matrices $A$, $X$, and $B$, and the following expression is true: $$AX=B.$$\n\nThen, suppose $X$ is unknown, such that we want to find the solution for $X$, when we rearrange: $$X = A^{-1} B,$$ where $A^{-1}$ is the multiplicative inverse of matrix $A$. To figure this out computationally, we can use the `solve()` function in R, as long as $A$ is a square matrix and has an inverse.\n\n```{r}\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n```\n\nWe can see, then, that `solve()` is internally evaluating $A^{-1}$. Remember that $A^{-1}$ is not trivial to calculate, as it is the matrix that must satisfy: $AA^{-1} = I$, where $I$ is an identity matrix. In fact, `solve(A)` returns the inverse of $A$, if it exists.\n\n```{r}\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\nX\n```"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":{"toggle":true},"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"printing","output-file":"ols.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","crossref":{"appendix-delim":":"},"bibliography":["references.bib"],"theme":"cosmo","smooth-scroll":true,"code-block-bg":true},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":true,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"toc-depth":2,"output-file":"ols.pdf"},"language":{},"metadata":{"block-headings":true,"crossref":{"appendix-delim":":"},"bibliography":["references.bib"],"documentclass":"scrreprt","lof":true,"lot":true,"fig-cap-location":"bottom","tbl-cap-location":"bottom","geometry":["margin=1in","heightrounded"],"hyperrefoptions":["linktoc=all"],"colorlinks":true,"linkcolor":"blue","biblio-style":"apalike","code-block-bg":true},"extensions":{"book":{}}}}}