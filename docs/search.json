[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INF511: Modern Regression I",
    "section": "",
    "text": "Preface\nWelcome to INF511: Modern Regression I. In this course, we will do a deep dive into three fundamental methods for estimating the linear relationships between random variables (i.e., linear regression analysis): ordinary least squares (Chapter 4), maximum likelihood (Chapter 5), and Bayesian inference (Chapter 6). This online book serves as a living document of resources for our class. The chapters provide links to lecture materials, which should be downloaded and printed prior to class, as well as links to recorded lectures. Each chapter also has material that is supplemental to lecture, with coded examples. We will often refer to these examples during class time, and they will be helpful for solving problem set and homework assignments. Problem sets will have dedicated in-class time, whereas homework assignments will be conducted entirely outside of class time.\nPlease refer to the Syllabus (Appendix A) for the course schedule, learning objectives, grading structure, course policies, etc."
  },
  {
    "objectID": "intro.html#load-a-package",
    "href": "intro.html#load-a-package",
    "title": "1  Introduction",
    "section": "1.1 Load a package",
    "text": "1.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "intro.html#load-a-data-set",
    "href": "intro.html#load-a-data-set",
    "title": "1  Introduction",
    "section": "1.2 Load a data set",
    "text": "1.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "intro.html#manipulate-and-visualize-the-data",
    "href": "intro.html#manipulate-and-visualize-the-data",
    "title": "1  Introduction",
    "section": "1.3 Manipulate and visualize the data",
    "text": "1.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "intro.html#is-there-a-linear-association",
    "href": "intro.html#is-there-a-linear-association",
    "title": "1  Introduction",
    "section": "1.4 Is there a linear association?",
    "text": "1.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from an well-informed standpoint.\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "app-Rintro.html#load-a-package",
    "href": "app-Rintro.html#load-a-package",
    "title": "Appendix A: Introduction to R",
    "section": "A.1 Load a package",
    "text": "A.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "app-Rintro.html#load-a-data-set",
    "href": "app-Rintro.html#load-a-data-set",
    "title": "Appendix A: Introduction to R",
    "section": "A.2 Load a data set",
    "text": "A.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "app-Rintro.html#manipulate-and-visualize-the-data",
    "href": "app-Rintro.html#manipulate-and-visualize-the-data",
    "title": "Appendix A: Introduction to R",
    "section": "A.3 Manipulate and visualize the data",
    "text": "A.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "app-Rintro.html#is-there-a-linear-association",
    "href": "app-Rintro.html#is-there-a-linear-association",
    "title": "Appendix A: Introduction to R",
    "section": "A.4 Is there a linear association?",
    "text": "A.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from an well-informed standpoint.\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "Rintro.html#load-a-package",
    "href": "Rintro.html#load-a-package",
    "title": "2  Introduction to R",
    "section": "2.1 Load a package",
    "text": "2.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "Rintro.html#load-a-data-set",
    "href": "Rintro.html#load-a-data-set",
    "title": "2  Introduction to R",
    "section": "2.2 Load a data set",
    "text": "2.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "Rintro.html#manipulate-and-visualize-the-data",
    "href": "Rintro.html#manipulate-and-visualize-the-data",
    "title": "2  Introduction to R",
    "section": "2.3 Manipulate and visualize the data",
    "text": "2.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "Rintro.html#is-there-a-linear-association",
    "href": "Rintro.html#is-there-a-linear-association",
    "title": "2  Introduction to R",
    "section": "2.4 Is there a linear association?",
    "text": "2.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\n\n\n\n\n\n\nTip\n\n\n\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from a well-informed standpoint.\n\n\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "prob.html#gaussian-normal-distribution",
    "href": "prob.html#gaussian-normal-distribution",
    "title": "3  Probability distributions",
    "section": "3.2 Gaussian (Normal) distribution",
    "text": "3.2 Gaussian (Normal) distribution\nAs we learned in lecture, the normal distribution is defined by two parameters, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). Here, we will use the normal distribution to demonstrate some of R’s functions to describe probability distributions and to draw random numbers from probability distributions. Let’s assume that random variable \\(x\\) follows a normal distribution, \\(x_i \\sim N(\\mu, \\sigma)\\).\n\n# Define the parameters\nmu = 10\nsigma = 2.5\n\n# Visualize the probability density function (pdf)\nx_vals = seq(0, 50, by = 0.1)\nnorm_pdf = dnorm(x_vals, mean = mu, sd = sigma)\n\n# Let's use some of the other R functions to describe the distribution\n\n## What is the probability density of specific values?\n## mean\np_mu = dnorm(mu, mean = mu, sd = sigma)\n## The next two values will describe the 95% probability density bounds\n## (Low) 2.5% cut off \nx_low95 = qnorm(0.025, mean = mu, sd = sigma)\np_low95 = dnorm(x_low95, mean = mu, sd = sigma)\n## (High) 97.5% cut off\nx_high95 = qnorm(0.975, mean = mu, sd = sigma)\np_high95 = dnorm(x_high95, mean = mu, sd = sigma)\n\n# So, what is the P(x <= x_high95)??\npnorm(x_high95, mean = mu, sd = sigma)\n\n[1] 0.975\n\n\n\n## Plot the pdf with segments\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"x\", ylab = expression(\"P(x |\"~mu~\",\"~sigma~\")\"))\nlines(norm_pdf ~ x_vals)\nsegments(x0 = c(x_low95, mu, x_high95), x1 = c(x_low95, mu, x_high95),\n         y0 = rep(0, times = 3), y1 = c(p_low95, p_mu, p_high95))\n# Now, let's draw random samples from this normal distribution\nn_rand = 1000\nx_rand = rnorm(n_rand, mean = mu, sd = sigma)\n\n# Plot a histogram and overlay the approximate expectations\n## The line below assumes you draw 'n_rand' samples\nhist(x_rand, breaks = 20, main = \"\")\nlines(norm_pdf*n_rand ~ x_vals)"
  },
  {
    "objectID": "prob.html#multivariate-normal-distribution",
    "href": "prob.html#multivariate-normal-distribution",
    "title": "3  Probability distributions",
    "section": "3.3 Multivariate normal distribution",
    "text": "3.3 Multivariate normal distribution\n\n3.3.1 Relation to residuals, \\(\\epsilon\\)\nRecall our linear model in matrix notation: \\(Y = XB + \\epsilon\\). We use the multivariate normal distribution to describe the probability density of the residuals, \\(\\epsilon\\). Recall that each individual residual, \\(\\epsilon_i\\) follows a normal distribution with mean zero and standard deviation equal to the residual error, \\(\\sigma\\): \\(\\epsilon_i \\sim N(0, \\sigma)\\). Also recall that the linear regression analysis assumes that \\(\\epsilon_i\\) are I.I.D. (independent and identically distributed). \\(\\epsilon_i \\sim N(0, \\sigma)\\) implies the identical distribution (i.e., each residual follows the same normal distribution). The “independent” part means that the residual values are not correlated in any way, meaning that they do not covariance is zero. Thus, we can use vector notation to say that the vector \\(\\epsilon\\) follows a multivariate normal distribution with all means equal to zero and covariance matrix \\(\\Sigma = \\sigma^2 I\\), where \\(I\\) is a square identity matrix: \\(\\epsilon \\sim N(0, \\sigma^2 I)\\). More about covariance and covariance matrices is available below (Footnotes 3.5.1).\nThe multivariate normal probability distribution is hard to visualize, because it is in multiple dimensions. But we can use similar R functions to understand the distribution. These functions are not in the base installation of R, so we need another package, MASS. We’ll also need the Matrix package later.\n\n# Install packages if you don't already have them, e.g., \n# install.packages(\"MASS\", dependencies = TRUE)\nlibrary(MASS)\nlibrary(Matrix)\n\n# Define mean and st.dev.\nmu_epsilon = 0\nsigma_epsilon = 2.0\n\n# sample size\nn_resid = 1000\n\n# we need a vector of means\nmu_vec = rep(0, n_resid)\n\n# we need an identity matrix\nI_mat = matrix(0, nrow = n_resid, ncol = n_resid)\n## specify the diagonal = 1\ndiag(I_mat) = 1\n\n# Draw randomly from the multivariate normal\nmvn_epsilon = MASS::mvrnorm(n = 1, \n                            mu = mu_vec,\n                            Sigma = sigma_epsilon^2*I_mat)\n# We can see that an entire array of size n_resid is drawn\nstr(mvn_epsilon)\n\n num [1:1000] 0.497 0.068 -1.496 2.692 0.756 ...\n\n# How does this compare to drawing them independently?\nnorm_epsilon = rnorm(n_resid, mean = mu_epsilon, sd = sigma_epsilon)\nc(mean(mvn_epsilon), mean(norm_epsilon))\n\n[1] -0.03601079 -0.03554803\n\nc(sd(mvn_epsilon), sd(norm_epsilon))\n\n[1] 1.972903 1.921486\n\n\n\n# Compare these two vectors visually:\nhist(mvn_epsilon)\nhist(norm_epsilon)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Multivariate normal distribution with non-independent variables\nLet’s explore the multivariate normal a bit more. Suppose we have three random variates \\(a\\), \\(b\\), and \\(c\\). Suppose further that \\(a\\) and \\(b\\) are positively correlated with each other, but \\(c\\) is not correlated with either other variate.\n\n# Establish means and variances of a, b, and c\nmu_vec = c(1.0, 2.2, 1.5)\nsd_vec = c(1.5, 0.5, 0.75)\n\n# Manually construct the covariance matrix:\ncov_mat_test = matrix(\n    data = c(0.0, 0.6, 0.0,\n             0.6, 0.0, 0.0,\n             0.0, 0.0, 0.0),\n    ncol = 3, nrow = 3,\n    byrow = TRUE\n)\ndiag(cov_mat_test) = sd_vec^2\n\n# Matrix must be positive definite (PD). \n# This gives closest PD\ncov_mat = Matrix::nearPD(cov_mat_test)$mat\n# Look if you want: str(cov_mat)\n\n# Draw some random vectors:\nabc_array = mvrnorm(n = 100, mu = mu_vec, Sigma = cov_mat)\n# Look at structure if you want; str(abc_array)\n\n# Visualize the relationships between a, b, and c:\ncolnames(abc_array) = letters[1:3]\npairs(abc_array)"
  },
  {
    "objectID": "prob.html#poisson-distribution",
    "href": "prob.html#poisson-distribution",
    "title": "3  Probability distributions",
    "section": "3.4 Poisson distribution",
    "text": "3.4 Poisson distribution\nThe normal and multivariate normal probability distributions have PDFs related to continuous random variables. In many cases our data are not continuous, but are instead discrete. The Poisson distribution represents the PMF (probability mass function) of count data and is described by a single parameter, \\(\\lambda\\), which is equal to the mean and variance of the distribution. In regression, we can use the Poisson distribution to analyze a generalized linear model between a discrete response variable (e.g., count data) and its covariates, but we will not deal with that in our class.\n\n# Define the parameter\nlambda = 8\n\n# Visualize the probability density function (pdf)\n## Remember this is a discrete distribution\nk_vals = c(0:20)\npois_pdf = dpois(k_vals, lambda = lambda)\n\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"k\", ylab = expression(\"P(k |\"~lambda~\")\"))\npoints(pois_pdf ~ k_vals)\nsegments(x0 = k_vals, x1 = k_vals,\n         y0 = 0, y1 = pois_pdf)\n## Compare to randomly drawn values:\nk_rand = rpois(n_rand, lambda = lambda)\nhist(k_rand, breaks = 25, main = \"\")\npoints(pois_pdf*n_rand ~ k_vals, pch = 19)\n# On your own, use the ppois() and qpois() functions to understand their inputs/outputs"
  },
  {
    "objectID": "prob.html#footnotes",
    "href": "prob.html#footnotes",
    "title": "3  Probability distributions",
    "section": "3.5 Footnotes",
    "text": "3.5 Footnotes\n\n3.5.1 Covariance matrix\nAs reminder, the variance of a random variable, \\(x\\), with sample size \\(n\\) is: \\[\\sigma^2_x = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(x_i - \\bar{x}) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})^2.\\] And \\(\\bar{x}\\) is the sample mean. Similarly, then, the covariance of samples from two random variables, \\(x\\) and \\(y\\), can be calculated as: \\[\\sigma(x,y) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y}).\\] The syntax for the covariance of a sample population with itself is, for example, \\(\\sigma(x, x)\\), which is simply equal to the variance \\(\\sigma_x^2\\). The covariance matrix for these two sample populations would be: \\[C = \\begin{bmatrix}\n\\sigma(x,x) & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma(y,y)\n\\end{bmatrix}.\\] This can be simplified using the variance notation: \\[C = \\begin{bmatrix}\n\\sigma^2_x & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma^2_y\n\\end{bmatrix}.\\]"
  },
  {
    "objectID": "ols.html#lecture-material",
    "href": "ols.html#lecture-material",
    "title": "4  Ordinary Least Squares",
    "section": "4.1 Lecture material",
    "text": "4.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "ols.html#in-class-code",
    "href": "ols.html#in-class-code",
    "title": "4  Ordinary Least Squares",
    "section": "In-class Code",
    "text": "In-class Code\nRemember that our goal is to estimate the linear relationship between data observations of response variable, \\(y\\), and its measured covariate, \\(x\\), following: \\(Y = XB + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2 I).\\) Our coefficients to estimate are therefore \\(\\hat{B}\\), which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), \\(\\hat{\\sigma}\\). To estimate the coefficients, we are attempting to minimize the residual sum of squares, \\(|| \\epsilon || ^ 2\\). See Footnotes 4.8.1 for more information regarding this notation."
  },
  {
    "objectID": "ols.html#footnotes",
    "href": "ols.html#footnotes",
    "title": "4  Ordinary Least Squares",
    "section": "4.8 Footnotes",
    "text": "4.8 Footnotes\n\n4.8.1 Euclidean norm & cross product\nWe often see the syntax, \\(|| a ||\\), which is the Euclidean norm of the \\(n\\)-sized vector \\(a\\): \\[|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,\\] so that when we see \\(|| a ||^2\\), this results in the sum of squares of vector \\(a\\), \\(\\sum_{i=1}^{n} a_i^2\\).\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, \\(\\epsilon_i\\), are in vector, \\(\\epsilon\\). The sum of squares of vector \\(\\epsilon\\) is therefore \\(|| \\epsilon ||^2\\). Algebraically, we can find this value as the cross product of \\(\\epsilon\\), which is \\(\\epsilon^{T}\\epsilon\\). Let’s do a coded example with vector \\(x\\).\n\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n\n[1] 30\n\n# Evaluated as cross-product\nt(x) %*% x\n\n     [,1]\n[1,]   30\n\n## Or with crossprod()\ncrossprod(x,x)\n\n     [,1]\n[1,]   30\n\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n\n[1] 30\n\n\n\n\n4.8.2 solve() and Inverse of matrix\nSuppose we have matrices \\(A\\), \\(X\\), and \\(B\\), and the following expression is true: \\[AX=B.\\]\nThen, suppose \\(X\\) is unknown, such that we want to find the solution for \\(X\\), when we rearrange: \\[X = A^{-1} B,\\] where \\(A^{-1}\\) is the multiplicative inverse of matrix \\(A\\). To figure this out computationally, we can use the solve() function in R, as long as \\(A\\) is a square matrix and has an inverse.\n\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\nWe can see, then, that solve() is internally evaluating \\(A^{-1}\\). Remember that \\(A^{-1}\\) is not trivial to calculate, as it is the matrix that must satisfy: \\(AA^{-1} = I\\), where \\(I\\) is an identity matrix. In fact, solve(A) returns the inverse of \\(A\\), if it exists.\n\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\n\n     [,1]\n[1,]    2\n[2,]    3\n\nX\n\n     [,1]\n[1,]    2\n[2,]    3"
  },
  {
    "objectID": "prob.html#lecture-material",
    "href": "prob.html#lecture-material",
    "title": "3  Probability distributions",
    "section": "3.1 Lecture material",
    "text": "3.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "INF511: Modern Regression I",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nThis website is published using Github Pages.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Appendix A: Syllabus",
    "section": "",
    "text": "The syllabus will live here."
  },
  {
    "objectID": "max-lik.html#lecture-material",
    "href": "max-lik.html#lecture-material",
    "title": "5  Maximum Likelihood",
    "section": "5.1 Lecture material",
    "text": "5.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "max-lik.html#in-class-code",
    "href": "max-lik.html#in-class-code",
    "title": "5  Maximum Likelihood",
    "section": "5.2 In-class Code",
    "text": "5.2 In-class Code"
  },
  {
    "objectID": "bayesian.html#lecture-material",
    "href": "bayesian.html#lecture-material",
    "title": "6  Bayesian inference",
    "section": "6.1 Lecture material",
    "text": "6.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "bayesian.html#in-class-code",
    "href": "bayesian.html#in-class-code",
    "title": "6  Bayesian inference",
    "section": "6.2 In-class Code",
    "text": "6.2 In-class Code\nTBA"
  },
  {
    "objectID": "ols.html#generate-the-data",
    "href": "ols.html#generate-the-data",
    "title": "4  Ordinary Least Squares",
    "section": "4.2 Generate the data",
    "text": "4.2 Generate the data\nWe’ll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we’ll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    1    5\n[4,]    1    1\n\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n\n     [,1]\n[1,]    8\n[2,]   10\n[3,]   14\n[4,]    6\n\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon"
  },
  {
    "objectID": "ols.html#plot-the-relationship",
    "href": "ols.html#plot-the-relationship",
    "title": "4  Ordinary Least Squares",
    "section": "4.3 Plot the relationship",
    "text": "4.3 Plot the relationship\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)"
  },
  {
    "objectID": "ols.html#estimate-the-coefficients-using-rs-lm-function",
    "href": "ols.html#estimate-the-coefficients-using-rs-lm-function",
    "title": "4  Ordinary Least Squares",
    "section": "4.4 Estimate the coefficients using R’s lm() function",
    "text": "4.4 Estimate the coefficients using R’s lm() function\n\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ 1 + x1)\n\nResiduals:\n     1      2      3      4 \n-1.029 -1.657  1.086  1.600 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   5.7714     2.0500   2.815    0.106\nx1            1.6286     0.6565   2.481    0.131\n\nResidual standard error: 1.942 on 2 degrees of freedom\nMultiple R-squared:  0.7547,    Adjusted R-squared:  0.6321 \nF-statistic: 6.153 on 1 and 2 DF,  p-value: 0.1313\n\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n\n(Intercept)          x1 \n   5.771429    1.628571"
  },
  {
    "objectID": "ols.html#estimate-the-coefficients-manually",
    "href": "ols.html#estimate-the-coefficients-manually",
    "title": "4  Ordinary Least Squares",
    "section": "4.5 Estimate the coefficients manually",
    "text": "4.5 Estimate the coefficients manually\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, \\(Y\\). Remember that we estimate the coefficient vector, \\(\\hat{B}\\) from: \\[X^TX \\hat{B} = X^T Y\\] \\[\\hat{B} = (X^TX)^{-1} X^T Y\\] These equations include the multiplicative inverse matrix, \\((X^TX)^{-1}\\). See the Footnotes 4.8.2 for more information about inverse matrices and the solve() function.\n\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?"
  },
  {
    "objectID": "ols.html#plot-the-estimated-relationships",
    "href": "ols.html#plot-the-estimated-relationships",
    "title": "4  Ordinary Least Squares",
    "section": "4.6 Plot the estimated relationships",
    "text": "4.6 Plot the estimated relationships\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)"
  },
  {
    "objectID": "ols.html#why-are-the-hatb-different-from-true-b",
    "href": "ols.html#why-are-the-hatb-different-from-true-b",
    "title": "4  Ordinary Least Squares",
    "section": "4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?",
    "text": "4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), \\(|| \\epsilon ||^2\\).\n\n# True sum of squares:\nsum(epsilon)^2\n\n[1] 9\n\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals)^2\n\n[1] 0\n\n## From manual OLS\nsum( (y - xmat %*% bhat_solve) )^2\n\n[1] 7.099748e-30\n\n\nYou can see that the OLS strategy effectively minimized the SSE to zero."
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "6  Bayesian inference",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "max-lik.html",
    "href": "max-lik.html",
    "title": "5  Maximum Likelihood",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "syllabus.html#tentative-course-schedule",
    "href": "syllabus.html#tentative-course-schedule",
    "title": "Appendix A: Syllabus",
    "section": "A.1 Tentative course schedule",
    "text": "A.1 Tentative course schedule"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "1  Software",
    "section": "",
    "text": "You will need to have all of the following free software downloaded and in working order on your laptop.\n\n\n\n\n\n\nPrior to first lecture\n\n\n\nYou must have the following on your laptops prior to the first lecture.\n\n\n\nLatest version of RStudio Desktop IDE\nCompatible version of R software environment\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\n\n\n\n\n\n\n\nPrior to Bayesian inference\n\n\n\nYou must have the following on your laptops prior to the sections on Bayesian inference.\n\n\n\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function."
  }
]