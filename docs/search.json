[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INF511: Modern Regression I",
    "section": "",
    "text": "Preface\nWelcome to INF511: Modern Regression I. In this course, we will do a deep dive into three fundamental methods for estimating the linear relationships between random variables (i.e., linear regression analysis): ordinary least squares (Chapter 4), maximum likelihood (Chapter 6), and Bayesian inference (Chapter 8). We will also explore null hypothesis testing (Chapter 5), and linear models with categorical covariates (i.e., ANOVA, Chapter 7). This online book serves as a living document of resources for our class. The chapters provide links to lecture materials, which should be downloaded and printed prior to class, as well as links to recorded lectures. Each chapter also has material that is supplemental to lecture, with coded examples. We will often refer to these examples during class time, and they will be helpful for solving problem set and homework assignments. Problem sets will have dedicated in-class time, whereas homework assignments will be conducted entirely outside of class time.\nPlease refer to the Syllabus (Appendix A) for the course schedule, learning objectives, grading structure, course policies, etc."
  },
  {
    "objectID": "intro.html#load-a-package",
    "href": "intro.html#load-a-package",
    "title": "1  Introduction",
    "section": "1.1 Load a package",
    "text": "1.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "intro.html#load-a-data-set",
    "href": "intro.html#load-a-data-set",
    "title": "1  Introduction",
    "section": "1.2 Load a data set",
    "text": "1.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "intro.html#manipulate-and-visualize-the-data",
    "href": "intro.html#manipulate-and-visualize-the-data",
    "title": "1  Introduction",
    "section": "1.3 Manipulate and visualize the data",
    "text": "1.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "intro.html#is-there-a-linear-association",
    "href": "intro.html#is-there-a-linear-association",
    "title": "1  Introduction",
    "section": "1.4 Is there a linear association?",
    "text": "1.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from an well-informed standpoint.\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "app-Rintro.html#load-a-package",
    "href": "app-Rintro.html#load-a-package",
    "title": "Appendix A: Introduction to R",
    "section": "A.1 Load a package",
    "text": "A.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "app-Rintro.html#load-a-data-set",
    "href": "app-Rintro.html#load-a-data-set",
    "title": "Appendix A: Introduction to R",
    "section": "A.2 Load a data set",
    "text": "A.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "app-Rintro.html#manipulate-and-visualize-the-data",
    "href": "app-Rintro.html#manipulate-and-visualize-the-data",
    "title": "Appendix A: Introduction to R",
    "section": "A.3 Manipulate and visualize the data",
    "text": "A.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "app-Rintro.html#is-there-a-linear-association",
    "href": "app-Rintro.html#is-there-a-linear-association",
    "title": "Appendix A: Introduction to R",
    "section": "A.4 Is there a linear association?",
    "text": "A.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from an well-informed standpoint.\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "Rintro.html#load-a-package",
    "href": "Rintro.html#load-a-package",
    "title": "2  Introduction to R",
    "section": "2.1 Load a package",
    "text": "2.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "Rintro.html#load-a-data-set",
    "href": "Rintro.html#load-a-data-set",
    "title": "2  Introduction to R",
    "section": "2.2 Load a data set",
    "text": "2.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "Rintro.html#manipulate-and-visualize-the-data",
    "href": "Rintro.html#manipulate-and-visualize-the-data",
    "title": "2  Introduction to R",
    "section": "2.3 Manipulate and visualize the data",
    "text": "2.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "Rintro.html#is-there-a-linear-association",
    "href": "Rintro.html#is-there-a-linear-association",
    "title": "2  Introduction to R",
    "section": "2.4 Is there a linear association?",
    "text": "2.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\n\n\n\n\n\n\nTip\n\n\n\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from a well-informed standpoint.\n\n\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "prob.html#gaussian-normal-distribution",
    "href": "prob.html#gaussian-normal-distribution",
    "title": "3  Probability distributions",
    "section": "3.2 Gaussian (Normal) distribution",
    "text": "3.2 Gaussian (Normal) distribution\nAs we learned in lecture, the normal distribution is defined by two parameters, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). Here, we will use the normal distribution to demonstrate some of R’s functions to describe probability distributions and to draw random numbers from probability distributions. Let’s assume that random variable \\(x\\) follows a normal distribution, \\(x_i \\sim N(\\mu, \\sigma)\\).\n\n# Define the parameters\nmu = 10\nsigma = 2.5\n\n# Visualize the probability density function (pdf)\nx_vals = seq(0, 50, by = 0.1)\nnorm_pdf = dnorm(x_vals, mean = mu, sd = sigma)\n\n# Let's use some of the other R functions to describe the distribution\n\n## What is the probability density of specific values?\n## mean\np_mu = dnorm(mu, mean = mu, sd = sigma)\n## The next two values will describe the 95% probability density bounds\n## (Low) 2.5% cut off \nx_low95 = qnorm(0.025, mean = mu, sd = sigma)\np_low95 = dnorm(x_low95, mean = mu, sd = sigma)\n## (High) 97.5% cut off\nx_high95 = qnorm(0.975, mean = mu, sd = sigma)\np_high95 = dnorm(x_high95, mean = mu, sd = sigma)\n\n# So, what is the P(x <= x_high95)??\npnorm(x_high95, mean = mu, sd = sigma)\n\n[1] 0.975\n\n\n\n## Plot the pdf with segments\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"x\", ylab = expression(\"P(x |\"~mu~\",\"~sigma~\")\"))\nlines(norm_pdf ~ x_vals)\nsegments(x0 = c(x_low95, mu, x_high95), x1 = c(x_low95, mu, x_high95),\n         y0 = rep(0, times = 3), y1 = c(p_low95, p_mu, p_high95))\n# Now, let's draw random samples from this normal distribution\nn_rand = 1000\nx_rand = rnorm(n_rand, mean = mu, sd = sigma)\n\n# Plot a histogram and overlay the approximate expectations\n## The line below assumes you draw 'n_rand' samples\nhist(x_rand, breaks = 20, main = \"\")\nlines(norm_pdf*n_rand ~ x_vals)"
  },
  {
    "objectID": "prob.html#multivariate-normal-distribution",
    "href": "prob.html#multivariate-normal-distribution",
    "title": "3  Probability distributions",
    "section": "3.3 Multivariate normal distribution",
    "text": "3.3 Multivariate normal distribution\n\n3.3.1 Relation to residuals, \\(\\epsilon\\)\nRecall our linear model in matrix notation: \\(Y = XB + \\epsilon\\). We use the multivariate normal distribution to describe the probability density of the residuals, \\(\\epsilon\\). Recall that each individual residual, \\(\\epsilon_i\\) follows a normal distribution with mean zero and standard deviation equal to the residual error, \\(\\sigma\\): \\(\\epsilon_i \\sim N(0, \\sigma)\\). Also recall that the linear regression analysis assumes that \\(\\epsilon_i\\) are I.I.D. (independent and identically distributed). \\(\\epsilon_i \\sim N(0, \\sigma)\\) implies the identical distribution (i.e., each residual follows the same normal distribution). The “independent” part means that the residual values are not correlated in any way, meaning that they do not covariance is zero. Thus, we can use vector notation to say that the vector \\(\\epsilon\\) follows a multivariate normal distribution with all means equal to zero and covariance matrix \\(\\Sigma = \\sigma^2 I\\), where \\(I\\) is a square identity matrix: \\(\\epsilon \\sim N(0, \\sigma^2 I)\\). More about covariance and covariance matrices is available below (Footnotes 3.5.1).\nThe multivariate normal probability distribution is hard to visualize, because it is in multiple dimensions. But we can use similar R functions to understand the distribution. These functions are not in the base installation of R, so we need another package, MASS. We’ll also need the Matrix package later.\n\n# Install packages if you don't already have them, e.g., \n# install.packages(\"MASS\", dependencies = TRUE)\nlibrary(MASS)\nlibrary(Matrix)\n\n# Define mean and st.dev.\nmu_epsilon = 0\nsigma_epsilon = 2.0\n\n# sample size\nn_resid = 1000\n\n# we need a vector of means\nmu_vec = rep(0, n_resid)\n\n# we need an identity matrix\nI_mat = matrix(0, nrow = n_resid, ncol = n_resid)\n## specify the diagonal = 1\ndiag(I_mat) = 1\n\n# Draw randomly from the multivariate normal\nmvn_epsilon = MASS::mvrnorm(n = 1, \n                            mu = mu_vec,\n                            Sigma = sigma_epsilon^2*I_mat)\n# We can see that an entire array of size n_resid is drawn\nstr(mvn_epsilon)\n\n num [1:1000] 0.497 0.068 -1.496 2.692 0.756 ...\n\n# How does this compare to drawing them independently?\nnorm_epsilon = rnorm(n_resid, mean = mu_epsilon, sd = sigma_epsilon)\nc(mean(mvn_epsilon), mean(norm_epsilon))\n\n[1] -0.03601079 -0.03554803\n\nc(sd(mvn_epsilon), sd(norm_epsilon))\n\n[1] 1.972903 1.921486\n\n\n\n# Compare these two vectors visually:\nhist(mvn_epsilon)\nhist(norm_epsilon)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Multivariate normal distribution with non-independent variables\nLet’s explore the multivariate normal a bit more. Suppose we have three random variates \\(a\\), \\(b\\), and \\(c\\). Suppose further that \\(a\\) and \\(b\\) are positively correlated with each other, but \\(c\\) is not correlated with either other variate.\n\n# Establish means and variances of a, b, and c\nmu_vec = c(1.0, 2.2, 1.5)\nsd_vec = c(1.5, 0.5, 0.75)\n\n# Manually construct the covariance matrix:\ncov_mat_test = matrix(\n    data = c(0.0, 0.6, 0.0,\n             0.6, 0.0, 0.0,\n             0.0, 0.0, 0.0),\n    ncol = 3, nrow = 3,\n    byrow = TRUE\n)\ndiag(cov_mat_test) = sd_vec^2\n\n# Matrix must be positive definite (PD). \n# This gives closest PD\ncov_mat = Matrix::nearPD(cov_mat_test)$mat\n# Look if you want: str(cov_mat)\n\n# Draw some random vectors:\nabc_array = mvrnorm(n = 100, mu = mu_vec, Sigma = cov_mat)\n# Look at structure if you want; str(abc_array)\n\n# Visualize the relationships between a, b, and c:\ncolnames(abc_array) = letters[1:3]\npairs(abc_array)"
  },
  {
    "objectID": "prob.html#poisson-distribution",
    "href": "prob.html#poisson-distribution",
    "title": "3  Probability distributions",
    "section": "3.4 Poisson distribution",
    "text": "3.4 Poisson distribution\nThe normal and multivariate normal probability distributions have PDFs related to continuous random variables. In many cases our data are not continuous, but are instead discrete. The Poisson distribution represents the PMF (probability mass function) of count data and is described by a single parameter, \\(\\lambda\\), which is equal to the mean and variance of the distribution. In regression, we can use the Poisson distribution to analyze a generalized linear model between a discrete response variable (e.g., count data) and its covariates, but we will not deal with that in our class.\n\n# Define the parameter\nlambda = 8\n\n# Visualize the probability density function (pdf)\n## Remember this is a discrete distribution\nk_vals = c(0:20)\npois_pdf = dpois(k_vals, lambda = lambda)\n\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"k\", ylab = expression(\"P(k |\"~lambda~\")\"))\npoints(pois_pdf ~ k_vals)\nsegments(x0 = k_vals, x1 = k_vals,\n         y0 = 0, y1 = pois_pdf)\n## Compare to randomly drawn values:\nk_rand = rpois(n_rand, lambda = lambda)\nhist(k_rand, breaks = 25, main = \"\")\npoints(pois_pdf*n_rand ~ k_vals, pch = 19)\n# On your own, use the ppois() and qpois() functions to understand their inputs/outputs"
  },
  {
    "objectID": "prob.html#footnotes",
    "href": "prob.html#footnotes",
    "title": "3  Probability distributions",
    "section": "3.5 Footnotes",
    "text": "3.5 Footnotes\n\n3.5.1 Covariance matrix\nAs reminder, the variance of a random variable, \\(x\\), with sample size \\(n\\) is: \\[\\sigma^2_x = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(x_i - \\bar{x}) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})^2.\\] And \\(\\bar{x}\\) is the sample mean. Similarly, then, the covariance of samples from two random variables, \\(x\\) and \\(y\\), can be calculated as: \\[\\sigma(x,y) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y}).\\] The syntax for the covariance of a sample population with itself is, for example, \\(\\sigma(x, x)\\), which is simply equal to the variance \\(\\sigma_x^2\\). The covariance matrix for these two sample populations would be: \\[C = \\begin{bmatrix}\n\\sigma(x,x) & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma(y,y)\n\\end{bmatrix}.\\] This can be simplified using the variance notation: \\[C = \\begin{bmatrix}\n\\sigma^2_x & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma^2_y\n\\end{bmatrix}.\\]"
  },
  {
    "objectID": "ols.html#lecture-material",
    "href": "ols.html#lecture-material",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.1 Lecture material",
    "text": "4.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "ols.html#in-class-code",
    "href": "ols.html#in-class-code",
    "title": "4  Ordinary Least Squares",
    "section": "In-class Code",
    "text": "In-class Code\nRemember that our goal is to estimate the linear relationship between data observations of response variable, \\(y\\), and its measured covariate, \\(x\\), following: \\(Y = XB + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2 I).\\) Our coefficients to estimate are therefore \\(\\hat{B}\\), which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), \\(\\hat{\\sigma}\\). To estimate the coefficients, we are attempting to minimize the residual sum of squares, \\(|| \\epsilon || ^ 2\\). See Footnotes 4.11.1 for more information regarding this notation."
  },
  {
    "objectID": "ols.html#footnotes",
    "href": "ols.html#footnotes",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.11 Footnotes",
    "text": "4.11 Footnotes\n\n4.11.1 Euclidean norm & cross product\nWe often see the syntax, \\(|| a ||\\), which is the Euclidean norm of the \\(n\\)-sized vector \\(a\\): \\[|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,\\] so that when we see \\(|| a ||^2\\), this results in the sum of squares of vector \\(a\\), \\(\\sum_{i=1}^{n} a_i^2\\).\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, \\(\\epsilon_i\\), are in vector, \\(\\epsilon\\). The sum of squares of vector \\(\\epsilon\\) is therefore \\(|| \\epsilon ||^2\\). Algebraically, we can find this value as the cross product of \\(\\epsilon\\), which is \\(\\epsilon^{T}\\epsilon\\). Let’s do a coded example with vector \\(x\\).\n\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n\n[1] 30\n\n# Evaluated as cross-product\nt(x) %*% x\n\n     [,1]\n[1,]   30\n\n## Or with crossprod()\ncrossprod(x,x)\n\n     [,1]\n[1,]   30\n\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n\n[1] 30\n\n\n\n4.11.2 solve() and Inverse of matrix\nSuppose we have matrices \\(A\\), \\(X\\), and \\(B\\), and the following expression is true: \\[AX=B.\\]\nThen, suppose \\(X\\) is unknown, such that we want to find the solution for \\(X\\), when we rearrange: \\[X = A^{-1} B,\\] where \\(A^{-1}\\) is the multiplicative inverse of matrix \\(A\\). To figure this out computationally, we can use the solve() function in R, as long as \\(A\\) is a square matrix and has an inverse.\n\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\nWe can see, then, that solve() is internally evaluating \\(A^{-1}\\). Remember that \\(A^{-1}\\) is not trivial to calculate, as it is the matrix that must satisfy: \\(AA^{-1} = I\\), where \\(I\\) is an identity matrix. In fact, solve(A) returns the inverse of \\(A\\), if it exists.\n\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\n\n     [,1]\n[1,]    2\n[2,]    3\n\nX\n\n     [,1]\n[1,]    2\n[2,]    3"
  },
  {
    "objectID": "prob.html#lecture-material",
    "href": "prob.html#lecture-material",
    "title": "3  Probability distributions",
    "section": "3.1 Lecture material",
    "text": "3.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "INF511: Modern Regression I",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nThis website is published using Github Pages.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Appendix A: Syllabus",
    "section": "",
    "text": "The syllabus will live here."
  },
  {
    "objectID": "max-lik.html#lecture-material",
    "href": "max-lik.html#lecture-material",
    "title": "5  Maximum Likelihood",
    "section": "5.1 Lecture material",
    "text": "5.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "max-lik.html#in-class-code",
    "href": "max-lik.html#in-class-code",
    "title": "5  Maximum Likelihood",
    "section": "5.2 In-class Code",
    "text": "5.2 In-class Code"
  },
  {
    "objectID": "bayesian.html#lecture-material",
    "href": "bayesian.html#lecture-material",
    "title": "6  Bayesian inference",
    "section": "6.1 Lecture material",
    "text": "6.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "bayesian.html#in-class-code",
    "href": "bayesian.html#in-class-code",
    "title": "6  Bayesian inference",
    "section": "6.2 In-class Code",
    "text": "6.2 In-class Code\nTBA"
  },
  {
    "objectID": "ols.html#generate-the-data",
    "href": "ols.html#generate-the-data",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.2 Generate the data",
    "text": "4.2 Generate the data\nWe’ll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we’ll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    1    5\n[4,]    1    1\n\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n\n     [,1]\n[1,]    8\n[2,]   10\n[3,]   14\n[4,]    6\n\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon"
  },
  {
    "objectID": "ols.html#plot-the-relationship",
    "href": "ols.html#plot-the-relationship",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.3 Plot the relationship",
    "text": "4.3 Plot the relationship\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)"
  },
  {
    "objectID": "ols.html#estimate-the-coefficients-using-rs-lm-function",
    "href": "ols.html#estimate-the-coefficients-using-rs-lm-function",
    "title": "4  Ordinary Least Squares",
    "section": "4.4 Estimate the coefficients using R’s lm() function",
    "text": "4.4 Estimate the coefficients using R’s lm() function\n\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ 1 + x1)\n\nResiduals:\n     1      2      3      4 \n-1.029 -1.657  1.086  1.600 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   5.7714     2.0500   2.815    0.106\nx1            1.6286     0.6565   2.481    0.131\n\nResidual standard error: 1.942 on 2 degrees of freedom\nMultiple R-squared:  0.7547,    Adjusted R-squared:  0.6321 \nF-statistic: 6.153 on 1 and 2 DF,  p-value: 0.1313\n\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n\n(Intercept)          x1 \n   5.771429    1.628571"
  },
  {
    "objectID": "ols.html#estimate-the-coefficients-manually",
    "href": "ols.html#estimate-the-coefficients-manually",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.5 Estimate the coefficients manually",
    "text": "4.5 Estimate the coefficients manually\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, \\(Y\\). Remember that we estimate the coefficient vector, \\(\\hat{B}\\) from: \\[X^TX \\hat{B} = X^T Y\\] \\[\\hat{B} = (X^TX)^{-1} X^T Y\\] These equations include the multiplicative inverse matrix, \\((X^TX)^{-1}\\). See the Footnotes 4.11.2 for more information about inverse matrices and the solve() function.\n\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?"
  },
  {
    "objectID": "ols.html#plot-the-estimated-relationships",
    "href": "ols.html#plot-the-estimated-relationships",
    "title": "4  Ordinary Least Squares",
    "section": "4.6 Plot the estimated relationships",
    "text": "4.6 Plot the estimated relationships\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)"
  },
  {
    "objectID": "ols.html#why-are-the-hatb-different-from-true-b",
    "href": "ols.html#why-are-the-hatb-different-from-true-b",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?",
    "text": "4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), \\(|| \\epsilon ||^2\\).\n\n# True sum of squares:\nsum(epsilon)^2\n\n[1] 9\n\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals)^2\n\n[1] 0\n\n## From manual OLS\nsum( (y - xmat %*% bhat_solve) )^2\n\n[1] 7.099748e-30\n\n\nYou can see that the OLS strategy effectively minimized the SSE to zero."
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "8  Bayesian inference",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "max-lik.html",
    "href": "max-lik.html",
    "title": "6  Maximum Likelihood",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "syllabus.html#tentative-course-schedule",
    "href": "syllabus.html#tentative-course-schedule",
    "title": "Appendix A: Syllabus",
    "section": "\nA.1 Tentative course schedule",
    "text": "A.1 Tentative course schedule\n\n\n\n\n\n Week \n    Date \n    Topic \n    Reading_Due \n    Assign_Due \n    Quiz \n  \n\n\n Week 1 \n    Jan-16 \n    MLK Holiday - NO CLASS \n     \n     \n     \n  \n\n Jan-18 \n    Introduction \n    JM( \n     \n     \n  \n\n Week 2 \n    Jan-23 \n    Probability distributions \n    JB(pg1-20), JM(Ch1), FAR(Ch1) \n    PS-0 \n     \n  \n\n Jan-25 \n   \n     \n     \n     \n  \n\n Week 3 \n    Jan-30 \n    Least Squares \n    JB(pg20-36), JM(Ch \n    PS-1 \n     \n  \n\n Feb-1 \n   \n     \n     \n    Quiz 1 \n  \n\n Week 4 \n    Feb-6 \n   \n     \n    PS-2 \n     \n  \n\n Feb-8 \n   \n     \n     \n     \n  \n\n Week 5 \n    Feb-13 \n   \n     \n     \n     \n  \n\n Feb-15 \n    Hypothesis Testing \n     \n     \n     \n  \n\n Week 6 \n    Feb-20 \n   \n     \n     \n     \n  \n\n Feb-22 \n    Maximum Likelihood \n     \n     \n    Quiz 2 \n  \n\n Week 7 \n    Feb-27 \n   \n     \n     \n     \n  \n\n Mar-1 \n   \n     \n     \n     \n  \n\n Week 8 \n    Mar-6 \n   \n     \n     \n     \n  \n\n Mar-8 \n   \n     \n     \n     \n  \n\n Week 9 \n    Mar-13 \n    Spring Break - NO CLASS \n     \n     \n     \n  \n\n Mar-15 \n   \n     \n     \n    Quiz 3 \n  \n\n Week 10 \n    Mar-20 \n    ANOVA \n     \n     \n     \n  \n\n Mar-22 \n   \n     \n     \n     \n  \n\n Week 11 \n    Mar-27 \n   \n     \n     \n     \n  \n\n Mar-29 \n   \n     \n     \n     \n  \n\n Week 12 \n    Apr-3 \n    Model Comparison \n     \n     \n     \n  \n\n Apr-5 \n   \n     \n     \n    Quiz 4 \n  \n\n Week 13 \n    Apr-10 \n   \n     \n     \n     \n  \n\n Apr-12 \n    Bayesian Inference \n     \n     \n     \n  \n\n Week 14 \n    Apr-17 \n   \n     \n     \n     \n  \n\n Apr-19 \n   \n     \n     \n     \n  \n\n Week 15 \n    Apr-24 \n   \n     \n     \n     \n  \n\n Apr-26 \n   \n     \n     \n    Quiz 5 \n  \n\n Week 16 \n    May-1 \n    Research Lecture \n     \n     \n     \n  \n\n May-3"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "1  Software",
    "section": "",
    "text": "You will need to have all of the following free software downloaded and in working order on your laptop.\n\n\n\n\n\n\nPrior to first lecture\n\n\n\nYou must have the following on your laptops prior to the first lecture.\n\n\n\nLatest version of RStudio Desktop IDE\nCompatible version of R software environment\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\n\n\n\n\n\n\n\nPrior to Bayesian inference\n\n\n\nYou must have the following on your laptops prior to the sections on Bayesian inference.\n\n\n\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function."
  },
  {
    "objectID": "syllabus.html#living-course-schedule",
    "href": "syllabus.html#living-course-schedule",
    "title": "Appendix A: Syllabus",
    "section": "\nA.1 Living course schedule",
    "text": "A.1 Living course schedule\nThis schedule will be consistently updated throughout the course. Check back often.\n\n\n\n\n\n Week \n    Date \n    Topic \n    Reading_Due \n    Assign_Due \n    Quiz \n  \n\n\n Week 1 \n    Jan-16 \n    MLK Holiday - NO CLASS \n     \n     \n     \n  \n\n Jan-18 \n    Introduction \n    JM(Ch1,2) \n     \n     \n  \n\n Week 2 \n    Jan-23 \n    Probability distributions \n    JB(pg1-20), JM(Ch3), FAR(Ch1) \n    PS-0 \n     \n  \n\n Jan-25 \n    Probability distributions \n     \n     \n     \n  \n\n Week 3 \n    Jan-30 \n    Least Squares \n    JB(pg20-36), JM(Ch \n    PS-1 \n     \n  \n\n Feb-1 \n    Least Squares \n     \n     \n    Quiz 1 \n  \n\n Week 4 \n    Feb-6 \n    Least Squares \n     \n    PS-2 \n     \n  \n\n Feb-8 \n    Least Squares \n     \n     \n     \n  \n\n Week 5 \n    Feb-13 \n    Least Squares \n     \n     \n     \n  \n\n Feb-15 \n    Hypothesis Testing \n     \n     \n     \n  \n\n Week 6 \n    Feb-20 \n    Hypothesis Testing \n     \n     \n    Quiz 2 \n  \n\n Feb-22 \n    Maximum Likelihood \n     \n     \n     \n  \n\n Week 7 \n    Feb-27 \n    Maximum Likelihood \n     \n     \n     \n  \n\n Mar-1 \n    Maximum Likelihood \n     \n     \n     \n  \n\n Week 8 \n    Mar-6 \n    Maximum Likelihood \n     \n     \n     \n  \n\n Mar-8 \n    Maximum Likelihood \n     \n     \n    Quiz 3 \n  \n\n Week 9 \n    Mar-13 \n    Spring Break - NO CLASS \n     \n     \n     \n  \n\n Mar-15 \n    Spring Break - NO CLASS \n     \n     \n     \n  \n\n Week 10 \n    Mar-20 \n    ANOVA \n     \n     \n     \n  \n\n Mar-22 \n    ANOVA \n     \n     \n     \n  \n\n Week 11 \n    Mar-27 \n    ANOVA \n     \n     \n     \n  \n\n Mar-29 \n    ANOVA \n     \n     \n     \n  \n\n Week 12 \n    Apr-3 \n    Model Comparison \n     \n     \n     \n  \n\n Apr-5 \n    Model Comparison \n     \n     \n     \n  \n\n Week 13 \n    Apr-10 \n    Model Comparison \n     \n     \n     \n  \n\n Apr-12 \n    Bayesian Inference \n     \n     \n    Quiz 4 \n  \n\n Week 14 \n    Apr-17 \n    Bayesian Inference \n     \n     \n     \n  \n\n Apr-19 \n    Bayesian Inference \n     \n     \n     \n  \n\n Week 15 \n    Apr-24 \n    Bayesian Inference \n     \n     \n     \n  \n\n Apr-26 \n    Bayesian Inference \n     \n     \n     \n  \n\n Week 16 \n    May-1 \n    Research Lecture \n     \n     \n    Quiz 5 \n  \n\n May-3 \n    Research Lecture"
  },
  {
    "objectID": "syllabus.html#bblearn-collaborate-ultra-recorded-lectures",
    "href": "syllabus.html#bblearn-collaborate-ultra-recorded-lectures",
    "title": "Appendix A: Syllabus",
    "section": "A.1 BbLearn, Collaborate Ultra & Recorded Lectures",
    "text": "A.1 BbLearn, Collaborate Ultra & Recorded Lectures\nWe will use the learning management system, BbLearn, to conduct some course business, including assignment disbursement and submitting. I will use BbLearn’s Collaborate Ultra to record lectures for future viewing, but you may also use Collaborate Ultra to attend virtually, if you cannot attend physically."
  },
  {
    "objectID": "syllabus.html#inf511-book-website",
    "href": "syllabus.html#inf511-book-website",
    "title": "Appendix A: Syllabus",
    "section": "A.2 INF511 Book Website",
    "text": "A.2 INF511 Book Website\nDr. Mihaljevic has compiled a course website that is based on a GitHub repository. This website will have required readings, coded examples which we will walk through in class and problem set materials. The website will link to Bblearn for recorded lecture materials and for posted homework. There will be other required readings (see Section A.9)."
  },
  {
    "objectID": "syllabus.html#monitoring-attendance",
    "href": "syllabus.html#monitoring-attendance",
    "title": "Appendix A: Syllabus",
    "section": "A.3 Monitoring Attendance",
    "text": "A.3 Monitoring Attendance\nI will allow in-person and virtual attendance, although technically NAU policy is for in person attendance unless an absence is formally excused. We will monitor attendance in class and through Collaborate Ultra. See below for specific guidance on how attendance can affect your grade."
  },
  {
    "objectID": "syllabus.html#course-purpose",
    "href": "syllabus.html#course-purpose",
    "title": "Appendix A: Syllabus",
    "section": "A.4 Course Purpose",
    "text": "A.4 Course Purpose\nINF 511 Modern Regression I is the first course in a two-semester sequence required for the MS and PhD in Informatics and Computing (INF). (See INF 512 Modern Regression II.) These courses are designed to serve the computationally oriented statistical analysis needs of the INF graduate program. Through a series of hands-on individual or team-based assignments, students will master statistical analyses, from preparing data, exploring data using numerical and/or graphical methods, modeling data, diagnosing model assumptions, remodeling and final inference. This course will provide INF graduate students with the necessary foundation for more specialized statistical methods and applications that students will encounter in subsequent INF courses, such as INF 626 Applied Bayesian Modeling and the more prediction-oriented INF 504 Data Mining and Machine Learning. More generally, INF 511 provides skills widely applicable to analysis of data across science and engineering.\nINF 511 Modern Regression I covers fundamental probability models and their use in the analysis of independent data with linear models within both frequentist and Bayesian statistical frameworks. Random variables, expectation, variance, covariance, correlation. Joint, conditional and marginal distributions. Linear combinations of random variables; central limit theorem; matrices, vectors, basic matrix arithmetic, matrix formulation of linear statistical models (regression and ANOVA) for independent data, normal likelihood, least squares, Gauss-Markov theorem, \\(t\\) and \\(F\\) sampling distribution-based inference for linear combinations of parameters. Corresponding Bayesian analysis including prior and posterior distributions, introductory Markov chain Monte Carlo methods. Diagnostics, including graphical residual analysis. Scope of inference (randomization and causality, random sampling and population)."
  },
  {
    "objectID": "syllabus.html#course-student-learning-outcomes",
    "href": "syllabus.html#course-student-learning-outcomes",
    "title": "Appendix A: Syllabus",
    "section": "A.5 Course Student Learning Outcomes",
    "text": "A.5 Course Student Learning Outcomes\nThe overall learning outcome for this course is a demonstrated acquisition of skills and conceptual understanding of statistical methods to enable complete and valid statistical analyses of primarily independent data modeled with relatively traditional linear statistical models from both a frequentist and Bayesian perspective at a level commensurate with high expectations of a well-trained graduate student in the quantitatively and computationally intensive field of informatics. For these data, and using the methods and concepts detailed in the Course Purpose, students should be able to:\n\nUse numerical and graphical exploratory data analysis tools to prepare data and to develop conceptual mod- els of data\nTransform conceptual models of data into formal linear statistical models of data\nImplement linear model methods in modern software, such as R or Stan, to analyze data\nDemonstrate an understanding that models are an abstracted simplification of real processes by effectively using linear model diagnostics, remedial measures, remodeling and final model validation/confirmation methods.\nDemonstrate an understanding of the limitations of data and methods by communicating how the data and methods relate to randomization and causality, random selection and population, over-fitting and exploratory/confirmatory analyses, and the trade-off between efficiency of inference and robustness to departures from method assumptions\nDemonstrate an ability to work effectively in a team environment to solve realistic problems, which may be beyond any one individual!s ability to address, as indicated by peer review or other assessments of team- work."
  },
  {
    "objectID": "syllabus.html#sec-assess",
    "href": "syllabus.html#sec-assess",
    "title": "Appendix A: Syllabus",
    "section": "A.6 Assessments of Course Student Learning Outcomes",
    "text": "A.6 Assessments of Course Student Learning Outcomes\nThere will be three assessment strategies: problem sets, homework, and quizzes. Problem sets will have dedicated in-class time to complete, whereas homework assignments will be done entirely outside of class. Problem sets will be completed individually, and will be primarily assessed as complete/incomplete, whereas homework assignments can be completed in teams of one to three students and will be graded in full. Assignments typically feature one or more data sets as the focus one or more aspects of a complete statistical analysis, from preparing the data, exploring the data using numerical and/or graphical methods, modeling the data, diagnosing model assumptions, remodeling and final inference. Assignment format is designed in part to mimic and reinforce the similar presentation of analyses in class/notes and to encourage discussion among students. There will be five in-class quizzes, one every $$3 weeks, in lieu of exams. Attendance will also be used to assess your course performance."
  },
  {
    "objectID": "syllabus.html#grading-system",
    "href": "syllabus.html#grading-system",
    "title": "Appendix A: Syllabus",
    "section": "A.7 Grading System",
    "text": "A.7 Grading System\n\n\n\nProblem Sets\nHomework\nQuizzes\nAttendance\n\n\n\n\n20%\n35%\n40%\n5%\n\n\n\n\nAssignments. There are two categories of assignments, Problem Sets and Homework. See Section A.6 for the distinctions. See Section A.10 for due-dates. Assignments will be posted periodically via BbLearn. Assignments are to be submitted electronically, via BbLearn, on the due date/time indicated in BbLearn. Late homework will not be accepted. For Homework, students are free to arrange different teams for different homeworks; this will not be monitored. For Homework, please ensure that all team member names are on a submitted assignment. Everyone on a team will submit exactly same homework; only one team member’s submission will be selected and graded with all members receiving the same score.\nQuizzes. There will be five in-class quizzes that are all cumulative. Each quiz will be designed to take approximately 15-20 minutes, and each will be scaled to 100 points so that they are equally weighted.\nAttendance & Participation. Attendance will be recorded regularly via an in-class sign-up sheet and via Collaborate Ultra. Late arrivals or early departures exceeding 15 minutes may be recorded as an absence. You are responsible to plan with your fellow classmates to obtain in-class material not received due to your absence. (Recall that lectures will be recorded, and nearly all, if not all, material will be available in BbLearn.) Participation in the form of responding to questions in class, asking questions, and attending office hours may be used to determine “borderline” grade cases."
  },
  {
    "objectID": "syllabus.html#course-grades",
    "href": "syllabus.html#course-grades",
    "title": "Appendix A: Syllabus",
    "section": "A.8 Course Grades",
    "text": "A.8 Course Grades\nOverall course grades will follow a typical scale:\n\n\n\nTo earn the letter grade ->\nA\nB\nC\nD\nF\n\n\n\n\nYou need at least this score\n90\n80\n70\n60\n0\n\n\n\nWhile you should be able to compute an estimate of your current grade using the information above, I (or your TA) will attempt to use the Grade feature in BbLearn so that you are able to check your grades. Grading mistakes may occur, and students are encouraged to discuss such concerns with the instructor or TA during office hours or by appointment."
  },
  {
    "objectID": "syllabus.html#readings-and-materials",
    "href": "syllabus.html#readings-and-materials",
    "title": "Appendix A: Syllabus",
    "section": "\nA.9 Readings and Materials",
    "text": "A.9 Readings and Materials\n\nLecture Materials: Lecture topics, course notes, readings, and assignments will be made available as the semester progresses. For each lecture, a document will be linked on the course website. During lecture, we will work together to fill out this document with written notes presented on the (virtual) whiteboard. Therefore, it is essential to downlaod and print these materials prior to attending class.\n\nRequired Text:\n\nBarber, J.J. (2022). INF511 Modern Regression I - Lecture Slides. Posted on Bblearn. Referred to as JB in Section A.10.\nMihaljevic, J (2023). INF511 Modern Regression I - Online Book. https://joseph-mihaljevic.github.io/inf511-book/. Referred to as JM in Section A.10.\nFaraway, J.J. (2014). Linear Models with R (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b17144 Referred to as FAR in Section A.10.\n\n\n\n\nComputing. Each student must bring their laptop to class with the following (freely available) software pre-installed:\n\nLatest version of RStudio Desktop IDE\n\nCompatible version of R software environment\n\n\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function."
  },
  {
    "objectID": "syllabus.html#sec-schedule",
    "href": "syllabus.html#sec-schedule",
    "title": "Appendix A: Syllabus",
    "section": "A.10 Living course schedule",
    "text": "A.10 Living course schedule\nThis schedule will be consistently updated throughout the course. Check back often.\n\n\n\n\n \n  \n    Week \n    Date \n    Topic \n    Reading_Due \n    Assign_Due \n    Quiz \n  \n \n\n  \n    Week 1 \n    16-Jan \n    MLK Holiday - NO CLASS \n     \n     \n     \n  \n  \n    Week 1 \n    18-Jan \n    Introduction \n    Syllabus, JM(Ch1&2), JB(1-20, AppA) \n     \n     \n  \n  \n    Week 2 \n    23-Jan \n    Probability distributions \n    JB(pg1-20), JM(Ch3), FAR(Ch1) \n    PS-0 \n     \n  \n  \n    Week 2 \n    25-Jan \n    Probability distributions \n     \n     \n     \n  \n  \n    Week 3 \n    30-Jan \n    Least Squares \n    JB(pg20-38, AppB), JM(Ch4) \n    PS-1, HW-1 \n     \n  \n  \n    Week 3 \n    1-Feb \n    Least Squares \n    FAR(Ch2) \n     \n    Quiz 1 \n  \n  \n    Week 4 \n    6-Feb \n    Least Squares \n    JB(pg41-60) \n    PS-2 \n     \n  \n  \n    Week 4 \n    8-Feb \n    Least Squares \n     \n    HW-2 \n     \n  \n  \n    Week 5 \n    13-Feb \n    Least Squares \n    JB(pg157-170), FAR(Ch3) \n     \n     \n  \n  \n    Week 5 \n    15-Feb \n    Hypothesis Testing \n    JB(pg61-79), JM(Ch5), FAR(Ch10) \n    PS-3 \n     \n  \n  \n    Week 6 \n    20-Feb \n    Hypothesis Testing \n    JB(pg80-127), FAR(Ch4) \n     \n    Quiz 2 \n  \n  \n    Week 6 \n    22-Feb \n    Maximum Likelihood \n    JB(pg38-40), JM(Ch6) \n    HW-3 \n     \n  \n  \n    Week 7 \n    27-Feb \n    Maximum Likelihood \n    FAR(Ch5) \n     \n     \n  \n  \n    Week 7 \n    1-Mar \n    Maximum Likelihood \n    FAR(Ch6), JB(skimCh6) \n    PS-4 \n     \n  \n  \n    Week 8 \n    6-Mar \n    Maximum Likelihood \n    FAR(Ch7) \n     \n     \n  \n  \n    Week 8 \n    8-Mar \n    Maximum Likelihood \n    FAR(Ch8) \n    HW-4 \n    Quiz 3 \n  \n  \n    Week 9 \n    13-Mar \n    Spring Break - NO CLASS \n     \n     \n     \n  \n  \n    Week 9 \n    15-Mar \n    Spring Break - NO CLASS \n     \n     \n     \n  \n  \n    Week 10 \n    20-Mar \n    ANOVA \n    JB(Ch14), JM(Ch7), FAR(Ch14) \n    PS-5 \n     \n  \n  \n    Week 10 \n    22-Mar \n    ANOVA \n    JB(Ch15.1-15.3) \n     \n     \n  \n  \n    Week 11 \n    27-Mar \n    ANOVA \n    JB(Ch15.4-15.5), FAR(Ch15) \n    PS-6 \n     \n  \n  \n    Week 11 \n    29-Mar \n    ANOVA \n    FAR(Ch16) \n     \n     \n  \n  \n    Week 12 \n    3-Apr \n    Model Comparison \n    JB(refreshCh3.2), FAR(Ch10) \n    HW-5 \n     \n  \n  \n    Week 12 \n    5-Apr \n    Model Comparison \n    FAR(Ch11) \n     \n     \n  \n  \n    Week 13 \n    10-Apr \n    Model Comparison \n    JB(pg170-223) \n    PS-7 \n     \n  \n  \n    Week 13 \n    12-Apr \n    Bayesian Inference \n     \n     \n    Quiz 4 \n  \n  \n    Week 14 \n    17-Apr \n    Bayesian Inference \n     \n     \n     \n  \n  \n    Week 14 \n    19-Apr \n    Bayesian Inference \n     \n    HW-6 \n     \n  \n  \n    Week 15 \n    24-Apr \n    Bayesian Inference \n     \n    PS-8 \n     \n  \n  \n    Week 15 \n    26-Apr \n    Bayesian Inference \n     \n     \n     \n  \n  \n    Week 16 \n    1-May \n    Research Lecture \n     \n    PS-9 \n    Quiz 5 \n  \n  \n    Week 16 \n    3-May \n    Research Lecture \n     \n    HW-7"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Appendix A: Syllabus",
    "section": "A.11 Course Policies",
    "text": "A.11 Course Policies\n\nVisiting the instructor during office hours is encouraged! I am happy to talk about the class, careers, research, and topics related (even loosely) to this course. If a student cannot attend regular office hours, an appointment may be made via email, with sufficient advanced notice.\nEmail addressed to the instructor and TA must be respectful and professional. The instructor will respond to emails promptly, within 2 business days, but the instructor will generally not respond to emails on weekends or late in the evenings, so please plan accordingly.\nCheating, including plagiarism of writing or computer code, will not be tolerated. The University’s Academic Integrity policies (Section A.12) will be strictly enforced.\nThe paramount policy of this course is that each student is required to demonstrate respect towards their peers and the instructor. The behavior of the instructor is held to the same standard. Students and instructors come from all walks of life, and may identify with a variety of ethnic, racial, religious, gender and sexual identities. Diversity of thought and perspective enhances our science.\nAttendance is required and will be recorded. Email the instructor to discuss excused absences. Excessive unexcused absences will reduce your overall grade: 2 unexcused absences leads to 10% course grade reduction, \\(\\dots\\), 5 unexcused absences leads to 40% course grade reduction, and 6 unexcused absences leads to a failing grade in the course.\nThe instructor will not provide copies of course notes. These materials should be sought from the students’ peers or by watching the recorded lectures.\nNo late work will be accepted (i.e., any late assignments will receive zero points). This policy can be relaxed in unique circumstances that are negotiated beforehand with the professor.\nCheating and plagiarism are strictly prohibited. All academic integrity violations are treated seriously. Academic integrity violations will result in penalties including, but not limited to, a zero on the assignment, a failing grade in the class, or expulsion from NAU. For group assignments, the “Roles of PIs” document must be of sufficient detail that the instructor can understand the intellectual contributions of all parties. For individual-based assignments, students are encouraged to discuss the intellectual aspects of assignments with other class participants. However, each student is responsible for formulating solutions on their own and in their own words.\nElectronic device usage must support learning in the class. All cell phones, PDAs, music players and other entertainment devices must be turned off (or put on silent) during lecture.\nGrades will be entered in BbLearn. Please check LOUIE for your final grade."
  },
  {
    "objectID": "syllabus.html#sec-univ-policy",
    "href": "syllabus.html#sec-univ-policy",
    "title": "Appendix A: Syllabus",
    "section": "A.12 University Policies",
    "text": "A.12 University Policies\n\nA.12.1 COVID-19 REQUIREMENTS AND INFORMATION\nAdditional information about the University’s response to COVID-19 is available from the Jacks are Back! web page located at https://nau.edu/jacks-are-back.\n\n\nA.12.2 ACADEMIC INTEGRITY\nNAU expects every student to firmly adhere to a strong ethical code of academic integrity in all their scholarly pursuits. The primary attributes of academic integrity are honesty, trustworthiness, fairness, and responsibility. As a student, you are expected to submit original work while giving proper credit to other people’s ideas or contributions. Acting with academic integrity means completing your assignments independently while truthfully acknowledging all sources of information, or collaboration with others when appropriate. When you submit your work, you are implicitly declaring that the work is your own. Academic integrity is expected not only during formal coursework, but in all your relationships or interactions that are connected to the educational enterprise. All forms of academic deceit such as plagiarism, cheating, collusion, falsification or fabrication of results or records, permitting your work to be submitted by another, or inappropriately recycling your own work from one class to another, constitute academic misconduct that may result in serious disciplinary consequences. All students and faculty members are responsible for reporting suspected instances of academic misconduct. All students are encouraged to complete NAU’s online academic integrity workshop available in the E-Learning Center and should review the full Academic Integrity policy available at https://policy.nau.edu/policy/policy.aspx?num=100601.\n\n\nA.12.3 COPYRIGHT INFRINGEMENT\nAll lectures and course materials, including but not limited to exams, quizzes, study outlines, and similar materials are protected by copyright. These materials may not be shared, uploaded, distributed, reproduced, or publicly displayed without the express written permission of NAU. Sharing materials on websites such as Course Hero, Chegg, or related websites is considered copyright infringement subject to United States Copyright Law and a violation of NAU Student Code of Conduct. For additional information on ABOR policies relating to course materials, please refer to ABOR Policy 6-908 A(2)(5).\n\n\nA.12.4 COURSE TIME COMMITMENT\nPursuant to Arizona Board of Regents guidance (ABOR Policy 2-224, Academic Credit), each unit of credit requires a minimum of 45 hours of work by students, including but not limited to, class time, preparation, homework, and studying. For example, for a 3-credit course a student should expect to work at least 8.5 hours each week in a 16-week session and a minimum of 33 hours per week for a 3-credit course in a 4-week session.\n\n\nA.12.5 DISRUPTIVE BEHAVIOR\nMembership in NAU’s academic community entails a special obligation to maintain class environments that are conductive to learning, whether instruction is taking place in the classroom, a laboratory or clinical setting, during course-related fieldwork, or online. Students have the obligation to engage in the educational process in a manner that does not interfere with normal class activities or violate the rights of others. Instructors have the authority and responsibility to address disruptive behavior that interferes with student learning, which can include the involuntary withdrawal of a student from a course with a grade of “W”. For additional information, see NAU’s Disruptive Behavior in an Instructional Setting policy at https://nau.edu/university-policy-library/disruptive-behavior.\n\n\nA.12.6 NONDISCRIMINATION AND ANTI-HARASSMENT\nNAU prohibits discrimination and harassment based on sex, gender, gender identity, race, color, age, national origin, religion, sexual orientation, disability, veteran status and genetic information. Certain consensual amorous or sexual relationships between faculty and students are also prohibited as set forth in the Consensual Romantic and Sexual Relationships policy. The Equity and Access Office (EAO) responds to complaints regarding discrimination and harassment that fall under NAU’s Nondiscrimination and Anti- Harassment policy. EAO also assists with religious accommodations. For additional information about nondiscrimination or anti-harassment or to file a complaint, contact EAO located in Old Main (building 10), Room 113, PO Box 4083, Flagstaff, AZ 86011, or by phone at 928-523-3312 (TTY: 928-523-1006), fax at 928-523-9977, email at equityandaccess@nau.edu, or visit the EAO website at https://nau.edu/equity-and-access.\n\n\nA.12.7 TITLE IX\nTitle IX of the Education Amendments of 1972, as amended, protects individuals from discrimination based on sex in any educational program or activity operated by recipients of federal financial assistance. In accordance with Title IX, Northern Arizona University prohibits discrimination based on sex or gender in all its programs or activities. Sex discrimination includes sexual harassment, sexual assault, relationship violence, and stalking. NAU does not discriminate on the basis of sex in the education programs or activities that it operates, including in admission and employment. NAU is committed to providing an environment free from discrimination based on sex or gender and provides a number of supportive measures that assist students, faculty, and staff.\nOne may direct inquiries concerning the application of Title IX to either or both the Title IX Coordinator or the U.S. Department of Education, Assistant Secretary, Office of Civil Rights. You may contact the Title IX Coordinator in the Office for the Resolution of Sexual Misconduct by phone at 928-523-5434, by fax at 928-523-0640, or by email at titleix@nau.edu. In furtherance of its Title IX obligations, NAU promptly will investigate or equitably resolve all reports of sex or gender-based discrimination, harassment, or sexual misconduct and will eliminate any hostile environment as defined by law. The Office for the Resolution of Sexual Misconduct (ORSM): Title IX Institutional Compliance, Prevention & Response addresses matters that fall under the university’s Sexual Misconduct policy. Additional important information and related resources, including how to request immediate help or confidential support following an act of sexual violence, is available at https://in.nau.edu/title-ix.\n\n\nA.12.8 ACCESSIBILITY\nProfessional disability specialists are available at Disability Resources to facilitate a range of academic support services and accommodations for students with disabilities. If you have a documented disability, you can request assistance by contacting Disability Resources at 928-523-8773 (voice), ,928-523-8747 (fax), or dr@nau.edu (e-mail). Once eligibility has been determined, students register with Disability Resources every semester to activate their approved accommodations. Although a student may request an accommodation at any time, it is best to initiate the application process at least four weeks before a student wishes to receive an accommodation. Students may begin the accommodation process by submitting a self-identification form online at https://nau.edu/disability-resources/student-eligibility-process or by contacting Disability Resources. The Director of Disability Resources, Jamie Axelrod, serves as NAU’s Americans with Disabilities Act Coordinator and Section 504 Compliance Officer. He can be reached at jamie.axelrod@nau.edu.\n\n\nA.12.9 RESPONSIBLE CONDUCT OF RESEARCH\nStudents who engage in research at NAU must receive appropriate Responsible Conduct of Research (RCR) training. This instruction is designed to help ensure proper awareness and application of well-established professional norms and ethical principles related to the performance of all scientific research activities. More information regarding RCR training is available at https://nau.edu/research/compliance/research-integrity.\n\n\nA.12.10 MISCONDUCT IN RESEARCH\nAs noted, NAU expects every student to firmly adhere to a strong code of academic integrity in all their scholarly pursuits. This includes avoiding fabrication, falsification, or plagiarism when conducting research or reporting research results. Engaging in research misconduct may result in serious disciplinary consequences. Students must also report any suspected or actual instances of research misconduct of which they become aware. Allegations of research misconduct should be reported to your instructor or the University’s Research Integrity Officer, Dr. David Faguy, who can be reached at david.faguy@nau.edu or 928-523-6117. More information about misconduct in research is available at https://nau.edu/university-policy-library/misconduct-in-research.\n\n\nA.12.11 SENSITIVE COURSE MATERIALS\nUniversity education aims to expand student understanding and awareness. Thus, it necessarily involves engagement with a wide range of information, ideas, and creative representations. In their college studies, students can expect to encounter and to critically appraise materials that may differ from and perhaps challenge familiar understandings, ideas, and beliefs. Students are encouraged to discuss these matters with faculty."
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "7  ANOVA",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "syllabus.html#sec-readings",
    "href": "syllabus.html#sec-readings",
    "title": "Appendix A: Syllabus",
    "section": "A.9 Readings and Materials",
    "text": "A.9 Readings and Materials\n\nLecture Materials: Lecture topics, course notes, readings, and assignments will be made available as the semester progresses. For each lecture, a document will be linked on the course website. During lecture, we will work together to fill out this document with written notes presented on the (virtual) whiteboard. Therefore, it is essential to downlaod and print these materials prior to attending class.\nRequired Text:\n\nBarber, J.J. (2022). INF511 Modern Regression I - Lecture Slides. Posted on Bblearn. Referred to as JB in Section A.10.\nMihaljevic, J (2023). INF511 Modern Regression I - Online Book. https://joseph-mihaljevic.github.io/inf511-book/. Referred to as JM in Section A.10.\nFaraway, J.J. (2014). Linear Models with R (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b17144 Referred to as FAR in Section A.10.\n\n\nComputing. Each student must bring their laptop to class with the following (freely available) software pre-installed:\n\nLatest version of RStudio Desktop IDE\nCompatible version of R software environment\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function."
  },
  {
    "objectID": "ols.html#sec-lm-output",
    "href": "ols.html#sec-lm-output",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.4 Estimate the coefficients using R’s lm() function",
    "text": "4.4 Estimate the coefficients using R’s lm() function\n\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ 1 + x1)\n\nResiduals:\n     1      2      3      4 \n-1.029 -1.657  1.086  1.600 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   5.7714     2.0500   2.815    0.106\nx1            1.6286     0.6565   2.481    0.131\n\nResidual standard error: 1.942 on 2 degrees of freedom\nMultiple R-squared:  0.7547,    Adjusted R-squared:  0.6321 \nF-statistic: 6.153 on 1 and 2 DF,  p-value: 0.1313\n\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n\n(Intercept)          x1 \n   5.771429    1.628571"
  },
  {
    "objectID": "ols.html#sec-est-plot",
    "href": "ols.html#sec-est-plot",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.6 Plot the estimated relationships",
    "text": "4.6 Plot the estimated relationships\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)"
  },
  {
    "objectID": "ols.html#understanding-uncertainty-in-hatb",
    "href": "ols.html#understanding-uncertainty-in-hatb",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.8 Understanding Uncertainty in \\(\\hat{B}\\)\n",
    "text": "4.8 Understanding Uncertainty in \\(\\hat{B}\\)\n\nWhile the OLS analysis estimates the regression coefficients, \\(\\hat{B}\\), from the observed data \\(Y\\), our estimates of these coefficients have error (i.e., uncertainty), such that the estimates are only as good as the data. Specifically, if we have fewer data points (i.e., \\(n\\) is low), we have less certainty in \\(\\hat{B}\\). In lecture, we showed, that: \\[\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right), \\] and we know that \\(\\hat{\\sigma}^2\\) depends on sample size \\(n\\), following: \\[\\hat{\\sigma}^2 \\quad = \\quad \\frac{1}{n-p} (Y_{obs} - Y_{pred})^T (Y_{obs} - Y_{pred}) \\quad = \\quad \\frac{1}{n-p} \\hat{\\epsilon}^T \\hat{\\epsilon}\\]\nUsing these equations, we showed then that \\(SE(\\beta_i) = \\sqrt{diag\\left( (X^TX)^{-1} \\right)_i \\hat{\\sigma}^2}\\). Let’s calculate this manually and compare to the output of the lm() function.\n\n# Extract the model summary, which has useful components\nlm_out_summary = summary(lm_out)\n# Extract the estimated residual standard deviation, sigma\nest_sigma = lm_out_summary$sigma\nest_sigma\n\n[1] 1.942017\n\n# We already calculated (X^T X)^{-1} as inv_xtx\nbeta_cov_mat = inv_xtx * est_sigma^2\nbeta_cov_mat\n\n          [,1]       [,2]\n[1,]  4.202449 -1.1853061\n[2,] -1.185306  0.4310204\n\nse_beta = sqrt(diag(beta_cov_mat))\nse_beta\n\n[1] 2.0499876 0.6565214\n\n\nCompare these values to the output of the summary() of Section 4.4 in the column labelled Std. Error."
  },
  {
    "objectID": "ols.html#sec-conf-beta",
    "href": "ols.html#sec-conf-beta",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.9 Confidence Intervals for \\(\\hat{B}\\)\n",
    "text": "4.9 Confidence Intervals for \\(\\hat{B}\\)\n\nTo calculate confidence intervals for \\(\\hat{B}\\), we first must understand the \\(t\\) (a.k.a. Student’s \\(t\\)) probability distribution. This distribution represents the case when we are estimating the mean of a normally distributed variable and either the sample size is small or the variable’s standard deviation is unknown. Essentially, the \\(t\\) distribution increases the uncertainty (i.e., variance) in cases of low sample size (i.e., small \\(n\\)). With low sample size (and/or high number of parameters), the degrees of freedom of the \\(t\\)-distribution, \\(\\nu\\) is low, whereas with high sample size, \\(\\nu\\) is large. As \\(\\nu\\) approaches infinity, the \\(t\\)-distribution approximates the standard normal distribution (i.e., \\(N(\\mu, \\sigma)|\\mu=0,\\sigma=1\\)).\n\n\n\n\n\nIt is the case for \\(\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right)\\) that we do not know the mean (\\(B\\)), and we are estimating the variance, \\(\\hat{\\sigma}^2\\). Specifically, we are estimating the true mean vector, \\(B\\), as \\(\\hat{B}\\), and we are estimating the variance of the residuals as \\(\\hat{\\sigma}^2\\). We can therefore re-write the uncertainty in \\(\\hat{B}\\) as a multivariate \\(t\\) distribution: \\[(\\hat{B} - B) \\sim t_{\\nu} \\left( 0, \\Sigma \\right),\\] where the means are zero, \\(\\nu\\) is the degrees of freedom (i.e., \\(n-p\\)), and \\(\\Sigma = (X^TX)^{-1} \\hat{\\sigma}^2\\). \\((\\hat{B} - B)\\) represents the deviation of the estimated coefficients from the true coefficients, which is why the distribution is centered around zero. It is perhaps easier to separate the individual estimated coefficients, \\(\\beta_i\\), into their separate \\(t\\)-distributions, estimated as the off-diagonals of \\((X^TX)^{-1} \\hat{\\sigma}^2\\): \\[(\\hat{\\beta}_i - \\beta_i) \\sim t_{\\nu} \\hat{\\sigma},\\] which shows that the \\(t\\)-distribution is scaled by the estimated \\(SE(\\hat{\\beta}_i)\\). As shown in Dr. Barber’s materials, using this information, we can derive the confidence interval (at the \\(\\alpha\\) confidence level) calculation for \\(\\hat{\\beta}_i\\) as: \\[ \\hat{\\beta}_i \\pm t \\left(\\frac{1-\\alpha}{2}, \\nu \\right) SE(\\hat{\\beta}_i),\\] where the \\(t()\\) notation represents the critical value of the \\(t\\)-distribution with \\(\\nu\\) degrees of freedom, \\(t_{crit}\\), for which \\(P(z \\le t_{crit}) = \\frac{1-\\alpha}{2}\\), and \\(z\\) is a continuous, random variable.\n\n\n\n\n\n\nCovariance of \\(\\hat{\\beta}_i\\)\n\n\n\nAlthough it is convenient and easier to digest the confidence interval of individual \\(\\hat{\\beta}_i\\), we must realize that the estimates of the \\(\\beta_i\\) can covary (i.e., have non-zero covariance), which is quantified in the variance-covariance matrix of \\(\\hat{B}\\), \\((X^TX)^{-1} \\hat{\\sigma}^2\\). We will show how this is important below.\n\n\nLet’s manually calculate the 95% confidence intervals in \\(\\hat{B}\\) and compare to R’s internal function confint().\n\n# Extract the degrees of freedom from the model (\\nu)\n# which can also be calculated as n - p\nt_df = lm_out$df.residual\n\n# Calculate t critical for alpha = 0.05\n# This will give us the 95% conf interval (CI)\nt_crit = qt(1-(0.05/2), df = t_df)\n\n# Calculate the upper and lower CI for both betas\nci_int = lm_coef[1] + c(-1,1)*t_crit*se_beta[1]\nci_slope = lm_coef[2] + c(-1,1)*t_crit*se_beta[2]\n\n# Construct a table of values\nci_mat = \n    rbind(c(lm_coef[1], ci_int),\n          c(lm_coef[2], ci_slope))\ncolnames(ci_mat) = c(\"coef\", \"lowCI\", \"highCI\")\nrownames(ci_mat) = c(\"intercept\", \"slope\")\nci_mat\n\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n\n# Compare these manual calculations to built-in\n# function confint(), which by default extracts the \n# 95% CI for a lm() model's coefficients\nconfint(lm_out)\n\n                2.5 %    97.5 %\n(Intercept) -3.048956 14.591813\nx1          -1.196212  4.453355"
  },
  {
    "objectID": "ols.html#propagate-uncertainty-in-hatb-for-predictions-of-y",
    "href": "ols.html#propagate-uncertainty-in-hatb-for-predictions-of-y",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.10 Propagate uncertainty in \\(\\hat{B}\\) for predictions of \\(Y\\)\n",
    "text": "4.10 Propagate uncertainty in \\(\\hat{B}\\) for predictions of \\(Y\\)\n\nThere are several ways to calculate and visualize our uncertainty in model predictions of observed data \\(Y\\) and unobserved data of the dependent variable (i.e., interpolation). The colored lines drawn on the figure in Section 4.6 represent the expected values of \\(Y\\) based on the OLS analysis’ estimate of \\(\\hat{B}\\), but this line does not include uncertainty in these coefficient values.\n\n4.10.1 Multivariate \\(t\\)-distribution method\nFirst, we will calculate uncertainty by sampling from the multivariate \\(t\\) distribution that represents error in regression coefficients, \\(\\hat{B}\\).\n\n# We will \"bootstrap\" 1000 samples of intercept and slope\nset.seed(3)\nn_samp = 500\n\n# Draw from the multivariate t \n# which represents (\\hat{B} - B)\ntest_mat_deviates = \n  mnormt::rmt(n_samp, mean = c(0,0), S = beta_cov_mat, df = t_df)\n\n# Now calculate the realized intercept and slope\n# using the t-distributed deviates\ntest_mat_t = cbind(\n  lm_coef[1] + c(test_mat_deviates[,1]),\n  lm_coef[2] + c(test_mat_deviates[,2])\n)\n\n# Calculate the 95% quantiles and compare to the \n# calculated 95% confidence intervals from above\napply(test_mat_t, \n      MARGIN = 2, # applies function (FUN) to columns (dim 2)\n      FUN = quantile, probs = c(0.025, 0.5, 0.975))\n\n           [,1]       [,2]\n2.5%  -3.820226 -0.5319227\n50%    5.809501  1.5967318\n97.5% 13.564890  4.1240734\n\n# Compare\nci_mat\n\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n\n# Plot the relationship between intercept and slope\n# Notice the covariance\nplot(test_mat_t, xlab = \"Intercept\", ylab = \"Slope\")\n\n\n\n\nNext, for each pair of intercept and slope randomly drawn above, we will calculate the expected values of \\(Y\\) across the range of covariate \\(x\\). We will then summarize the 95% quantile of expected \\(Y\\) at each value of \\(x\\) in this interpolation. To do this, we need a function to calculate the expected value of \\(Y\\). This function will have the intercept and slope as inputs and will output the expected values of \\(Y\\) across a range of \\(x\\). Then, we will apply() this function using all of the values of intercept and slope, in a vectorized and therefore very efficient manner, rather than using any for loops.\n\n# Create a matrix that holds the values of x\n# over which we want to interpolate the expected\n# values of Y\nx_fake_mat = \n  cbind(\n    rep(1, times = 100),\n    seq(0,max(x1),length.out = 100)\n  )\n\n# Create a function that will calculate the expected values\ny_hat_fun = function(x, x_mat){\n  x_mat %*% x\n}\n\n# Apply this function to all intercepts and slopes that\n# we drew from the multivariate t\ny_pred_mt = apply(test_mat_t, 1, y_hat_fun, x_mat=x_fake_mat)\n\n# Summarize the 95% quantile of the expected value of Y\n# at each value of x \ny_pred_mt_summary = apply(y_pred_mt, 1, quantile, probs = c(0.025, 0.975))\nstr(y_pred_mt_summary)\n\n num [1:2, 1:100] -3.82 13.56 -3.62 13.58 -3.41 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"2.5%\" \"97.5%\"\n  ..$ : NULL\n\n\n\n4.10.2 predict() function method\nR has a built-in function predict() (see specific variant predict.lm()) which calculates expected values of the dependent variable from a linear regression model estimated using the function lm().\n\n# Note that 'newdata' must be a data frame that includes the ranges\n# of each covariate in the regression model for which you want \n# to generate interpolated or predicted values of the dependent variable\n\n# Here we are calculated the expected values as well as the \n# 95% confidence intervals for those expected values\ny_predict = predict(lm_out,\n                 newdata = data.frame(x1 = c(x_fake_mat[,2])),\n                 interval = \"confidence\", level = 0.95)\nstr(y_predict)\n\n num [1:100, 1:3] 5.77 5.85 5.94 6.02 6.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:3] \"fit\" \"lwr\" \"upr\"\n\n\n\n4.10.3 Compare the two methods\nLet’s visualize the output of the two methods to compare.\n\n# plot\nplot(x=NA,y=NA,xlab = \"x\", ylab = \"y\",\n     xlim = c(0,max(x1)), ylim = c(-5, 25), pch = 19)\n# Plot the expected values of Y for each pair of int/slope \nfor(i in 1:n_samp){\n  lines(y_pred_mt[,i] ~ x_fake_mat[,2],\n        # Reduce the opacity of each line\n        col = scales::alpha(\"black\", alpha = 0.1), lwd = 2)\n}\n# Add the data points\npoints(y ~ x1, col = 'orange', pch = 19, cex = 2)\n# Add the expected values of Y from \\hat{B}\nabline(coef = lm_coef, col = \"orange\", lwd = 3)\n# Add the conf int of expected Y using multivariate t\nlines(y_pred_mt_summary[1,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\nlines(y_pred_mt_summary[2,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\n# Add the conf int of expected Y using predict() function\nlines(y_predict[,\"lwr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\nlines(y_predict[,\"upr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\n\n\n\n\nThere is yet a third option to calculate the uncertainty in predicted (i.e., interpolated or extrapolated) values of \\(Y\\), which is to derive an exact calculation of the confidence interval using the \\(t\\) distribution, similar to that shown in Section 4.9. See Ch4.1 of Dr. Barber’s book for this derivation."
  }
]