[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INF511: Modern Regression I",
    "section": "",
    "text": "Welcome to INF511: Modern Regression I. In this course, we will do a deep dive into three fundamental methods for estimating the linear relationships between random variables (i.e., linear regression analysis): ordinary least squares (Chapter 4), maximum likelihood (Chapter 6), and Bayesian inference (Chapter 9). We will also explore null hypothesis testing (Chapter 5), and linear models with categorical covariates (i.e., ANOVA, Chapter 8). This online book serves as a living document of resources for our class. The chapters provide links to lecture materials, which should be downloaded and printed prior to class, as well as links to recorded lectures. Each chapter also has material that is supplemental to lecture, with coded examples. We will often refer to these examples during class time, and they will be helpful for solving problem set and homework assignments. Problem sets will have dedicated in-class time, whereas homework assignments will be conducted entirely outside of class time.\nPlease refer to the Syllabus (Appendix A) for the course schedule, learning objectives, grading structure, course policies, etc."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "INF511: Modern Regression I",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nThis website is published using Github Pages.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "1  Software",
    "section": "",
    "text": "You will need to have all of the following free software downloaded and in working order on your laptop.\n\n\n\n\n\n\nPrior to first lecture\n\n\n\nYou must have the following on your laptops prior to the first lecture.\n\n\n\nLatest version of RStudio Desktop IDE\nCompatible version of R software environment\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\n\n\n\n\n\n\n\nPrior to Bayesian inference\n\n\n\nYou must have the following on your laptops prior to the sections on Bayesian inference.\n\n\n\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function."
  },
  {
    "objectID": "Rintro.html#load-a-package",
    "href": "Rintro.html#load-a-package",
    "title": "2  Introduction to R",
    "section": "\n2.1 Load a package",
    "text": "2.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "Rintro.html#load-a-data-set",
    "href": "Rintro.html#load-a-data-set",
    "title": "2  Introduction to R",
    "section": "\n2.2 Load a data set",
    "text": "2.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own."
  },
  {
    "objectID": "Rintro.html#manipulate-and-visualize-the-data",
    "href": "Rintro.html#manipulate-and-visualize-the-data",
    "title": "2  Introduction to R",
    "section": "\n2.3 Manipulate and visualize the data",
    "text": "2.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character"
  },
  {
    "objectID": "Rintro.html#is-there-a-linear-association",
    "href": "Rintro.html#is-there-a-linear-association",
    "title": "2  Introduction to R",
    "section": "\n2.4 Is there a linear association?",
    "text": "2.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\n\n\n\n\n\n\nTip\n\n\n\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from a well-informed standpoint.\n\n\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\nData with fitted linear relationship."
  },
  {
    "objectID": "prob.html#lecture-material",
    "href": "prob.html#lecture-material",
    "title": "\n3  Probability distributions\n",
    "section": "\n3.1 Lecture material",
    "text": "3.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "prob.html#gaussian-normal-distribution",
    "href": "prob.html#gaussian-normal-distribution",
    "title": "\n3  Probability distributions\n",
    "section": "\n3.2 Gaussian (Normal) distribution",
    "text": "3.2 Gaussian (Normal) distribution\nAs we learned in lecture, the normal distribution is defined by two parameters, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). Here, we will use the normal distribution to demonstrate some of R’s functions to describe probability distributions and to draw random numbers from probability distributions. Let’s assume that random variable \\(x\\) follows a normal distribution, \\(x_i \\sim N(\\mu, \\sigma)\\).\n\n# Define the parameters\nmu = 10\nsigma = 2.5\n\n# Visualize the probability density function (pdf)\nx_vals = seq(0, 50, by = 0.1)\nnorm_pdf = dnorm(x_vals, mean = mu, sd = sigma)\n\n# Let's use some of the other R functions to describe the distribution\n\n## What is the probability density of specific values?\n## mean\np_mu = dnorm(mu, mean = mu, sd = sigma)\n## The next two values will describe the 95% probability density bounds\n## (Low) 2.5% cut off \nx_low95 = qnorm(0.025, mean = mu, sd = sigma)\np_low95 = dnorm(x_low95, mean = mu, sd = sigma)\n## (High) 97.5% cut off\nx_high95 = qnorm(0.975, mean = mu, sd = sigma)\np_high95 = dnorm(x_high95, mean = mu, sd = sigma)\n\n# So, what is the P(x <= x_high95)??\npnorm(x_high95, mean = mu, sd = sigma)\n\n[1] 0.975\n\n\n\n## Plot the pdf with segments\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"x\", ylab = expression(\"P(x |\"~mu~\",\"~sigma~\")\"))\nlines(norm_pdf ~ x_vals)\nsegments(x0 = c(x_low95, mu, x_high95), x1 = c(x_low95, mu, x_high95),\n         y0 = rep(0, times = 3), y1 = c(p_low95, p_mu, p_high95))\n# Now, let's draw random samples from this normal distribution\nn_rand = 1000\nx_rand = rnorm(n_rand, mean = mu, sd = sigma)\n\n# Plot a histogram and overlay the approximate expectations\n## The line below assumes you draw 'n_rand' samples\nhist(x_rand, breaks = 20, main = \"\")\nlines(norm_pdf*n_rand ~ x_vals)"
  },
  {
    "objectID": "prob.html#multivariate-normal-distribution",
    "href": "prob.html#multivariate-normal-distribution",
    "title": "\n3  Probability distributions\n",
    "section": "\n3.3 Multivariate normal distribution",
    "text": "3.3 Multivariate normal distribution\n\n3.3.1 Relation to residuals, \\(\\epsilon\\)\n\nRecall our linear model in matrix notation: \\(Y = XB + \\epsilon\\). We use the multivariate normal distribution to describe the probability density of the residuals, \\(\\epsilon\\). Recall that each individual residual, \\(\\epsilon_i\\) follows a normal distribution with mean zero and standard deviation equal to the residual error, \\(\\sigma\\): \\(\\epsilon_i \\sim N(0, \\sigma)\\). Also recall that the linear regression analysis assumes that \\(\\epsilon_i\\) are I.I.D. (independent and identically distributed). \\(\\epsilon_i \\sim N(0, \\sigma)\\) implies the identical distribution (i.e., each residual follows the same normal distribution). The “independent” part means that the residual values are not correlated in any way, meaning that they do not covariance is zero. Thus, we can use vector notation to say that the vector \\(\\epsilon\\) follows a multivariate normal distribution with all means equal to zero and covariance matrix \\(\\Sigma = \\sigma^2 I\\), where \\(I\\) is a square identity matrix: \\(\\epsilon \\sim N(0, \\sigma^2 I)\\). More about covariance and covariance matrices is available below (Footnotes 3.5.1).\nThe multivariate normal probability distribution is hard to visualize, because it is in multiple dimensions. But we can use similar R functions to understand the distribution. These functions are not in the base installation of R, so we need another package, MASS. We’ll also need the Matrix package later.\n\n# Install packages if you don't already have them, e.g., \n# install.packages(\"MASS\", dependencies = TRUE)\nlibrary(MASS)\nlibrary(Matrix)\n\n# Define mean and st.dev.\nmu_epsilon = 0\nsigma_epsilon = 2.0\n\n# sample size\nn_resid = 1000\n\n# we need a vector of means\nmu_vec = rep(0, n_resid)\n\n# we need an identity matrix\nI_mat = matrix(0, nrow = n_resid, ncol = n_resid)\n## specify the diagonal = 1\ndiag(I_mat) = 1\n\n# Draw randomly from the multivariate normal\nmvn_epsilon = MASS::mvrnorm(n = 1, \n                            mu = mu_vec,\n                            Sigma = sigma_epsilon^2*I_mat)\n# We can see that an entire array of size n_resid is drawn\nstr(mvn_epsilon)\n\n num [1:1000] 1.604 -2.189 -0.414 1.649 -1.294 ...\n\n# How does this compare to drawing them independently?\nnorm_epsilon = rnorm(n_resid, mean = mu_epsilon, sd = sigma_epsilon)\nc(mean(mvn_epsilon), mean(norm_epsilon))\n\n[1] -0.06196956  0.03755476\n\nc(sd(mvn_epsilon), sd(norm_epsilon))\n\n[1] 2.011022 2.002009\n\n\n\n# Compare these two vectors visually:\nhist(mvn_epsilon)\nhist(norm_epsilon)\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Multivariate normal distribution with non-independent variables\nLet’s explore the multivariate normal a bit more. Suppose we have three random variates \\(a\\), \\(b\\), and \\(c\\). Suppose further that \\(a\\) and \\(b\\) are positively correlated with each other, but \\(c\\) is not correlated with either other variate.\n\n# Establish means and variances of a, b, and c\nmu_vec = c(1.0, 2.2, 1.5)\nsd_vec = c(1.5, 0.5, 0.75)\n\n# Manually construct the covariance matrix:\ncov_mat_test = matrix(\n    data = c(0.0, 0.6, 0.0,\n             0.6, 0.0, 0.0,\n             0.0, 0.0, 0.0),\n    ncol = 3, nrow = 3,\n    byrow = TRUE\n)\ndiag(cov_mat_test) = sd_vec^2\n\n# Matrix must be positive definite (PD). \n# This gives closest PD\ncov_mat = Matrix::nearPD(cov_mat_test)$mat\n# Look if you want: str(cov_mat)\n\n# Draw some random vectors:\nabc_array = mvrnorm(n = 100, mu = mu_vec, Sigma = cov_mat)\n# Look at structure if you want; str(abc_array)\n\n# Visualize the relationships between a, b, and c:\ncolnames(abc_array) = letters[1:3]\npairs(abc_array)"
  },
  {
    "objectID": "prob.html#poisson-distribution",
    "href": "prob.html#poisson-distribution",
    "title": "\n3  Probability distributions\n",
    "section": "\n3.4 Poisson distribution",
    "text": "3.4 Poisson distribution\nThe normal and multivariate normal probability distributions have PDFs related to continuous random variables. In many cases our data are not continuous, but are instead discrete. The Poisson distribution represents the PMF (probability mass function) of count data and is described by a single parameter, \\(\\lambda\\), which is equal to the mean and variance of the distribution. In regression, we can use the Poisson distribution to analyze a generalized linear model between a discrete response variable (e.g., count data) and its covariates, but we will not deal with that in our class.\n\n# Define the parameter\nlambda = 8\n\n# Visualize the probability density function (pdf)\n## Remember this is a discrete distribution\nk_vals = c(0:20)\npois_pdf = dpois(k_vals, lambda = lambda)\n\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"k\", ylab = expression(\"P(k |\"~lambda~\")\"))\npoints(pois_pdf ~ k_vals)\nsegments(x0 = k_vals, x1 = k_vals,\n         y0 = 0, y1 = pois_pdf)\n## Compare to randomly drawn values:\nk_rand = rpois(n_rand, lambda = lambda)\nhist(k_rand, breaks = 25, main = \"\")\npoints(pois_pdf*n_rand ~ k_vals, pch = 19)\n# On your own, use the ppois() and qpois() functions to understand their inputs/outputs"
  },
  {
    "objectID": "prob.html#footnotes",
    "href": "prob.html#footnotes",
    "title": "\n3  Probability distributions\n",
    "section": "\n3.5 Footnotes",
    "text": "3.5 Footnotes\n\n3.5.1 Covariance matrix\nAs reminder, the variance of a random variable, \\(x\\), with sample size \\(n\\) is: \\[\\sigma^2_x = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(x_i - \\bar{x}) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})^2.\\] And \\(\\bar{x}\\) is the sample mean. Similarly, then, the covariance of samples from two random variables, \\(x\\) and \\(y\\), can be calculated as: \\[\\sigma(x,y) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y}).\\] The syntax for the covariance of a sample population with itself is, for example, \\(\\sigma(x, x)\\), which is simply equal to the variance \\(\\sigma_x^2\\). The covariance matrix for these two sample populations would be: \\[C = \\begin{bmatrix}\n\\sigma(x,x) & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma(y,y)\n\\end{bmatrix}.\\] This can be simplified using the variance notation: \\[C = \\begin{bmatrix}\n\\sigma^2_x & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma^2_y\n\\end{bmatrix}.\\]"
  },
  {
    "objectID": "ols.html#lecture-material",
    "href": "ols.html#lecture-material",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.1 Lecture material",
    "text": "4.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "ols.html#in-class-code",
    "href": "ols.html#in-class-code",
    "title": "4  Ordinary Least Squares",
    "section": "In-class Code",
    "text": "In-class Code\nRemember that our goal is to estimate the linear relationship between data observations of response variable, \\(y\\), and its measured covariate, \\(x\\), following: \\(Y = XB + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2 I).\\) Our coefficients to estimate are therefore \\(\\hat{B}\\), which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), \\(\\hat{\\sigma}\\). To estimate the coefficients, we are attempting to minimize the residual sum of squares, \\(|| \\epsilon || ^ 2\\). See Footnotes 4.12.1 for more information regarding this notation."
  },
  {
    "objectID": "ols.html#generate-the-data",
    "href": "ols.html#generate-the-data",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.2 Generate the data",
    "text": "4.2 Generate the data\nWe’ll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we’ll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    1    5\n[4,]    1    1\n\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n\n     [,1]\n[1,]    8\n[2,]   10\n[3,]   14\n[4,]    6\n\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon"
  },
  {
    "objectID": "ols.html#plot-the-relationship",
    "href": "ols.html#plot-the-relationship",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.3 Plot the relationship",
    "text": "4.3 Plot the relationship\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)"
  },
  {
    "objectID": "ols.html#sec-lm-output",
    "href": "ols.html#sec-lm-output",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.4 Estimate the coefficients using R’s lm() function",
    "text": "4.4 Estimate the coefficients using R’s lm() function\n\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ 1 + x1)\n\nResiduals:\n     1      2      3      4 \n-1.029 -1.657  1.086  1.600 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   5.7714     2.0500   2.815    0.106\nx1            1.6286     0.6565   2.481    0.131\n\nResidual standard error: 1.942 on 2 degrees of freedom\nMultiple R-squared:  0.7547,    Adjusted R-squared:  0.6321 \nF-statistic: 6.153 on 1 and 2 DF,  p-value: 0.1313\n\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n\n(Intercept)          x1 \n   5.771429    1.628571"
  },
  {
    "objectID": "ols.html#estimate-the-coefficients-manually",
    "href": "ols.html#estimate-the-coefficients-manually",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.5 Estimate the coefficients manually",
    "text": "4.5 Estimate the coefficients manually\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, \\(Y\\). Remember that we estimate the coefficient vector, \\(\\hat{B}\\) from: \\[X^TX \\hat{B} = X^T Y\\] \\[\\hat{B} = (X^TX)^{-1} X^T Y\\] These equations include the multiplicative inverse matrix, \\((X^TX)^{-1}\\). See the Footnotes 4.12.2 for more information about inverse matrices and the solve() function.\n\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?"
  },
  {
    "objectID": "ols.html#sec-est-plot",
    "href": "ols.html#sec-est-plot",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.6 Plot the estimated relationships",
    "text": "4.6 Plot the estimated relationships\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)"
  },
  {
    "objectID": "ols.html#why-are-the-hatb-different-from-true-b",
    "href": "ols.html#why-are-the-hatb-different-from-true-b",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?",
    "text": "4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), \\(|| \\epsilon ||^2\\).\n\n# True sum of squares:\nsum(epsilon)^2\n\n[1] 9\n\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals)^2\n\n[1] 0\n\n## From manual OLS\nsum( (y - xmat %*% bhat_solve) )^2\n\n[1] 7.099748e-30\n\n\nYou can see that the OLS strategy effectively minimized the SSE to zero."
  },
  {
    "objectID": "ols.html#understanding-uncertainty-in-hatb",
    "href": "ols.html#understanding-uncertainty-in-hatb",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.8 Understanding Uncertainty in \\(\\hat{B}\\)\n",
    "text": "4.8 Understanding Uncertainty in \\(\\hat{B}\\)\n\nWhile the OLS analysis estimates the regression coefficients, \\(\\hat{B}\\), from the observed data \\(Y\\), our estimates of these coefficients have error (i.e., uncertainty), such that the estimates are only as good as the data. Specifically, if we have fewer data points (i.e., \\(n\\) is low), we have less certainty in \\(\\hat{B}\\). In lecture, we showed, that: \\[\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right), \\] and we know that \\(\\hat{\\sigma}^2\\) depends on sample size \\(n\\), following: \\[\\hat{\\sigma}^2 \\quad = \\quad \\frac{1}{n-p} (Y_{obs} - Y_{pred})^T (Y_{obs} - Y_{pred}) \\quad = \\quad \\frac{1}{n-p} \\hat{\\epsilon}^T \\hat{\\epsilon}\\]\nUsing these equations, we showed then that \\(SE(\\beta_i) = \\sqrt{diag\\left( (X^TX)^{-1} \\right)_i \\hat{\\sigma}^2}\\). Let’s calculate this manually and compare to the output of the lm() function.\n\n# Extract the model summary, which has useful components\nlm_out_summary = summary(lm_out)\n# Extract the estimated residual standard deviation, sigma\nest_sigma = lm_out_summary$sigma\nest_sigma\n\n[1] 1.942017\n\n# We already calculated (X^T X)^{-1} as inv_xtx\nbeta_cov_mat = inv_xtx * est_sigma^2\nbeta_cov_mat\n\n          [,1]       [,2]\n[1,]  4.202449 -1.1853061\n[2,] -1.185306  0.4310204\n\nse_beta = sqrt(diag(beta_cov_mat))\nse_beta\n\n[1] 2.0499876 0.6565214\n\n\nCompare these values to the output of the summary() of Section 4.4 in the column labelled Std. Error."
  },
  {
    "objectID": "ols.html#sec-conf-beta",
    "href": "ols.html#sec-conf-beta",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.9 Confidence Intervals for \\(\\hat{B}\\)\n",
    "text": "4.9 Confidence Intervals for \\(\\hat{B}\\)\n\nTo calculate confidence intervals for \\(\\hat{B}\\), we first must understand the \\(t\\) (a.k.a. Student’s \\(t\\)) probability distribution. This distribution represents the case when we are estimating the mean of a normally distributed variable and either the sample size is small or the variable’s standard deviation is unknown. Essentially, the \\(t\\) distribution increases the uncertainty (i.e., variance) in cases of low sample size (i.e., small \\(n\\)). With low sample size (and/or high number of parameters), the degrees of freedom of the \\(t\\)-distribution, \\(\\nu\\) is low, whereas with high sample size, \\(\\nu\\) is large. As \\(\\nu\\) approaches infinity, the \\(t\\)-distribution approximates the standard normal distribution (i.e., \\(N(\\mu, \\sigma)|\\mu=0,\\sigma=1\\)).\n\n\n\n\n\nIt is the case for \\(\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right)\\) that we do not know the mean (\\(B\\)), and we are estimating the variance, \\(\\hat{\\sigma}^2\\). Specifically, we are estimating the true mean vector, \\(B\\), as \\(\\hat{B}\\), and we are estimating the variance of the residuals as \\(\\hat{\\sigma}^2\\). We can therefore re-write the uncertainty in \\(\\hat{B}\\) as a multivariate \\(t\\) distribution: \\[(\\hat{B} - B) \\sim t_{\\nu} \\left( 0, \\Sigma \\right),\\] where the means are zero, \\(\\nu\\) is the degrees of freedom (i.e., \\(n-p\\)), and \\(\\Sigma = (X^TX)^{-1} \\hat{\\sigma}^2\\). \\((\\hat{B} - B)\\) represents the deviation of the estimated coefficients from the true coefficients, which is why the distribution is centered around zero. It is perhaps easier to separate the individual estimated coefficients, \\(\\beta_i\\), into their separate \\(t\\)-distributions: \\[\\frac{(\\hat{\\beta}_i - \\beta_i)}{SE(\\hat{\\beta}_i)} \\sim t_{\\nu}\\] \\[(\\hat{\\beta}_i - \\beta_i) \\sim t_{\\nu} SE(\\hat{\\beta}_i),\\] which shows that the \\(t\\)-distribution that describes the deviation of regression coefficients from the true value of those coefficients is scaled by the uncertainty in the estimated coefficients \\(SE(\\hat{\\beta}_i)\\). As shown in Dr. Barber’s materials, using this information, we can derive the confidence interval (at the \\(\\alpha\\) confidence level) calculation for \\(\\hat{\\beta}_i\\) as: \\[ \\hat{\\beta}_i \\pm t \\left(\\frac{1-\\alpha}{2}, \\nu \\right) SE(\\hat{\\beta}_i),\\] where the \\(t()\\) notation represents the critical value of the \\(t\\)-distribution, \\(t_{crit}\\), with \\(\\nu\\) degrees of freedom, for which \\(P(z \\le t_{crit}) = \\frac{1-\\alpha}{2}\\), and \\(z\\) is a continuous, random variable. This critical value can be calculated in R using the qt() function, which we show below.\n\n\n\n\n\n\nCovariance of \\(\\hat{\\beta}_i\\)\n\n\n\nAlthough it is convenient and easier to digest the confidence interval of individual \\(\\hat{\\beta}_i\\), we must realize that the estimates of the \\(\\beta_i\\) can covary (i.e., have non-zero covariance), which is quantified in the variance-covariance matrix of \\(\\hat{B}\\), \\((X^TX)^{-1} \\hat{\\sigma}^2\\). We will show why this is important below.\n\n\nLet’s manually calculate the 95% confidence intervals in \\(\\hat{B}\\) and compare to R’s internal function confint().\n\n# Extract the degrees of freedom from the model (\\nu)\n# which can also be calculated as n - p\nt_df = lm_out$df.residual\n\n# Calculate t critical for alpha = 0.05\n# This will give us the 95% conf interval (CI)\nt_crit = qt(1-(0.05/2), df = t_df)\n\n# Calculate the upper and lower CI for both betas\nci_int = lm_coef[1] + c(-1,1)*t_crit*se_beta[1]\nci_slope = lm_coef[2] + c(-1,1)*t_crit*se_beta[2]\n\n# Construct a table of values\nci_mat = \n    rbind(c(lm_coef[1], ci_int),\n          c(lm_coef[2], ci_slope))\ncolnames(ci_mat) = c(\"coef\", \"lowCI\", \"highCI\")\nrownames(ci_mat) = c(\"intercept\", \"slope\")\nci_mat\n\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n\n# Compare these manual calculations to built-in\n# function confint(), which by default extracts the \n# 95% CI for a lm() model's coefficients\nconfint(lm_out)\n\n                2.5 %    97.5 %\n(Intercept) -3.048956 14.591813\nx1          -1.196212  4.453355"
  },
  {
    "objectID": "ols.html#propagate-uncertainty-in-hatb-for-predictions-of-y",
    "href": "ols.html#propagate-uncertainty-in-hatb-for-predictions-of-y",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.10 Propagate uncertainty in \\(\\hat{B}\\) for predictions of \\(Y\\)\n",
    "text": "4.10 Propagate uncertainty in \\(\\hat{B}\\) for predictions of \\(Y\\)\n\nThere are several ways to calculate and visualize our uncertainty in model predictions of observed data \\(Y\\) and unobserved data of the dependent variable (i.e., interpolation). The colored lines drawn on the figure in Section 4.6 represent the expected values of \\(Y\\) based on the OLS analysis’ estimate of \\(\\hat{B}\\), but this line does not include uncertainty in these coefficient values.\n\n4.10.1 Multivariate \\(t\\)-distribution method\nFirst, we will calculate uncertainty by sampling from the multivariate \\(t\\) distribution that represents error in regression coefficients, \\(\\hat{B}\\).\n\n# We will \"bootstrap\" 1000 samples of intercept and slope\nset.seed(3)\nn_samp = 500\n\n# Draw from the multivariate t \n# which represents (\\hat{B} - B)\ntest_mat_deviates = \n  mnormt::rmt(n_samp, mean = c(0,0), S = beta_cov_mat, df = t_df)\n\n# Now calculate the realized intercept and slope\n# using the t-distributed deviates\ntest_mat_t = cbind(\n  lm_coef[1] + c(test_mat_deviates[,1]),\n  lm_coef[2] + c(test_mat_deviates[,2])\n)\n\n# Calculate the 95% quantiles and compare to the \n# calculated 95% confidence intervals from above\napply(test_mat_t, \n      MARGIN = 2, # applies function (FUN) to columns (dim 2)\n      FUN = quantile, probs = c(0.025, 0.5, 0.975))\n\n           [,1]       [,2]\n2.5%  -3.820226 -0.5319227\n50%    5.809501  1.5967318\n97.5% 13.564890  4.1240734\n\n# Compare\nci_mat\n\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n\n# Plot the relationship between intercept and slope\n# Notice the covariance\nplot(test_mat_t, xlab = \"Intercept\", ylab = \"Slope\")\n\n\n\n\nNext, for each pair of intercept and slope randomly drawn above, we will calculate the expected values of \\(Y\\) across the range of covariate \\(x\\). We will then summarize the 95% quantile of expected \\(Y\\) at each value of \\(x\\) in this interpolation. To do this, we need a function to calculate the expected value of \\(Y\\). This function will have the intercept and slope as inputs and will output the expected values of \\(Y\\) across a range of \\(x\\). Then, we will apply() this function using all of the values of intercept and slope, in a vectorized and therefore very efficient manner, rather than using any for loops.\n\n# Create a matrix that holds the values of x\n# over which we want to interpolate the expected\n# values of Y\nx_fake_mat = \n  cbind(\n    rep(1, times = 100),\n    seq(0,max(x1),length.out = 100)\n  )\n\n# Create a function that will calculate the expected values\ny_hat_fun = function(x, x_mat){\n  x_mat %*% x\n}\n\n# Apply this function to all intercepts and slopes that\n# we drew from the multivariate t\ny_pred_mt = apply(test_mat_t, 1, y_hat_fun, x_mat=x_fake_mat)\n\n# Summarize the 95% quantile of the expected value of Y\n# at each value of x \ny_pred_mt_summary = apply(y_pred_mt, 1, quantile, probs = c(0.025, 0.975))\nstr(y_pred_mt_summary)\n\n num [1:2, 1:100] -3.82 13.56 -3.62 13.58 -3.41 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"2.5%\" \"97.5%\"\n  ..$ : NULL\n\n\n\n4.10.2 predict() function method\nR has a built-in function predict() (see specific variant predict.lm()) which calculates expected values of the dependent variable from a linear regression model estimated using the function lm().\n\n# Note that 'newdata' must be a data frame that includes the ranges\n# of each covariate in the regression model for which you want \n# to generate interpolated or predicted values of the dependent variable\n\n# Here we are calculated the expected values as well as the \n# 95% confidence intervals for those expected values\ny_predict = predict(lm_out,\n                 newdata = data.frame(x1 = c(x_fake_mat[,2])),\n                 interval = \"confidence\", level = 0.95)\nstr(y_predict)\n\n num [1:100, 1:3] 5.77 5.85 5.94 6.02 6.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:3] \"fit\" \"lwr\" \"upr\"\n\n\n\n4.10.3 Compare the two methods\nLet’s visualize the output of the two methods to compare.\n\n# plot\nplot(x=NA,y=NA,xlab = \"x\", ylab = \"y\",\n     xlim = c(0,max(x1)), ylim = c(-5, 25), pch = 19)\n# Plot the expected values of Y for each pair of int/slope \nfor(i in 1:n_samp){\n  lines(y_pred_mt[,i] ~ x_fake_mat[,2],\n        # Reduce the opacity of each line\n        col = scales::alpha(\"black\", alpha = 0.1), lwd = 2)\n}\n# Add the data points\npoints(y ~ x1, col = 'orange', pch = 19, cex = 2)\n# Add the expected values of Y from \\hat{B}\nabline(coef = lm_coef, col = \"orange\", lwd = 3)\n# Add the conf int of expected Y using multivariate t\nlines(y_pred_mt_summary[1,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\nlines(y_pred_mt_summary[2,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\n# Add the conf int of expected Y using predict() function\nlines(y_predict[,\"lwr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\nlines(y_predict[,\"upr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\n\n\n\n\nThere is yet a third option to calculate the uncertainty in predicted (i.e., interpolated or extrapolated) values of \\(Y\\), which is to derive an exact calculation of the confidence interval using the \\(t\\) distribution, similar to that shown in Section 4.9. See Ch4.1 of Dr. Barber’s book for this derivation."
  },
  {
    "objectID": "ols.html#footnotes",
    "href": "ols.html#footnotes",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.12 Footnotes",
    "text": "4.12 Footnotes\n\n4.12.1 Euclidean norm & cross product\nWe often see the syntax, \\(|| a ||\\), which is the Euclidean norm of the \\(n\\)-sized vector \\(a\\): \\[|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,\\] so that when we see \\(|| a ||^2\\), this results in the sum of squares of vector \\(a\\), \\(\\sum_{i=1}^{n} a_i^2\\).\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, \\(\\epsilon_i\\), are in vector, \\(\\epsilon\\). The sum of squares of vector \\(\\epsilon\\) is therefore \\(|| \\epsilon ||^2\\). Algebraically, we can find this value as the cross product of \\(\\epsilon\\), which is \\(\\epsilon^{T}\\epsilon\\). Let’s do a coded example with vector \\(x\\).\n\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n\n[1] 30\n\n# Evaluated as cross-product\nt(x) %*% x\n\n     [,1]\n[1,]   30\n\n## Or with crossprod()\ncrossprod(x,x)\n\n     [,1]\n[1,]   30\n\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n\n[1] 30\n\n\n\n4.12.2 solve() and Inverse of matrix\nSuppose we have matrices \\(A\\), \\(X\\), and \\(B\\), and the following expression is true: \\[AX=B.\\]\nThen, suppose \\(X\\) is unknown, such that we want to find the solution for \\(X\\), when we rearrange: \\[X = A^{-1} B,\\] where \\(A^{-1}\\) is the multiplicative inverse of matrix \\(A\\). To figure this out computationally, we can use the solve() function in R, as long as \\(A\\) is a square matrix and has an inverse.\n\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\nWe can see, then, that solve() is internally evaluating \\(A^{-1}\\). Remember that \\(A^{-1}\\) is not trivial to calculate, as it is the matrix that must satisfy: \\(AA^{-1} = I\\), where \\(I\\) is an identity matrix. In fact, solve(A) returns the inverse of \\(A\\), if it exists.\n\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\n\n     [,1]\n[1,]    2\n[2,]    3\n\nX\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\n\n4.12.3 Multiple linear regression with a categorical input\nLet’s simulate a case in which we have one categorical input variable that takes on values “low”, “medium”, and “high”, and one continuous input variable.\n\nset.seed(7)\nn=90\nsigma = 0.8\n\n# Xmatrix\n## Intercept\nx0 = rep(1, times = n) \n## Categorical input variable\n### Note that we need to code this as \"0\" \"1\" \"2\" to \n### simulate our outcome variable \"y\"\nx1 = rep(c(0,1,2), each=n/3)\nx1L = factor(x1, labels = c(\"low\", \"med\", \"high\"))\n## Continuous input variable\nx2 = rnorm(n, 0, 2.5)\nxmat = cbind(x0,x1,x2)\nhead(xmat)\n\n     x0 x1        x2\n[1,]  1  0  5.718118\n[2,]  1  0 -2.991929\n[3,]  1  0 -1.735731\n[4,]  1  0 -1.030732\n[5,]  1  0 -2.426683\n[6,]  1  0 -2.368200\n\n# Intercept and 2 slopes\nbetas=c(1.5, 1.2, -1.5)\n\n# Simulate outcome variable, as usual\ny2 = xmat %*% betas + rnorm(n,0,sigma)\n\n# Plot the relationships\npar(mfrow=c(1,2))\nplot(y2~x1)\nplot(y2~x2)\n\n\n\n# Run the model\n## Note that we us the \"factor\" input variable\n## \"x1L\", which has \"levels\"\nm_cat = lm(y2 ~ 1 + x1L + x2)\nsummary(m_cat)\n\n\nCall:\nlm(formula = y2 ~ 1 + x1L + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9182 -0.5032  0.1465  0.5061  1.2139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.66504    0.13741  12.117  < 2e-16 ***\nx1Lmed       1.15971    0.19113   6.068 3.38e-08 ***\nx1Lhigh      2.29169    0.19170  11.954  < 2e-16 ***\nx2          -1.51229    0.03275 -46.171  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7314 on 86 degrees of freedom\nMultiple R-squared:  0.9674,    Adjusted R-squared:  0.9663 \nF-statistic: 851.1 on 3 and 86 DF,  p-value: < 2.2e-16\n\ncoef(m_cat)\n\n(Intercept)      x1Lmed     x1Lhigh          x2 \n   1.665040    1.159710    2.291690   -1.512293 \n\n\nHow do we interpret the slopes, because we see there is a separate slope for x1Lmed and x1Lhigh? We can understand better by seeing how the linear model addes up. For instance, what is the expected value of the outcome variable when \\(x_1\\) is high, and \\(x_2 = 2.0\\)?\n\n## Using m1_binL:\ny2_pred = \n    1*coef(m_cat)[1] + # Global average (intercept)\n    0*coef(m_cat)[2] + # Not \"med\"\n    1*coef(m_cat)[3] + # Yes \"high\"\n    2.0*coef(m_cat)[4] # x2=2.0 * slope\nas.numeric(y2_pred)\n\n[1] 0.9321446\n\n\nWhen we assigned the slope of the categorical input variable as \\(1.2\\), remember this is the expected change in \\(y\\) as the input variable changes by a value of \\(1.0\\). In the model, we code the \\(x_1\\) variable as taking numerical values \\(0\\), \\(1\\), and \\(2\\) to represent categories, “low”, “medium”, and “high”. So, the slope for x1med is the expected change in \\(y\\) as the input variable changes from “low” to “medium”, an effective change of \\(1.0\\). Then, the slope for x1high is the expected change in \\(y\\) as the input variable changes from “low” to “high”, an effective change of \\(2.0\\); hence, this slope is estimated as \\(2.29\\), with standard error \\(0.19\\). Notice how this slope is approximately twice our “known” slope for the input variable, which was \\(1.2\\)."
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "max-lik.html",
    "href": "max-lik.html",
    "title": "6  Maximum Likelihood",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "8  ANOVA",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "9  Bayesian inference",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "syllabus.html#bblearn-collaborate-ultra-recorded-lectures",
    "href": "syllabus.html#bblearn-collaborate-ultra-recorded-lectures",
    "title": "Appendix A: Syllabus",
    "section": "\nA.1 BbLearn, Collaborate Ultra & Recorded Lectures",
    "text": "A.1 BbLearn, Collaborate Ultra & Recorded Lectures\nWe will use the learning management system, BbLearn, to conduct some course business, including assignment disbursement and submitting. We will use BbLearn’s Collaborate Ultra to record lectures for future viewing."
  },
  {
    "objectID": "syllabus.html#inf511-book-website",
    "href": "syllabus.html#inf511-book-website",
    "title": "Appendix A: Syllabus",
    "section": "\nA.2 INF511 Book Website",
    "text": "A.2 INF511 Book Website\nWe have compiled a course website and associated GitHub repository. This website will have coded examples that we will walk through in class, and which will be required reading. There will be other required reading materials (see Section A.8)."
  },
  {
    "objectID": "syllabus.html#course-purpose",
    "href": "syllabus.html#course-purpose",
    "title": "Appendix A: Syllabus",
    "section": "\nA.3 Course Purpose",
    "text": "A.3 Course Purpose\nINF 511 Modern Regression I is the first course in a two-semester sequence required for the MS and PhD in Informatics and Computing (INF). (See INF 512 Modern Regression II.) These courses are designed to serve the computationally oriented statistical analysis needs of the INF graduate program. Through a series of hands-on individual or team-based assignments, students will master statistical analyses, from preparing data, exploring data using numerical and/or graphical methods, modeling data, diagnosing model assumptions, remodeling and final inference. This course will provide INF graduate students with the necessary foundation for more specialized statistical methods and applications that students will encounter in subsequent INF courses, such as INF 626 Applied Bayesian Modeling and the more prediction-oriented INF 504 Data Mining and Machine Learning. More generally, INF 511 provides skills widely applicable to analysis of data across science and engineering.\nINF 511 Modern Regression I covers fundamental probability models and their use in the analysis of independent data with linear models within both frequentist and Bayesian statistical frameworks. Random variables, expectation, variance, covariance, correlation. Joint, conditional and marginal distributions. Linear combinations of random variables; central limit theorem; matrices, vectors, basic matrix arithmetic, matrix formulation of linear statistical models (regression and ANOVA) for independent data, normal likelihood, least squares, Gauss-Markov theorem, \\(t\\) and \\(F\\) sampling distribution-based inference for linear combinations of parameters. Corresponding Bayesian analysis including prior and posterior distributions, introductory Markov chain Monte Carlo methods. Diagnostics, including graphical residual analysis. Scope of inference (randomization and causality, random sampling and population)."
  },
  {
    "objectID": "syllabus.html#course-student-learning-outcomes",
    "href": "syllabus.html#course-student-learning-outcomes",
    "title": "Appendix A: Syllabus",
    "section": "\nA.4 Course Student Learning Outcomes",
    "text": "A.4 Course Student Learning Outcomes\nThe overall learning outcome for this course is a demonstrated acquisition of skills and conceptual understanding of statistical methods to enable complete and valid statistical analyses of primarily independent data modeled with relatively traditional linear statistical models from both a frequentist and Bayesian perspective at a level commensurate with high expectations of a well-trained graduate student in the quantitatively and computationally intensive field of informatics. For these data, and using the methods and concepts detailed in the Course Purpose, students should be able to:\n\nUse numerical and graphical exploratory data analysis tools to prepare data and to develop conceptual mod- els of data\nTransform conceptual models of data into formal linear statistical models of data\nImplement linear model methods in modern software, such as R or Stan, to analyze data\nDemonstrate an understanding that models are an abstracted simplification of real processes by effectively using linear model diagnostics, remedial measures, remodeling and final model validation/confirmation methods.\nDemonstrate an understanding of the limitations of data and methods by communicating how the data and methods relate to randomization and causality, random selection and population, over-fitting and exploratory/confirmatory analyses, and the trade-off between efficiency of inference and robustness to departures from method assumptions\nDemonstrate an ability to work effectively in a team environment to solve realistic problems, which may be beyond any one individual!s ability to address, as indicated by peer review or other assessments of team- work."
  },
  {
    "objectID": "syllabus.html#sec-assess",
    "href": "syllabus.html#sec-assess",
    "title": "Appendix A: Syllabus",
    "section": "\nA.5 Assessments of Course Student Learning Outcomes",
    "text": "A.5 Assessments of Course Student Learning Outcomes\nThere will be three assessment strategies: problem sets, homework, and quizzes. Problem sets will have dedicated in-class time to complete, whereas homework assignments will be done entirely outside of class. Problem sets will be completed individually, and will be primarily assessed as complete/incomplete, whereas Homework assignments can be completed in teams of one to three students and will be graded in full. Assignment format is designed in part to mimic and reinforce the similar presentation of analyses in class/notes and to encourage discussion among students. There will be five in-class Quizzes, one every $$3 weeks, in lieu of exams. Attendance & Participaiton will also be used to assess your course performance."
  },
  {
    "objectID": "syllabus.html#grading-system",
    "href": "syllabus.html#grading-system",
    "title": "Appendix A: Syllabus",
    "section": "\nA.6 Grading System",
    "text": "A.6 Grading System\n\n\nProblem Sets\nHomework\nQuizzes\nAttendance\n\n\n20%\n35%\n40%\n5%\n\n\n\n\nAttendance & Participation. In-person attendance is required. See University Policy on excused absences. You are responsible to plan with your fellow classmates to obtain in-class material not received due to your absence. (Recall that lectures will be recorded and will be available in BbLearn.) Participation in the form of responding to questions in class, asking questions, and attending office hours may be used to determine “borderline” grade cases.\n\n\n\n\n\n\n\nPrior notification of absence\n\n\n\nA student must notify a TA prior to absence. Students should notify a TA of an upcoming absence via email, and the TA will evaluate whether the absence will be counted as excused or unexcused. Only students with excused absences will be allowed to attend lecture virtually, via Bblearn Collaborate Ultra.\n\n\n\n\nAssignments. There are two categories of assignments, Problem Sets and Homework. See Section A.5 for the distinctions. See Section A.9 for due-dates. Assignments will be posted periodically via BbLearn. Assignments are to be submitted electronically, via BbLearn, on the due date/time indicated in BbLearn. For Homework, students are free to arrange different teams for different homeworks; this will not be monitored. For Homework, please ensure that all team member names are on a submitted assignment. Everyone on a team will submit exactly same homework; only one team member’s submission will be selected and graded with all members receiving the same score.\n\n\n\n\n\n\n\nLate assignments\n\n\n\nLate assignments will not be accepted. Late assignments will receive zero points.\n\n\n\n\nQuizzes. There will be five in-class quizzes that are all cumulative. Each quiz will be designed to take approximately 15-20 minutes, and each will be scaled to 100 points so that they are equally weighted.\n\n\n\n\n\n\n\nIn-class quizzes\n\n\n\nIf a student does not attend a class when a quiz is given, that quiz will receive zero points. The only exception is if a student notifies the TA, before class, of an impending absence. The absence must be formally excused in writing by the TA before class for a make-up quiz to be considered. Therefore, the notification of absence must be received at least several hours prior to class time."
  },
  {
    "objectID": "syllabus.html#course-grades",
    "href": "syllabus.html#course-grades",
    "title": "Appendix A: Syllabus",
    "section": "\nA.7 Course Grades",
    "text": "A.7 Course Grades\nOverall course grades will follow a typical scale:\n\n\nTo earn the letter grade ->\nA\nB\nC\nD\nF\n\n\nYou need at least this score\n90\n80\n70\n60\n0\n\n\nWhile you should be able to compute an estimate of your current grade using the information above, I (or your TA) will attempt to use the Grade feature in BbLearn so that you are able to check your grades. Grading mistakes may occur, and students are encouraged to discuss such concerns with the instructor or TA during office hours or by appointment."
  },
  {
    "objectID": "syllabus.html#sec-readings",
    "href": "syllabus.html#sec-readings",
    "title": "Appendix A: Syllabus",
    "section": "\nA.8 Readings and Materials",
    "text": "A.8 Readings and Materials\n\nLecture Materials: Lecture topics, course notes, readings, and assignments will be made available as the semester progresses. For each lecture, a document will be linked on the course website. During lecture, we will work together to fill out this document with written notes presented on the (virtual) whiteboard. Therefore, it is essential to downlaod and print these materials prior to attending class.\n\nRequired Text:\n\nBarber, J.J. (2022). INF511 Modern Regression I - Lecture Slides. Posted on Bblearn. Referred to as JB in Section A.9.\nMihaljevic, J (2023). INF511 Modern Regression I - Online Book. https://joseph-mihaljevic.github.io/inf511-book/. Referred to as JM in Section A.9.\nFaraway, J.J. (2014). Linear Models with R (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b17144 Referred to as FAR in Section A.9.\n\n\n\n\nComputing. Each student must bring their laptop to class with the following (freely available) software pre-installed:\n\nLatest version of RStudio Desktop IDE\n\nCompatible version of R software environment\n\n\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function."
  },
  {
    "objectID": "syllabus.html#sec-schedule",
    "href": "syllabus.html#sec-schedule",
    "title": "Appendix A: Syllabus",
    "section": "\nA.9 Living course schedule",
    "text": "A.9 Living course schedule\nThis schedule will be consistently updated throughout the course. Check back often.\n\n\n\n\n\n Week \n    Date \n    Topic \n    Reading_Due \n    Assign_Due \n    Quiz \n  \n\n\n Week 1 \n    16-Jan \n    MLK Holiday - NO CLASS \n     \n     \n     \n  \n\n Week 1 \n    18-Jan \n    SNOW CLOSURE - NO CLASS \n     \n     \n     \n  \n\n Week 2 \n    23-Jan \n    Introduction \n    Syllabus, JM(Ch1&2), JB(1-20, AppA) \n     \n     \n  \n\n Week 2 \n    25-Jan \n    Probability distributions \n    JB(pg1-20), JM(Ch3), FAR(Ch1) \n    PS-0 \n     \n  \n\n Week 3 \n    30-Jan \n    Probability distributions \n     \n     \n     \n  \n\n Week 3 \n    1-Feb \n    Least Squares \n    JB(pg20-38, AppB), JM(Ch4) \n    PS-1, HW-1 \n     \n  \n\n Week 4 \n    6-Feb \n    Least Squares \n    FAR(Ch2) \n     \n    Quiz 1 \n  \n\n Week 4 \n    8-Feb \n    Least Squares \n    JB(pg41-60) \n    PS-2 \n     \n  \n\n Week 5 \n    13-Feb \n    Least Squares \n     \n    HW-2 \n     \n  \n\n Week 5 \n    15-Feb \n    Least Squares \n    JB(pg157-170), FAR(Ch3) \n     \n     \n  \n\n Week 6 \n    20-Feb \n    Hypothesis Testing \n    JB(pg61-79), JM(Ch5), FAR(Ch10) \n    PS-3 \n     \n  \n\n Week 6 \n    22-Feb \n    SNOW CLOSURE - NO CLASS \n     \n     \n     \n  \n\n Week 7 \n    27-Feb \n    Hypothesis Testing \n    JB(pg80-127), FAR(Ch4) \n     \n    Quiz 2 \n  \n\n Week 7 \n    1-Mar \n    SNOW CLOSURE - NO CLASS \n     \n    HW-3 \n     \n  \n\n Week 8 \n    6-Mar \n    Maximum Likelihood \n    JB(pg38-40), JM(Ch6) \n     \n     \n  \n\n Week 8 \n    8-Mar \n    Maximum Likelihood \n    FAR(Ch6), JB(skimCh6) \n     \n     \n  \n\n Week 9 \n    13-Mar \n    Spring Break - NO CLASS \n     \n     \n     \n  \n\n Week 9 \n    15-Mar \n    Spring Break - NO CLASS \n     \n     \n     \n  \n\n Week 10 \n    20-Mar \n    Maximum Likelihood \n    FAR(Ch6), JB(skimCh6) \n     \n     \n  \n\n Week 10 \n    22-Mar \n    Maximum Likelihood \n    FAR(Ch7) \n    PS-4 \n     \n  \n\n Week 11 \n    27-Mar \n    Maximum Likelihood \n    FAR(Ch8) \n    HW-4 \n    Quiz 3 \n  \n\n Week 11 \n    29-Mar \n    Model Comparison \n    JB(refreshCh3.2), FAR(Ch10) \n     \n     \n  \n\n Week 12 \n    3-Apr \n    Model Comparison \n    FAR(Ch11) \n    PS-5 \n     \n  \n\n Week 12 \n    5-Apr \n    Model Comparison \n    JB(pg170-223) \n     \n     \n  \n\n Week 13 \n    10-Apr \n    ANOVA \n    JB(Ch14), JM(Ch7), FAR(Ch14) \n    HW-5 \n     \n  \n\n Week 13 \n    12-Apr \n    ANOVA \n    JB(Ch15.1-15.3) \n     \n     \n  \n\n Week 14 \n    17-Apr \n    ANOVA \n    JB(Ch15.4-15.5), FAR(Ch15) \n    PS-6 \n    Quiz 4 \n  \n\n Week 14 \n    19-Apr \n    ANOVA \n    FAR(Ch16) \n     \n     \n  \n\n Week 15 \n    24-Apr \n    Bayesian Inference \n     \n    HW-6 \n     \n  \n\n Week 15 \n    26-Apr \n    Bayesian Inference \n     \n    PS-7 \n     \n  \n\n Week 16 \n    1-May \n    Bayesian Inference \n     \n     \n     \n  \n\n Week 16 \n    3-May \n    Bayesian Inference \n     \n    HW-7 \n    Quiz 5"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Appendix A: Syllabus",
    "section": "\nA.10 Course Policies",
    "text": "A.10 Course Policies\n\nStudents are encouraged to attend the office hours of the TAs and the instructors. If a student cannot attend regular office hours with the instructors, an appointment may be considered if made via email with sufficient advanced notice.\nEmail addressed to the instructors and TAs must be respectful and professional. The TAs and instructors will respond to emails promptly, within 2 business days. The TAs and instructors will generally not respond to emails on weekends or late in the evenings, so please plan accordingly.\nCheating, including plagiarism of writing or computer code, will not be tolerated. All academic integrity violations are treated seriously. Academic integrity violations will result in penalties including, but not limited to, a zero on the assignment, a failing grade in the class, or expulsion from NAU. The University’s Academic Integrity policies (Section A.11) will be strictly enforced.\nThe paramount policy of this course is that each student is required to demonstrate respect towards their peers and the instructor. The behavior of the instructor is held to the same standard. Students and instructors come from all walks of life, and may identify with a variety of ethnic, racial, religious, gender and sexual identities. Diversity of thought and perspective enhances our science.\nAttendance is required and repeated, unexcused absences may affect the student’s grade.\nThe instructor will not provide copies of course notes. These materials should be sought from the students’ peers or by watching the recorded lectures.\nElectronic device usage must support learning in the class. All cell phones, PDAs, music players and other entertainment devices must be turned off (or put on silent) during lecture.\nGrades will be entered in BbLearn. Please check LOUIE for your final grade."
  },
  {
    "objectID": "syllabus.html#sec-univ-policy",
    "href": "syllabus.html#sec-univ-policy",
    "title": "Appendix A: Syllabus",
    "section": "\nA.11 University Policies",
    "text": "A.11 University Policies\n\nA.11.1 COVID-19 REQUIREMENTS AND INFORMATION\nAdditional information about the University’s response to COVID-19 is available from the Jacks are Back! web page located at https://nau.edu/jacks-are-back.\n\nA.11.2 ACADEMIC INTEGRITY\nNAU expects every student to firmly adhere to a strong ethical code of academic integrity in all their scholarly pursuits. The primary attributes of academic integrity are honesty, trustworthiness, fairness, and responsibility. As a student, you are expected to submit original work while giving proper credit to other people’s ideas or contributions. Acting with academic integrity means completing your assignments independently while truthfully acknowledging all sources of information, or collaboration with others when appropriate. When you submit your work, you are implicitly declaring that the work is your own. Academic integrity is expected not only during formal coursework, but in all your relationships or interactions that are connected to the educational enterprise. All forms of academic deceit such as plagiarism, cheating, collusion, falsification or fabrication of results or records, permitting your work to be submitted by another, or inappropriately recycling your own work from one class to another, constitute academic misconduct that may result in serious disciplinary consequences. All students and faculty members are responsible for reporting suspected instances of academic misconduct. All students are encouraged to complete NAU’s online academic integrity workshop available in the E-Learning Center and should review the full Academic Integrity policy available at https://policy.nau.edu/policy/policy.aspx?num=100601.\n\nA.11.3 COPYRIGHT INFRINGEMENT\nAll lectures and course materials, including but not limited to exams, quizzes, study outlines, and similar materials are protected by copyright. These materials may not be shared, uploaded, distributed, reproduced, or publicly displayed without the express written permission of NAU. Sharing materials on websites such as Course Hero, Chegg, or related websites is considered copyright infringement subject to United States Copyright Law and a violation of NAU Student Code of Conduct. For additional information on ABOR policies relating to course materials, please refer to ABOR Policy 6-908 A(2)(5).\n\nA.11.4 COURSE TIME COMMITMENT\nPursuant to Arizona Board of Regents guidance (ABOR Policy 2-224, Academic Credit), each unit of credit requires a minimum of 45 hours of work by students, including but not limited to, class time, preparation, homework, and studying. For example, for a 3-credit course a student should expect to work at least 8.5 hours each week in a 16-week session and a minimum of 33 hours per week for a 3-credit course in a 4-week session.\n\nA.11.5 DISRUPTIVE BEHAVIOR\nMembership in NAU’s academic community entails a special obligation to maintain class environments that are conductive to learning, whether instruction is taking place in the classroom, a laboratory or clinical setting, during course-related fieldwork, or online. Students have the obligation to engage in the educational process in a manner that does not interfere with normal class activities or violate the rights of others. Instructors have the authority and responsibility to address disruptive behavior that interferes with student learning, which can include the involuntary withdrawal of a student from a course with a grade of “W”. For additional information, see NAU’s Disruptive Behavior in an Instructional Setting policy at https://nau.edu/university-policy-library/disruptive-behavior.\n\nA.11.6 NONDISCRIMINATION AND ANTI-HARASSMENT\nNAU prohibits discrimination and harassment based on sex, gender, gender identity, race, color, age, national origin, religion, sexual orientation, disability, veteran status and genetic information. Certain consensual amorous or sexual relationships between faculty and students are also prohibited as set forth in the Consensual Romantic and Sexual Relationships policy. The Equity and Access Office (EAO) responds to complaints regarding discrimination and harassment that fall under NAU’s Nondiscrimination and Anti- Harassment policy. EAO also assists with religious accommodations. For additional information about nondiscrimination or anti-harassment or to file a complaint, contact EAO located in Old Main (building 10), Room 113, PO Box 4083, Flagstaff, AZ 86011, or by phone at 928-523-3312 (TTY: 928-523-1006), fax at 928-523-9977, email at equityandaccess@nau.edu, or visit the EAO website at https://nau.edu/equity-and-access.\n\nA.11.7 TITLE IX\nTitle IX of the Education Amendments of 1972, as amended, protects individuals from discrimination based on sex in any educational program or activity operated by recipients of federal financial assistance. In accordance with Title IX, Northern Arizona University prohibits discrimination based on sex or gender in all its programs or activities. Sex discrimination includes sexual harassment, sexual assault, relationship violence, and stalking. NAU does not discriminate on the basis of sex in the education programs or activities that it operates, including in admission and employment. NAU is committed to providing an environment free from discrimination based on sex or gender and provides a number of supportive measures that assist students, faculty, and staff.\nOne may direct inquiries concerning the application of Title IX to either or both the Title IX Coordinator or the U.S. Department of Education, Assistant Secretary, Office of Civil Rights. You may contact the Title IX Coordinator in the Office for the Resolution of Sexual Misconduct by phone at 928-523-5434, by fax at 928-523-0640, or by email at titleix@nau.edu. In furtherance of its Title IX obligations, NAU promptly will investigate or equitably resolve all reports of sex or gender-based discrimination, harassment, or sexual misconduct and will eliminate any hostile environment as defined by law. The Office for the Resolution of Sexual Misconduct (ORSM): Title IX Institutional Compliance, Prevention & Response addresses matters that fall under the university’s Sexual Misconduct policy. Additional important information and related resources, including how to request immediate help or confidential support following an act of sexual violence, is available at https://in.nau.edu/title-ix.\n\nA.11.8 ACCESSIBILITY\nProfessional disability specialists are available at Disability Resources to facilitate a range of academic support services and accommodations for students with disabilities. If you have a documented disability, you can request assistance by contacting Disability Resources at 928-523-8773 (voice), ,928-523-8747 (fax), or dr@nau.edu (e-mail). Once eligibility has been determined, students register with Disability Resources every semester to activate their approved accommodations. Although a student may request an accommodation at any time, it is best to initiate the application process at least four weeks before a student wishes to receive an accommodation. Students may begin the accommodation process by submitting a self-identification form online at https://nau.edu/disability-resources/student-eligibility-process or by contacting Disability Resources. The Director of Disability Resources, Jamie Axelrod, serves as NAU’s Americans with Disabilities Act Coordinator and Section 504 Compliance Officer. He can be reached at jamie.axelrod@nau.edu.\n\nA.11.9 RESPONSIBLE CONDUCT OF RESEARCH\nStudents who engage in research at NAU must receive appropriate Responsible Conduct of Research (RCR) training. This instruction is designed to help ensure proper awareness and application of well-established professional norms and ethical principles related to the performance of all scientific research activities. More information regarding RCR training is available at https://nau.edu/research/compliance/research-integrity.\n\nA.11.10 MISCONDUCT IN RESEARCH\nAs noted, NAU expects every student to firmly adhere to a strong code of academic integrity in all their scholarly pursuits. This includes avoiding fabrication, falsification, or plagiarism when conducting research or reporting research results. Engaging in research misconduct may result in serious disciplinary consequences. Students must also report any suspected or actual instances of research misconduct of which they become aware. Allegations of research misconduct should be reported to your instructor or the University’s Research Integrity Officer, Dr. David Faguy, who can be reached at david.faguy@nau.edu or 928-523-6117. More information about misconduct in research is available at https://nau.edu/university-policy-library/misconduct-in-research.\n\nA.11.11 SENSITIVE COURSE MATERIALS\nUniversity education aims to expand student understanding and awareness. Thus, it necessarily involves engagement with a wide range of information, ideas, and creative representations. In their college studies, students can expect to encounter and to critically appraise materials that may differ from and perhaps challenge familiar understandings, ideas, and beliefs. Students are encouraged to discuss these matters with faculty."
  },
  {
    "objectID": "ols.html#multiple-linear-regression",
    "href": "ols.html#multiple-linear-regression",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.11 Multiple Linear Regression",
    "text": "4.11 Multiple Linear Regression\nSo far, we have only discussed a single input variable in our model, which is a simple linear regression. When we have multiple input variables, we are dealing with multiple linear regression analysis, so the model looks like: \\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_{p-1} x_{p-1,i} + \\epsilon_i\\] where \\(p\\) is the number of model coefficients and \\(p-1\\) is the number of input variables. Still, in matrix notation the model is \\(Y = XB + \\epsilon\\), so the least squares regression analysis approach still works. However, our interpretation of the model coefficients becomes a bit more challenging.\nLet’s look at a data set within the faraway package.\n\nlibrary(faraway)\ndata(gala)\n\n# Plot the raw data relationships\npar(mfrow=c(1,3))\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Area, xlab = \"Area\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Adjacent, xlab = \"Adjacent\", ylab = \"Species\", pch = 19)\n\n\n\npar(mfrow=c(1,1))\n\n# Conduct multiple and single linear regressionm, focusing on Elevation\nm1 = lm(Species ~ Elevation + Area + Adjacent, data = gala)\nm2 = lm(Species ~ Elevation, data = gala)\ncoef(m1); coef(m2)\n\n(Intercept)   Elevation        Area    Adjacent \n-5.71892681  0.31498110 -0.02031217 -0.07527974 \n\n\n(Intercept)   Elevation \n 11.3351132   0.2007922 \n\n\n\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nabline(coef=coef(m1)[1:2])\nabline(coef=coef(m2)[1:2], lty = 2)\n\n\n\n\nWhat we see above is how the addition of Area and Adjacent input variables into the model “adjusts” the effect of Elevation, leading to two unique estimates of the slope (i.e., effect) of Elevation. Let’s probe multiple linear regression more closely by using simulated data.\nFirst, let’s simulate a model with 80 data points that correspond to observations of 4 input variables and one outcome variable. Note that in Footnotes 4.12.3, we show a case with a categorical/binary input variable.\n\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    hist(xmat[,i], main = paste(\"covariate \", i-1))\n}\n\n\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\npar(mfrow=c(1,1))\nhist(y)\n\n\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n\nHow do we figure out the expected value of \\(y\\) for a particular situation? Here’s an example. What is the expected value of \\(y\\) when \\(x_2 = 0.5\\), but the rest of the input variables are at their average values?\n\n# Written out long-ways:\npred_y = \n    betas[1]*1 + \n    betas[2]*mean(xmat[,2]) + \n    betas[3]*0.5 + \n    betas[4]*mean(xmat[,4]) + \n    betas[5]*mean(xmat[,5]) \npred_y\n\n[1] 22.6006\n\n\nNow let’s use ordinary least squares regression to estimate our model coefficients from the data, and then compare these to our “known” values of the model parameters.\n\n# Run the model:\nm1 = lm(y ~ 0 + xmat)\n# Note that the following two models give the same results\n#m2 = lm(y ~ 0 + X1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\n#m3 = lm(y ~ 1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 0 + xmat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n       Estimate Std. Error t value Pr(>|t|)    \nxmat1  1.589838   1.759860   0.903    0.369    \nxmat2  0.737086   0.035629  20.688   <2e-16 ***\nxmat3 -1.295274   0.044252 -29.270   <2e-16 ***\nxmat4 -0.003676   0.028481  -0.129    0.898    \nxmat5  1.826125   0.088971  20.525   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9763,    Adjusted R-squared:  0.9747 \nF-statistic: 616.7 on 5 and 75 DF,  p-value: < 2.2e-16\n\n#summary(m2)\n#summary(m3)\n\n# Compare known `betas` to estimated coefficients\ncbind(betas, coef(m1)) \n\n      betas             \nxmat1  1.00  1.589837527\nxmat2  0.75  0.737085861\nxmat3 -1.20 -1.295274007\nxmat4  0.00 -0.003676167\nxmat5  1.80  1.826125438\n\n# plot the regression lines with abline\ncoef_m1 = coef(m1)\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    \n    plot(y ~ xmat[,i], pch=19,\n         xlab = paste(\"covariate \", i-1),\n         ylab = \"y\",\n         ylim = range(y))\n    abline(coef=coef_m1[c(1,i)])\n}\n\n\n\n\nWell, those regression lines do not look correct. That is because we are interpretting the slopes and intercepts a little incorrectly and not plotting them in the correct manner.\n\n\n\n\n\n\nHow to plot the output of lm() for multiple linear regression\n\n\n\nWhen we isolate and visualize the relationship between the outcome and a single input variable, what we are really observing is the adjusted relationship, after accounting for the other input variables in the model. To understand the expected value of \\(y\\) for any particular value of the single input variable, we really need to set the other input variables to their mean value. Let’s demonstrate this below with the predict() function.\n\n\nLet’s determine the expected values of \\(y\\) for input variable 2 (\\(x_2\\)) and plot it.\n\n# Prediction for covariate 2 when all other input vars at mean\nmy_df = data.frame(xmat[,2:5])\nhead(my_df)\n\n          X1         X2       X3 X4\n1 -1.7268438 13.8180926 61.27634  8\n2 16.0748747  6.7393185 58.15099 14\n3 -5.0439349  0.8145552 36.82198 16\n4  5.5611421 18.1722388 38.24042 10\n5 18.6915270 16.7070212 51.91376 15\n6  0.1767361 12.9778881 45.11988 16\n\n# Re-run the model but with just the input variables, \n# and the intercept is implicit\nm2 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\n# Now let's try to predict y across a range of \n# input variable 2,\n# while holding the other input variables at\n# their average values\n\nn_pred = 100\nnew_df = data.frame(\n  X1 = rep(mean(my_df$X1), n_pred),\n  X2 = seq(0, 20, length.out = n_pred),\n  X3 = rep(mean(my_df$X3), n_pred),\n  X4 = rep(mean(my_df$X4), n_pred)\n)\n\ny_pred2 = predict(m2, newdata = new_df)\n\n# Now plot:\npar(mfrow=c(1,1))\nplot(y ~ my_df$X2, pch = 19,\n     xlab = \"covariate 2\", ylab = \"y\")\nlines(y_pred2 ~ new_df$X2)\n\n\n\n\nNow we see that the predict() function shows a more intuitive relationship between input variable \\(x_2\\) and outcome \\(y\\), while accounting for the effects of the three other input variables."
  },
  {
    "objectID": "hypothesis.html#lecture-material",
    "href": "hypothesis.html#lecture-material",
    "title": "5  Hypothesis Testing",
    "section": "\n5.1 Lecture material",
    "text": "5.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "hypothesis.html#sec-data",
    "href": "hypothesis.html#sec-data",
    "title": "5  Hypothesis Testing",
    "section": "\n5.2 Generate the data",
    "text": "5.2 Generate the data\nHere we are going to build on the lecture material by comparing manual calculations of hypothesis tests versus the metrics reported by the lm() function.\nFirst let’s generate data, in the same way we did for multiple linear regression in (Chapter 4).\n\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n# Create a data.frame\nmy_df = data.frame(y, xmat[,2:5])\nhead(my_df)\n\n          y         X1         X2       X3 X4\n1 -4.586806 -1.7268438 13.8180926 61.27634  8\n2 27.662332 16.0748747  6.7393185 58.15099 14\n3 26.466919 -5.0439349  0.8145552 36.82198 16\n4  0.843986  5.5611421 18.1722388 38.24042 10\n5 18.891783 18.6915270 16.7070212 51.91376 15\n6 12.660545  0.1767361 12.9778881 45.11988 16\n\n# Run the model, report the summary\nm1 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\nm1_summary = summary(m1)\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.589838   1.759860   0.903    0.369    \nX1           0.737086   0.035629  20.688   <2e-16 ***\nX2          -1.295274   0.044252 -29.270   <2e-16 ***\nX3          -0.003676   0.028481  -0.129    0.898    \nX4           1.826125   0.088971  20.525   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9497,    Adjusted R-squared:  0.9471 \nF-statistic: 354.3 on 4 and 75 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "hypothesis.html#tests-using-the-t-distribution",
    "href": "hypothesis.html#tests-using-the-t-distribution",
    "title": "5  Hypothesis Testing",
    "section": "\n5.3 Tests using the \\(t\\)-distribution",
    "text": "5.3 Tests using the \\(t\\)-distribution\nWe typically use the \\(t\\)-distribution to test the following hypothesis for a specific model coefficient (e.g., an intercept, or a slope): \\[\nH_0: \\beta_i = 0 \\\\\nH_A: \\beta_i \\ne 0\n\\] If \\(\\beta_i\\) is a slope, then we are specifically testing if input variable \\(x_i\\) has a significant linear relationship with \\(y\\). Put another way, we are testing whether \\(x_i\\) has a significant linear effect on \\(y\\).\nThe \\(t\\)-statistic is calculated as follows: \\[t_i = \\frac{\\beta_i - \\mu}{SE(\\beta_i)}, \\quad \\text{and}\\] \\[t_i \\sim t(\\nu),\\]\nwhere \\(t(\\nu)\\) is a \\(t\\)-distribution with \\(\\nu=n-p\\) degrees of freedom. Then we find \\(P(t > |t_i|) = 1 - P(t \\le |t_i|)\\), which is our \\(p\\)-value for the test.\nLet’s manually calculate the \\(t_i\\) and the \\(P(t > |t_i|)\\) for input variables \\(x_1\\), which has a significant positive effect on \\(y\\), and for \\(x_3\\), which has no detectable linear effect on \\(y\\).\n\n## Calculate SE(betas) - We did this in the OLS chapter\nest_sigma = summary(m1)$sigma\nxtx_inv = solve(crossprod(xmat))\nvarcov_betas = xtx_inv*est_sigma^2\nse_betas = sqrt(diag(varcov_betas))\n\n# Degrees of freedom\nt_df = n-p\n\n# extract coef and SE\n# for X1 and X3\ncoef_x1 = coef(m1)[2]\nse_beta_x1 = se_betas[2]\n\ncoef_x3 = coef(m1)[4]\nse_beta_x3 = se_betas[4]\n\n# Calculate t_i\nt_x1 = (coef_x1 - 0)/se_beta_x1\nt_x3 = (coef_x3 - 0)/se_beta_x3\n\n\n# Calculate P(t > |t_i|) = 1 - P(t <= |t_i|)\n# abs() calculates absolute value\np_x1 = 1 - pt(abs(t_x1), df = t_df)\np_x3 = 1 - pt(abs(t_x3), df = t_df)\n\n# Create a table\nt_table = cbind(\n    rbind(t_x1, t_x3),\n    rbind(p_x1*2, p_x3*2) #Multiply by 2 for two-tailed test\n)\ncolnames(t_table) = c(\"t value\", \"Pr(>|t|)\")\nrownames(t_table) = c(\"X1\", \"X3\")\nt_table\n\n      t value  Pr(>|t|)\nX1 20.6876284 0.0000000\nX3 -0.1290726 0.8976457\n\n# Compare to summary of lm()\n## We're extracting just the relevant rows and columns\n## from the summary table \nm1_summary$coefficients[c(2,4), 3:4]\n\n      t value     Pr(>|t|)\nX1 20.6876284 1.008681e-32\nX3 -0.1290726 8.976457e-01\n\n\nLet’s plot the \\(t_i\\) on the \\(t\\)-distribution to see if these \\(p\\)-values make sense.\n\ntseq = seq(-21, 21, by = 0.1)\nprob_tseq = dt(tseq, df = t_df)\n\nplot(prob_tseq ~ tseq, type = \"l\",\n     xlab = \"t\", ylab = \"P(t | df = n-p)\")\nabline(v = c(-1,1)*t_x1, lty = 2, col = \"blue\")\nabline(v = c(-1,1)*t_x3, lty = 2, col = \"orange\")\n\n\n\n\nWe can see that the \\(t\\)-statistic for input variable \\(x_3\\) has a very high probability density, suggesting that the chances that the null hypothesis is true (i.e., \\(\\beta_3 = 0\\)) is high. In contrast, the \\(t\\)-statistic for input variable \\(x_1\\) has a very low probability density, suggesting that we have enough evidence to reject the null in support of the alternative hypothesis (i.e., \\(\\beta_1 \\ne 0\\))."
  },
  {
    "objectID": "hypothesis.html#sec-ttest",
    "href": "hypothesis.html#sec-ttest",
    "title": "5  Hypothesis Testing",
    "section": "\n5.3 Tests using the \\(t\\)-distribution",
    "text": "5.3 Tests using the \\(t\\)-distribution\nWe typically use the \\(t\\)-distribution to test the following hypothesis for a specific model coefficient (e.g., an intercept, or a slope): \\[H_0: \\beta_i = 0\\] \\[H_A: \\beta_i \\ne 0\\] If \\(\\beta_i\\) is a slope, then we are specifically testing if input variable \\(x_i\\) has a significant linear relationship with \\(y\\). Put another way, we are testing whether \\(x_i\\) has a significant linear effect on \\(y\\).\nThe \\(t\\)-statistic is calculated as follows: \\[t_i = \\frac{\\beta_i - \\mu}{SE(\\beta_i)}, \\quad \\text{and}\\] \\[t_i \\sim t(\\nu),\\]\nwhere \\(\\mu=0\\) represents our null hypothesis, and \\(t(\\nu)\\) is a \\(t\\)-distribution with \\(\\nu=n-p\\) degrees of freedom. Then we find \\(P(t > |t_i|) = 1 - P(t \\le |t_i|)\\), and we multiple this value by 2 for our two-tailed test, which is then our \\(p\\)-value for the test.\nLet’s manually calculate the \\(t_i\\) and the \\(P(t > |t_i|)\\) for input variables \\(x_1\\), which has a significant positive effect on \\(y\\), and for \\(x_3\\), which has no detectable linear effect on \\(y\\).\n\n## Calculate SE(betas) - We did this in the OLS chapter\nest_sigma = summary(m1)$sigma\nxtx_inv = solve(crossprod(xmat))\nvarcov_betas = xtx_inv*est_sigma^2\nse_betas = sqrt(diag(varcov_betas))\n\n# Degrees of freedom\nt_df = n-p\n\n# extract coef and SE\n# for X1 and X3\ncoef_x1 = coef(m1)[2]\nse_beta_x1 = se_betas[2]\n\ncoef_x3 = coef(m1)[4]\nse_beta_x3 = se_betas[4]\n\n# Calculate t_i\nt_x1 = (coef_x1 - 0)/se_beta_x1\nt_x3 = (coef_x3 - 0)/se_beta_x3\n\n# Calculate P(t > |t_i|) = 1 - P(t <= |t_i|)\n# abs() calculates absolute value\np_x1 = 1 - pt(abs(t_x1), df = t_df)\np_x3 = 1 - pt(abs(t_x3), df = t_df)\n\n# Create a table\nt_table = cbind(\n    rbind(t_x1, t_x3),\n    rbind(p_x1*2, p_x3*2) #Multiply by 2 for two-tailed test\n)\ncolnames(t_table) = c(\"t value\", \"Pr(>|t|)\")\nrownames(t_table) = c(\"X1\", \"X3\")\nt_table\n\n      t value  Pr(>|t|)\nX1 20.6876284 0.0000000\nX3 -0.1290726 0.8976457\n\n# Compare to summary of lm()\n## We're extracting just the relevant rows and columns\n## from the summary table \nm1_summary$coefficients[c(2,4), 3:4]\n\n      t value     Pr(>|t|)\nX1 20.6876284 1.008681e-32\nX3 -0.1290726 8.976457e-01\n\n\nAlthough our manual \\(p\\)-value calculation is zero, what this really means is that the \\(p\\)-value is so low, that it exceeds the significant digits that are allowed in (computer) memory, which is why the lm() summary output reports the notation Pr(>|t|): < 2.2e-16.\nLet’s plot the \\(t_i\\) on the \\(t\\)-distribution to see if these \\(p\\)-values make sense.\n\ntseq = seq(-21, 21, by = 0.1)\nprob_tseq = dt(tseq, df = t_df)\n\nplot(prob_tseq ~ tseq, type = \"l\",\n     xlab = \"t\", ylab = \"P(t | df = n-p)\")\nabline(v = c(-1,1)*t_x1, lty = 2, col = \"blue\")\nabline(v = c(-1,1)*t_x3, lty = 2, col = \"orange\")\n\n\n\n\nWe can see that the \\(t\\)-statistic for input variable \\(x_3\\) has a very high probability density, suggesting that the chances that the null hypothesis is true (i.e., \\(\\beta_3 = 0\\)) is high. In contrast, the \\(t\\)-statistic for input variable \\(x_1\\) has a very low probability density, suggesting that we have enough evidence to reject the null in support of the alternative hypothesis (i.e., \\(\\beta_1 \\ne 0\\))."
  },
  {
    "objectID": "hypothesis.html#sec-ftest",
    "href": "hypothesis.html#sec-ftest",
    "title": "5  Hypothesis Testing",
    "section": "\n5.4 Tests using the \\(F\\)-distribution",
    "text": "5.4 Tests using the \\(F\\)-distribution\nWe can also test the hypothesis: \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_{p-1} = 0\\] \\[H_A: \\beta_i \\ne 0, \\quad \\text{for at least one } i\\]\nThis test helps us understand if any of the input variables have a significant linear effect on \\(y\\), which at this point might not be the most useful test. However, later we will use a version of this test to determine which linear combinations of input variables will lead to the best explanation of \\(y\\).\nFor the above hypothesis, we calculate the \\(F\\)-statistic as: \\[F_{stat} = \\frac{ \\frac{SSE(\\text{null}) - SSE(\\text{full})}{\\text{df}_{\\text{null}}-\\text{df}_{\\text{full}}} }{ \\frac{SSE(\\text{full})}{\\text{df}_{\\text{full}}} }, \\quad \\text{and}\\] \\[F_{stat} \\sim F(\\text{df}_{\\text{numerator}}, \\text{df}_{\\text{denominator}})\\]\nThe \\(SSE(\\text{null})\\) refers to the sum of squared errors (i.e., residuals) for the null model that takes the form \\(y_i = \\beta_0 + \\epsilon_i\\), such that the \\(E[y_i] = \\beta_0 = \\bar{y}\\). Because this reduced model only has one coefficient, \\(\\beta_0\\), then the \\(\\text{df}_{\\text{null}} = n - 1\\). Note that, following the expression for \\(F_{stat}\\), the \\(\\text{df}_{\\text{numerator}}\\) is equal to \\(\\text{df}_{\\text{null}} - \\text{df}_{\\text{full}}\\), and the \\(\\text{df}_{\\text{denominator}}\\) is equal to \\(\\text{df}_{\\text{full}}\\).\nIn this test, we are essentially trying to understand if the full model, which includes all of the input variables in the model, does a better job at explaining the outcome variable compared to a null model that simply explains the data by saying that we should expect to see \\(y\\) values that are most often close to the mean of \\(y\\), which equals \\(\\bar{y}\\).\nLet’s manually calculate the the \\(F_{stat}\\) for the above multiple linear regression model, and then calculate the \\(p\\)-value from the associated \\(F\\)-distribution. First, we need to estimate a null model, which only has an intercept, which again should be estimated as \\(\\bar{y}\\).\n\nm_null = lm(y~1)\ncoef(m_null)\n\n(Intercept) \n   10.89976 \n\nmean(y)\n\n[1] 10.89976\n\n\nNow we can extract all of the information we need from the respective lm() output.\n\n# Extract the residuals (errors)\nresid_null = m_null$residuals\nresid_full = m1$residuals\n\n# Sum of Square Errors (SSE)\nsse_null = crossprod(resid_null)\nsse_full = crossprod(resid_full)\n\n# degrees of freedom\ndf_null = n-1\ndf_full = n-p\n\n# Calculate F_stat\nf_stat = ((sse_null - sse_full)/(df_null - df_full)) / (sse_full/df_full)\nf_stat\n\n         [,1]\n[1,] 354.3369\n\n# Degrees of freedom for the F distribution:\ndf_numerator = df_null - df_full\ndf_denominator = df_full\ndf_numerator\n\n[1] 4\n\ndf_denominator\n\n[1] 75\n\n# Compare this to the lm() output:\nm1_summary$fstatistic\n\n   value    numdf    dendf \n354.3369   4.0000  75.0000 \n\n# Visualize the associated F distribution\nfseq = seq(0, 10, by = 0.1)\np_fseq = df(fseq,\n            df1 = df_numerator,\n            df2 = df_denominator)\nplot(p_fseq ~ fseq, type = \"l\",\n     xlab = \"F\", ylab = \"P(F | df1, df2)\")\n\n\n\n\nNotice how the \\(F_{stat} > 350\\), which is far outside the range of our figure above, meaning that it has a very low probability density. We can formally calculate the \\(p\\)-value below:\n\n#P(F > f_stat) = 1 - P(F <= f_stat)\np_f_m1 = 1 - pf(f_stat,\n                df1 = df_numerator,\n                df2 = df_denominator)\np_f_m1\n\n     [,1]\n[1,]    0\n\n\nAgain, although this \\(p\\)-value is showing zero, what this really means is that the \\(p\\)-value is so low, that it exceeds the significant digits that are allowed in (computer) memory, which is why the lm() output reports the p-value: < 2.2e-16.\n\n# Reminder:\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.589838   1.759860   0.903    0.369    \nX1           0.737086   0.035629  20.688   <2e-16 ***\nX2          -1.295274   0.044252 -29.270   <2e-16 ***\nX3          -0.003676   0.028481  -0.129    0.898    \nX4           1.826125   0.088971  20.525   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9497,    Adjusted R-squared:  0.9471 \nF-statistic: 354.3 on 4 and 75 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "hypothesis.html#sec-r2",
    "href": "hypothesis.html#sec-r2",
    "title": "5  Hypothesis Testing",
    "section": "\n5.5 Goodness of fit with \\(R^2\\)\n",
    "text": "5.5 Goodness of fit with \\(R^2\\)\n\nThe summary of the lm() output also reports the Multiple R-squared, which is also referred to as the coefficient of determination, or simply \\(R^2\\) of the model. \\(R^2\\) provides a metric of “goodness of fit” for the model. This value roughly equates to the fraction of the variability in outcome variable \\(y\\) that is explained by the linear combination of input variables that are included in the model.\n\\[ R^2 = 1 - \\frac{SSE(\\text{full})}{SSE(\\text{null})}\\] Thus, the \\(R^2\\) is approximately calculating the reduction in residual error that occurs when you add meaningful input variables, compared to the null model that only has an intercept (i.e., predicting \\(y\\) based on its mean value).\n\nr2 = 1 - sse_full/sse_null\nr2\n\n          [,1]\n[1,] 0.9497436\n\n# Compared to the lm() output\nm1_summary$r.squared\n\n[1] 0.9497436\n\n\nWhat we see here is that the inclusion of the four input variables in the model explains approximately 95% of the variability in outcome variable \\(y\\). This makes sense, because we simulated the data \\(y\\) from a known set of input variables with a known set of coefficients.\nWhat happens to this \\(R^2\\) value if we remove a meaningful input variable from the model? In other words, what if we didn’t actually know all of the correct input variables to measure in real life that explain \\(y\\), and we didn’t measure an important one?\n\n# Run a model that does not include input variable X2\nm2 = lm(y ~ 1 + X1 + X3 + X4, data = my_df)\nm2_summary = summary(m2)\n# Extract the R^2\nm2_summary$r.squared\n\n[1] 0.3756416\n\n\nThe \\(R^2\\) has reduced from 95% to 38%, meaning that without the inclusion of input variable \\(x_2\\) in our model, we reduce our ability to explain (i.e., predict) the outcome \\(y\\) by approximately 60%.\n\n5.5.1 Adjusted \\(R^2\\)\n\nAnother measure in the summary of lm() output is an “adjusted” value of \\(R^2\\), which penalizes the value of \\(R^2\\) for models that have a lot of parameters (\\(p\\)) compared to the number of data points (\\(n\\)).\n\\[\\text{Adjusted } R^2 = 1 - \\frac{n-1}{n-p} + \\frac{n-1}{n-p} R^2\\]\n\nadjusted_r2 = 1 - (n-1)/(n-p) + (n-1)/(n-p)*r2\nadjusted_r2\n\n          [,1]\n[1,] 0.9470633\n\n# Compared to lm() output\nm1_summary$adj.r.squared\n\n[1] 0.9470633\n\n\nIf you have more parameters (i.e., more input variables) relative to the number of observed data points, then the Adjusted \\(R^2\\) will be less than \\(R^2\\). In the model above, we have many data points (\\(n=80\\)) relative to the number of input variables (\\(p-1 = 4\\)), so there is only a small difference in the two metrics.\n\n\n\n\n\n\nCongratulations!\n\n\n\nLook at the summary() of the lm() output. You should now be able to explain (and manually calculate) every value that is printed in that output. You now have a deep understanding of the ordinary least squares (OLS) regression analysis and associated methods of hypothesis-testing."
  },
  {
    "objectID": "max-lik.html#lecture-material",
    "href": "max-lik.html#lecture-material",
    "title": "6  Maximum Likelihood",
    "section": "\n6.1 Lecture material",
    "text": "6.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "max-lik.html#sec-data",
    "href": "max-lik.html#sec-data",
    "title": "6  Maximum Likelihood",
    "section": "\n6.2 Generate some data",
    "text": "6.2 Generate some data\nFirst, let’s generate some data for the case of a simple linear regression.\n\n### PARAMS\nbeta0 = 1.5\nbeta1 = 0.5\nsigma = 0.4\nn = 30\n\n### GENERATE DATA\nset.seed(5)\nx = runif(n, -1.5, 1.5)\nexp_y = beta0 + beta1*x\ny = exp_y + rnorm(n, mean=0, sd=sigma)\n\n# Create a data frame \nmy_df = data.frame(y = y, x = x)\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")"
  },
  {
    "objectID": "max-lik.html#calculate-a-likelihood",
    "href": "max-lik.html#calculate-a-likelihood",
    "title": "6  Maximum Likelihood",
    "section": "\n6.3 Calculate a likelihood",
    "text": "6.3 Calculate a likelihood\nRemember that, for simple linear regression, the likelihood of a single data point is as follows: \\[y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\] \\[P(y_i | X_i B, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{e}^{-\\frac{1}{2}\\frac{(y_i - X_i B)^2}{\\sigma^2}}\\]\nThen, the full likelihood of the data set \\(Y\\) is computed as:\n\\[ P(Y | X B, \\sigma^2) = \\prod^n_{i=1} P(y_i | X_i B, \\sigma^2)\\] Or, on the natural logarithmic scale: \\[ ln\\left(P(Y | X B, \\sigma^2)\\right) = \\sum^n_{i=1} ln\\left(P(y_i | X_i B, \\sigma^2)\\right)\\]\n\n# How to calculate the likelihood of a single data point:\ndnorm(y[1], \n      mean = beta0 + beta1*x[1],\n      sd = sigma,\n      log = FALSE)\n\n[1] 0.987769\n\n# Calculate the full likelihood of the data, using the product\n## Vectorized:\nLH_notlog= \nprod(\n    dnorm(y, \n          mean = beta0 + beta1*x,\n          sd = sigma,\n          log = FALSE)\n)\n\n# Log-likelihood, vectorized\nLH_log = \nsum(\n    dnorm(y, \n          mean = beta0 + beta1*x,\n          sd = sigma,\n          log = TRUE)\n)\n\n# Make sure the output makes sense\nLH_notlog\n\n[1] 8.496937e-10\n\n# Indeed the log-likelihood is the log of the \n# likelihood on the raw probability scale.\nLH_log; log(LH_notlog)\n\n[1] -20.88615\n\n\n[1] -20.88615"
  },
  {
    "objectID": "max-lik.html#using-optim-to-minimize-the-negative-log-likelihood",
    "href": "max-lik.html#using-optim-to-minimize-the-negative-log-likelihood",
    "title": "6  Maximum Likelihood",
    "section": "\n6.4 Using optim() to minimize the negative log-likelihood",
    "text": "6.4 Using optim() to minimize the negative log-likelihood\nAs we discussed in lecture, it is more computationally convenient to minimize functions, rather than to maximize. Therefore, to conduct linear regression analysis with maximum likelihood methods, we will find the values of \\(\\hat{B}\\) that minimize the negative log-likelihood of the data: \\(-ln\\left(P(Y | X B, \\sigma^2)\\right)\\).\n\n6.4.1 Function to calculate the negative log-likelihood\nFirst, we need to construct a function that calculates the negative log-likelihood and that specifies the parameters of the model that eventually need to be estimated by the optim() function.\n\n### NEG LOG-LIK MINIMIZATION\nneg_log_lik = function(p, data_df){\n    beta0=p[1]\n    beta1=p[2]\n    sigma=p[3]\n    \n    mu = beta0 + beta1*data_df$x\n    \n    nll = -sum(dnorm(data_df$y, mean=mu, sd=sigma, log = TRUE))\n    return(nll)\n}\n\nHere you can see the inputs to the function: p is a vector of parameters to be estimated (i.e., optimized), and data_df is a data.frame that holds the values of outcome variable \\(y\\) and associated input variables, in this case, just one \\(x\\).\nThen, we can use the optim() function, which implements a gradient descent algorithm to estimate the values of the parameters that minimize the provided function, neg_log_lik(). We will learn more about gradient descent later, because this is a very important method used widely across machine learning and neural networks.\n\nm_nll = \n    optim(\n        par = c(0.1,0,0.1),\n        fn = neg_log_lik,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,-5,0.001),\n        upper=c(5,5,5),\n        hessian = TRUE\n    )\npar_tab_nll = rbind(m_nll$par)\ncolnames(par_tab_nll) = c(\"int\", \"slope\", \"sigma\")\npar_tab_nll\n\n          int   slope     sigma\n[1,] 1.598468 0.56205 0.4571571\n\n\nNote the optim() options. par specifies the initial guesses of the three parameters, whereas lower and upper specify the bounds across which to search for the best values of the parameters. With hessian = TRUE we are asking the function to output an estimate of the Hessian matrix of the function, which helps us to estimate the standard errors of the parameter estimates (see Footnotes 6.7.1). The method specifies the algorithm used to minimize the function, which in this case is a modified quasi-Newton method, L-BFGS-B, which is a type of gradient descent algorithm, to be discussed later.\nWe can see that the function outputs three point-estimates, which are \\(\\hat{B}\\) (i.e., slope and intercept), as well as the residual standard deviation, \\(\\hat{\\sigma}\\).\n\n6.4.2 Compare optim() results to the OLS output\n\n### COMPARE TO LM()\nm_ols = lm(y ~ 1 + x, data = my_df)\nm_ols_summary = summary(m_ols)\nm_ols_summary # Notice p-value\n\n\nCall:\nlm(formula = y ~ 1 + x, data = my_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04966 -0.37035  0.06069  0.37520  0.72646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.59847    0.08643  18.495  < 2e-16 ***\nx            0.56205    0.09864   5.698 4.14e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4732 on 28 degrees of freedom\nMultiple R-squared:  0.537, Adjusted R-squared:  0.5204 \nF-statistic: 32.47 on 1 and 28 DF,  p-value: 4.136e-06\n\npar_tab_ols = c(coef(m_ols), m_ols_summary$sigma)\nnames(par_tab_ols) = c(\"int\", \"slope\", \"sigma\")\npar_tab_ols; par_tab_nll\n\n      int     slope     sigma \n1.5984685 0.5620501 0.4732005 \n\n\n          int   slope     sigma\n[1,] 1.598468 0.56205 0.4571571\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")\n# Line from OLS\nabline(coef = coef(m_ols), col = \"black\", lwd = 2)\n# Line from MaxLikelihood\nabline(coef = m_nll$par[1:2], col = \"red\", lty = 2, lwd = 2)\n\n\n\n\nAs we learned in lecture, the estimates of \\(\\hat{B}\\) from least squares and maximum likelihood are equivalent. And indeed, we see the same estimates produced from lm() and optim(). Also if you look at Residual standard error in the lm() output, you see the equivalent estimate for \\(\\hat{\\sigma}\\) compared to the optim() output."
  },
  {
    "objectID": "max-lik.html#hypothesis-testing-for-maximum-likelihood-approach",
    "href": "max-lik.html#hypothesis-testing-for-maximum-likelihood-approach",
    "title": "6  Maximum Likelihood",
    "section": "\n6.5 Hypothesis-testing for maximum likelihood approach",
    "text": "6.5 Hypothesis-testing for maximum likelihood approach\nAs explained in lecture, we will use the likelihood ratio test to test:\n\\[H_0: \\beta_i = 0\\] \\[H_A: \\beta_i \\ne 0\\] In the least squares framework, we used a \\(t\\)-test. But, for maximum likelihood, we are going to base our test on the likelihood of a model that does or does not include the slope, similar to the \\(F\\)-test we learned before.\nFor the case of simple linear regression, we’re testing whether there is a significant difference between these models: \\[H_0: y_i = \\beta_0 + \\epsilon_i\\] \\[H_A: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Notice that in \\(H_0\\), the slope \\(\\beta_1\\) is assumed to be zero.\n\n6.5.1 Likelihood ratio and the \\(\\chi^2\\) test\nOur goal is to understand if the likelihood of the null model, \\(P(Y | \\beta_0, \\sigma^2)\\), is sufficiently low compared to the likelihood of the full model, \\(P(Y | \\beta_0, \\beta_1, x, \\sigma^2)\\), that we can reliably reject the null hypothesis.\nWe therefore construct a ratio of the likelihoods of the full and null model, very similar to the \\(F\\)-test framework. The log-likelihood ratio (\\(LHR\\)) becomes our test statistic: \\[LHR_{\\text{test}} = -2 ln \\left(\\frac{LH_{\\text{null}}}{LH_{\\text{full}}} \\right)\\]\nThen, folks smarter than I have done the math to prove that this test statistic is equivalent to a \\(\\chi^2\\) test statistic, such that:\n\\[LHR_{\\text{test}} \\sim  \\chi^2_k\\],\nwhere \\(\\chi^2_k\\) is a \\(\\chi^2\\) probability distribution with \\(k\\) degrees of freedom, and \\(k\\) is equal to \\(p_{\\text{full}} - p_{\\text{null}}\\) and \\(p\\) is the number of model coefficients. In the case of simple linear regression, where we are removing just one model coefficient from the full model (i.e., set slope equal to zero), then \\(k = 2-1 = 1\\).\nFinally, we can determine \\(P(\\chi^2 > LHR_{\\text{test}})\\), which gives us our \\(p\\)-value. This statistical test is known as the “likelihood ratio test,” and it is equivalently referred to as the “\\(\\chi^2\\)” test, which we’ll see in the code below.\n\n6.5.2 Manual calculation of the likelihood ratio test\nTo begin, we need to use maximum likelihood to estimate the likelihood of the “null” model. We need to adjust our function that will be used by optim() to only include two parameters: the intercept, and the residual standard deviation.\n\n# Need a null model:\nnll_null = function(p, data_df){\n    beta0=p[1]\n    sigma=p[2]\n    \n    mu = beta0\n    \n    nll = -sum(dnorm(data_df$y, mean=mu, sd=sigma, log = TRUE))\n    return(nll)\n}\n\nm_nll_null = \n    optim(\n        par = c(0.1,0.1),\n        fn = nll_null,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,0.001),\n        upper=c(5,5)\n    )\npar_tab_nll_null = rbind(m_nll_null$par)\ncolnames(par_tab_nll_null) = c(\"int\", \"sigma\")\npar_tab_nll_null\n\n          int     sigma\n[1,] 1.612056 0.6718164\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")\nabline(coef = m_nll$par[1:2], col = \"red\", lty = 1)\nabline(h = m_nll_null$par[1], col = \"red\", lty = 2)\n\n\n\n\nThe flat dashed line represents the null (intercept-only) model. Now, we calculate the likelihood ratio test statistic, and compare to the \\(\\chi^2\\) probability distribution to determine our \\(p\\)-value of the test. Note that within the optim() function’s output list, there is a numeric object called value. This value is the negative log-likelihood of the model with the estimated coefficients. We can use this to calculate our test statistic.\n\n# use exp() to convert the negative log likelihood to \n# raw probability scale\nlog_lh_full = -m_nll$value\nlog_lh_null = -m_nll_null$value\n\nlh_full = exp(log_lh_full)\nlh_null = exp(log_lh_null)\n\n# Now calculate LHR\nlhr = -2 * log(lh_null / lh_full)\nlhr\n\n[1] 23.09763\n\n# Of course, using rules of natural logs, this is equivalent:\n-2 * (log_lh_null - log_lh_full)\n\n[1] 23.09763\n\n\nNow that we have our value of \\(LHR_{\\text{test}}\\), we use the \\(\\chi^2\\)-distribution to find \\(P(\\chi^2 > LHR_{\\text{test}})\\), which is the \\(p\\)-value of the test.\n\n# How many parameters being \"removed\" (i.e., set to zero) in test:\ndf_chi = 2 - 1\n\n# Prob null is true\np_val = 1 - pchisq(lhr, df = df_chi)\np_val\n\n[1] 1.539803e-06\n\n\nBased on this low \\(p\\)-value, we would say there is sufficient evidence to reject the null hypothesis and that the slope \\(\\beta_1\\) is significantly different than zero.\nWe can compare this outcome to a built-in R function called drop1().\n\ndrop1(m_ols, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ny ~ 1 + x\n       Df Sum of Sq     RSS     AIC Pr(>Chi)    \n<none>               6.2697 -42.964             \nx       1    7.2703 13.5401 -21.866 1.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the function, we specified Chisq test, which implements the \\(\\chi^2\\) test using the likelihood ratio. What we see in this summary output is Pr(>Chi) which is equivalent to our manually computed value of \\(P(\\chi^2 > LHR_{\\text{test}})\\). This output from drop1() does not provide a whole lot of detail, but if you look at the help(), it says that if you specify test = \"Chisq\", it conducts a likelihood-ratio test. It doesn’t specifically output the likelihood ratio, but we can see the \\(p\\)-value is equivalent to our manual calculation above."
  },
  {
    "objectID": "max-lik.html#footnotes",
    "href": "max-lik.html#footnotes",
    "title": "6  Maximum Likelihood",
    "section": "\n6.7 Footnotes",
    "text": "6.7 Footnotes\n\n6.7.1 Hessian matrix\nThe optim() function provides point estimates for the maximum likelihood-derived model coefficients. Just like in least squares regression, however, we want to quantify the uncertainty in these estimates. We therefore want the standard error in the model coefficient estimates.\nIn the case of least squares, we showed how we can calculate a variance-covariance matrix for the model coefficients, and then the square-root of the diagonal of this matrix equals the standard error. For maximum likelihood we can estimate this same variance-covariance matrix, but it comes from a different matrix called the Hessian. We do not need to go into detail, but the Hessian is the matrix of second derivatives of the likelihood with respect to the parameters (I will not ask you to recall this information). Then the variance-covariance matrix of the estimated model coefficients is calculated as the inverse of the Hessian matrix that corresponds to the negative log-likelihood. If the Hessian matrix of the negative log-likelihood is \\(H\\), then \\[SE(\\hat{\\beta_i}) = \\sqrt{\\text{diag}\\left( H^{-1}\\right)_i}\\] I understand that’s complicated, but it’s easy enough to extract these values computationally from optim() output, assuming you use the option hessian = TRUE.\n\n# Extract the Hessian from the optim() output\nhessian = m_nll$hessian\n# Calculate the var-cov matrix from the inverse Hessian\n# Remember solve(X) gives X^-1\nparams_varcov = solve(hessian)\n# Then extract the diagonal and take the square root\n# This gives a vector of SE(\\param_i)\nse_params = sqrt(diag(params_varcov))\nparams_tab = cbind(m_nll$par, se_params)\ncolnames(params_tab) = c(\"Estimate\", \"Std. Error\")\nrownames(params_tab) = c(\"Intercept\", \"slope\", \"sigma\")\nparams_tab\n\n           Estimate Std. Error\nIntercept 1.5984685 0.08349686\nslope     0.5620500 0.09529343\nsigma     0.4571571 0.05901782\n\n# Same as OLS? \nm_ols_summary$coefficients[c(1:2), 1:2] # Pretty close!\n\n             Estimate Std. Error\n(Intercept) 1.5984685 0.08642710\nx           0.5620501 0.09863766\n\n\nWe can see that the standard errors for the maximum likelihood estimators are the same as the OLS estimators.\n\n6.7.2 optim() using least squares\nRemember that optim() is not specific to maximum likelihood, but rather it implements one of several optional minimization algorithms. Therefore, we can use it to minimize any quantity. To emphasize this point, remember that in least squares regression, we are finding the values of the model coefficients \\(\\hat{B}\\) that minimize the sum of squared errors, \\(\\sum_i^n \\epsilon_i^2 = \\epsilon^T\\epsilon\\). Let’s minimize this quantity using optim().\n\n### LEAST SQUARES MINIMIZATION\n\n# We need a function to calculate the sum of squared errors:\nleast_sq = function(p, data_df){\n    beta0=p[1]\n    beta1=p[2]\n    y = data_df$y\n    n = length(y)\n    \n    expected_y = beta0 + beta1*data_df$x\n    sse = 0\n    for(i in 1:n){\n        epsilon_i = y[i] - expected_y[i]\n        sse = sse + (epsilon_i)^2\n    }\n    \n    return(sse)\n}\n\n### OPTIMIZE LEAST SQUARES\nfit_least_sq = \n    optim(\n        par = c(0,0),\n        fn = least_sq,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,-5),\n        upper=c(5,5),\n        hessian = TRUE\n    )\n# Create a table of estimates:\npar_tab_least_sq = rbind(fit_least_sq$par)\ncolnames(par_tab_least_sq) = c(\"int\", \"slope\")\npar_tab_least_sq\n\n          int     slope\n[1,] 1.598469 0.5620501\n\n# Compare to original OLS estimates:\ncoef(m_ols)\n\n(Intercept)           x \n  1.5984685   0.5620501 \n\n\nYou could also use the Hessian output to calculate the standard errors of the model coefficients, but I will leave that up to you.\n\n6.7.3 Gradient descent, multiple parameters\nNow let’s suppose we want to estimate all of our model parameters (intercept, slope, residual standard deviation) simultaneously using gradient descent. Recall from lecture that we need to estimate three components of the gradient (the partial derivates of the function with respect to each model parameter).\n\n# How many params to estimate?\nn_param = 3\n\n# Set up some storage arrays:\n## Guess that the number of iterations will be 100 or less...\nparam_guess = array(0, dim=c(100,n_param))\nnll_guess = vector(\"numeric\", length = 100)\n\n# initial guesses\nparam = vector(\"numeric\", length = n_param)\nparam[1] = 0.0 # intercept\nparam[2] = 1.0\nparam[3] = 2.5\n\n# set h for approx of gradient\nh = 1e-4\n\n# set step size\nalpha = 0.005\n\n# Set gradient components to arbitrary high numbers\n## This makes the while() loop work\ngrad = rep(100, times = n_param)\n\n# initialize counter\ni = 1\n\n# While gradient is not yet \\approx zero\nwhile (norm(grad, \"2\") > 10^-4) {\n    \n    # Store the current value of slope:\n    param_guess[i, ] = param\n    \n    #############\n    # Approximate the gradient of the nll\n    #############\n    \n    ## Calculate nll with all current params\n    f_x = neg_log_lik(p = param, \n                          data_df = my_df)\n    ## Store the current nll\n    nll_guess[i] = f_x\n    \n    ## One param at a time, approximate gradient component\n    ## (i.e., partial derivative)\n    for(j in 1:n_param){\n        ## Keep all but one params the same\n        param_adj = NULL\n        param_adj = param\n        param_adj[j] = param[j] + h\n        \n        f_x_adj = neg_log_lik(p = param_adj, \n                              data_df = my_df)\n        ## Calculate gradient component\n        grad[j] = (f_x_adj - f_x) / h\n    }\n    \n    # search direction\n    ## Note that 'direct' is an array of size n_param\n    direct = -grad\n    \n    # update params\n    for(j in 1:n_param){\n        param[j] = param[j] + alpha * direct[j]\n    }\n    \n    # counter for x\n    i = i + 1\n}\n\n# Output the optimal slope and associated nll\nn_iter = i-1\n\ngrad_descent_tab = cbind(rbind(param_guess[n_iter,]), \n                         nll_guess[n_iter])\ncolnames(grad_descent_tab) = \n    c(\"int\", \"slope\", \"sigma\", \"neg_log_lik\")\n\n# Compare to optim() output\noptim_nll_tab = cbind(rbind(m_nll$par), \n                      m_nll$value)\ncolnames(optim_nll_tab) = colnames(grad_descent_tab)\n\nprint(\"Gradient Descent:\");grad_descent_tab\n\n[1] \"Gradient Descent:\"\n\n\n         int     slope     sigma neg_log_lik\n[1,] 1.59842 0.5620025 0.4571052    19.08618\n\nprint(\"optim() output:\");optim_nll_tab\n\n[1] \"optim() output:\"\n\n\n          int   slope     sigma neg_log_lik\n[1,] 1.598468 0.56205 0.4571571    19.08618\n\n\nNow, plot the gradient descent:\n\npar(mfrow=c(1,3))\nplot(nll_guess[1:n_iter]~param_guess[1:n_iter, 1], \n     pch=19, type = \"b\", \n     col = \"blue\",\n     ylab = \"Neg. Log Likelihood\", \n     xlab = expression(\"Intercept, \"~beta[0]))\nplot(nll_guess[1:n_iter]~param_guess[1:n_iter, 2],\n     pch=19, type = \"b\", \n     col = \"orange\",\n     ylab = \"Neg. Log Likelihood\", \n     xlab = expression(\"Slope, \"~beta[1]))\nplot(nll_guess[1:n_iter]~param_guess[1:n_iter, 3], \n     pch=19, type = \"b\", \n     col = \"red\",\n     ylab = \"Neg. Log Likelihood\", \n     xlab = expression(\"Residual Std.Dev., \"~sigma))\n\n\n\n\nWe can see for the intercept, our first guess was too low, so we descended the gradient by addition (i.e., search direction was positive), whereas for the slope and the residual standard deviation, our first guess was too large, so we descended the gradients by subtraction (i.e., search direction was negative)."
  },
  {
    "objectID": "max-lik.html#hypothesis-testing-for-maximum-likelihood",
    "href": "max-lik.html#hypothesis-testing-for-maximum-likelihood",
    "title": "6  Maximum Likelihood",
    "section": "\n6.5 Hypothesis-testing for maximum likelihood",
    "text": "6.5 Hypothesis-testing for maximum likelihood\nAs explained in lecture, we will use the likelihood ratio test to test:\n\\[H_0: \\beta_i = 0\\] \\[H_A: \\beta_i \\ne 0\\] In the least squares framework, we used a \\(t\\)-test. But, for maximum likelihood, we are going to base our test on the likelihood of a model that does or does not include the slope, similar to the \\(F\\)-test we learned before.\nFor the case of simple linear regression, we’re testing whether there is a significant difference between these models: \\[H_0: y_i = \\beta_0 + \\epsilon_i\\] \\[H_A: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Notice that in \\(H_0\\), the slope \\(\\beta_1\\) is assumed to be zero.\n\n6.5.1 Likelihood ratio and the \\(\\chi^2\\) test\nOur goal is to understand if the likelihood of the null model, \\(P(Y | \\beta_0, \\sigma^2)\\), is sufficiently low compared to the likelihood of the full model, \\(P(Y | \\beta_0, \\beta_1, x, \\sigma^2)\\), that we can reliably reject the null hypothesis.\nWe therefore construct a ratio of the likelihoods of the full and null model, very similar to the \\(F\\)-test framework. The log-likelihood ratio (\\(LHR\\)) becomes our test statistic: \\[LHR_{\\text{test}} = -2 ln \\left(\\frac{LH_{\\text{null}}}{LH_{\\text{full}}} \\right)\\]\nThen, folks smarter than I have done the math to prove that this test statistic is equivalent to a \\(\\chi^2\\) test statistic, such that:\n\\[LHR_{\\text{test}} \\sim  \\chi^2_k\\]\nwhere \\(\\chi^2_k\\) is a \\(\\chi^2\\) probability distribution with \\(k\\) degrees of freedom. \\(k\\) is equal to \\(p_{\\text{full}} - p_{\\text{null}}\\), where \\(p\\) is the number of model coefficients. In the case of simple linear regression, where we are removing just one model coefficient from the full model (i.e., set slope equal to zero), then \\(k = 2-1 = 1\\).\nFinally, we can determine \\(P(\\chi^2 > LHR_{\\text{test}})\\), which gives us our \\(p\\)-value. This statistical test is known as the “likelihood ratio test,” and it is equivalently referred to as the “\\(\\chi^2\\)” test, which we’ll see in the code below.\n\n6.5.2 Manual calculation of the likelihood ratio test\nTo begin, we need to use maximum likelihood to estimate the likelihood of the “null” model. We need to adjust our function that will be used by optim() to only include two parameters: the intercept, and the residual standard deviation.\n\n# Need a null model:\nnll_null = function(p, data_df){\n    beta0=p[1]\n    sigma=p[2]\n    \n    mu = beta0\n    \n    nll = -sum(dnorm(data_df$y, mean=mu, sd=sigma, log = TRUE))\n    return(nll)\n}\n\nm_nll_null = \n    optim(\n        par = c(0.1,0.1),\n        fn = nll_null,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,0.001),\n        upper=c(5,5)\n    )\npar_tab_nll_null = rbind(m_nll_null$par)\ncolnames(par_tab_nll_null) = c(\"int\", \"sigma\")\npar_tab_nll_null\n\n          int     sigma\n[1,] 1.612056 0.6718164\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")\nabline(coef = m_nll$par[1:2], col = \"red\", lty = 1)\nabline(h = m_nll_null$par[1], col = \"red\", lty = 2)\n\n\n\n\nThe flat dashed line represents the null (intercept-only) model. Now, we calculate the likelihood ratio test statistic, and compare to the \\(\\chi^2\\) probability distribution to determine our \\(p\\)-value of the test. Note that within the optim() function’s output list, there is a numeric object called value. This value is the negative log-likelihood of the model with the estimated coefficients. We can use this to calculate our test statistic.\n\n# use exp() to convert the negative log likelihood to \n# raw probability scale\nlog_lh_full = -m_nll$value\nlog_lh_null = -m_nll_null$value\n\nlh_full = exp(log_lh_full)\nlh_null = exp(log_lh_null)\n\n# Now calculate LHR\nlhr = -2 * log(lh_null / lh_full)\nlhr\n\n[1] 23.09763\n\n# Of course, using rules of natural logs, this is equivalent:\n-2 * (log_lh_null - log_lh_full)\n\n[1] 23.09763\n\n\nNow that we have our value of \\(LHR_{\\text{test}}\\), we use the \\(\\chi^2\\)-distribution to find \\(P(\\chi^2 > LHR_{\\text{test}})\\), which is the \\(p\\)-value of the test.\n\n# How many parameters being \"removed\" (i.e., set to zero) in test:\ndf_chi = 2 - 1\n\n# Prob null is true\np_val = 1 - pchisq(lhr, df = df_chi)\np_val\n\n[1] 1.539803e-06\n\n\nBased on this low \\(p\\)-value, we would say there is sufficient evidence to reject the null hypothesis and that the slope \\(\\beta_1\\) is significantly different than zero.\nWe can compare this outcome to a built-in R function called drop1().\n\ndrop1(m_ols, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ny ~ 1 + x\n       Df Sum of Sq     RSS     AIC Pr(>Chi)    \n<none>               6.2697 -42.964             \nx       1    7.2703 13.5401 -21.866 1.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the function, we specified Chisq test, which implements the \\(\\chi^2\\) test using the likelihood ratio. What we see in this summary output is Pr(>Chi) which is equivalent to our manually computed value of \\(P(\\chi^2 > LHR_{\\text{test}})\\). This output from drop1() does not provide a whole lot of detail, but if you look at the help(), it says that if you specify test = \"Chisq\", it conducts a likelihood-ratio test. It doesn’t specifically output the likelihood ratio, but we can see the \\(p\\)-value is equivalent to our manual calculation above."
  },
  {
    "objectID": "max-lik.html#gradient-descent-algorithm",
    "href": "max-lik.html#gradient-descent-algorithm",
    "title": "6  Maximum Likelihood",
    "section": "\n6.6 Gradient descent algorithm",
    "text": "6.6 Gradient descent algorithm\nHow does the optim() function work? There are various optimization algorithms that can be implemented by optim() that are specified by the user in the method option. Several of these are gradient-based algorithms, which is a family of algorithms that are very common in engineering problems (e.g., optimal control of drones) and machine learning (e.g., neural networks, reinforcement learning). These algorithms generally minimize an “objective function”: \\(\\min_{{x\\in {\\mathbb R}^{n}}}\\;f(x)\\).\nA gradient descent algorithm is the most common algorithm for minimizing a function. There are many varieties of these algorithms that employ various “tricks” to make the algorithms more efficient (e.g., find the solution in a smaller number of iterations) or more reliable. In lecture we outlined the foundational gradient descent algorithm, upon which many more advanced algorithms are based. For a function \\(f(x)\\), find the value of \\(x\\) that solves the problem: \\(\\min _{{x\\in {\\mathbb R}^{n}}}\\;f(x)\\)\n\nChoose a starting value of \\(x\\)\n\nEvaluate \\(\\nabla f(x)\\). With \\(h=10^{-4}\\): \\[\\text{grad} = \\frac{f(x+h) - f(x)}{h}\\]\n\nSet the search direction as \\(\\text{direct} = -\\nabla f(x)\\)\n\nSet the step size as a fraction of \\(\\nabla f(x)\\): \\(\\text{alpha} = 0.005\\)\n\nUpdate \\(x\\) for next iteration: \\[x = x + \\text{alpha}* \\text{direct}\\]\n\nRepeat Steps 2-5 until \\(\\nabla f(x) \\approx 0\\) (i.e., \\(||\\text{grad}|| \\le 10^{-4}\\))\n\nLet’s implement this algorithm for the maximum likelihood solution of simple linear regression, using the data set above. To make this easier, we’re going to assume that we know the intercept and the residual standard deviation perfectly, so we are only trying to estimate the slope. See Footnotes 6.7.3 for the case in which we estimate all three model parameters simultaneously using gradient descent.\n\n# Set up some storage arrays:\nslope_guess = NULL\nnll_guess = NULL\n\n# initial guess\nslope = 0.1\n\n# set h for approx of gradient\nh = 1e-4\n\n# set step size\nalpha = 0.005\n\n# Set gradient to arbitrary high number\n## This makes the while() loop work\ngrad = 100\n\n# initialize counter\ni = 1\n\n# While gradient is not yet \\approx zero\nwhile (norm(grad, \"2\") > 10^-4) {\n    \n    # Store the current value of slope:\n    slope_guess[i] = slope\n    \n    #############\n    # Approximate the gradient of the nll\n    #############\n    ## Adjust the slope by a small number, h\n    slope_adj = slope + h\n    ## Calculate f(slope)\n    f_slope = neg_log_lik(p = c(beta0, slope, sigma), \n                          data_df = my_df)\n    ## Store the current nll\n    nll_guess[i] = f_slope\n    ## Calculate f(slope + h)\n    f_slope_adj = neg_log_lik(p = c(beta0, slope_adj, sigma), \n                              data_df = my_df)\n    ## Calculate gradient\n    grad = (f_slope_adj - f_slope) / h\n    \n    # search direction\n    direct = -grad\n    \n    # update slope\n    slope = slope + alpha * direct\n    \n    # counter for x\n    i = i + 1\n}\n\n# Output the optimal slope and associated nll\nn_iter = i-1\n\ngrad_descent_tab = cbind(slope_guess[n_iter], nll_guess[n_iter])\ncolnames(grad_descent_tab) = c(\"slope\", \"neg_log_lik\")\n\n# Compare to optim() outpu\noptim_nll_tab = cbind(m_nll$par[2], m_nll$value)\ncolnames(optim_nll_tab) = c(\"slope\", \"neg_log_lik\")\n\nprint(\"Gradient Descent:\");grad_descent_tab\n\n[1] \"Gradient Descent:\"\n\n\n         slope neg_log_lik\n[1,] 0.5651002    20.58064\n\nprint(\"optim() output:\");optim_nll_tab\n\n[1] \"optim() output:\"\n\n\n       slope neg_log_lik\n[1,] 0.56205    19.08618\n\n\nNote that the estimates are not exactly the same, especially because when we used optim() we were estimating all three parameters simultaneously: intercept, slope, residual standard error.\nNow, let’s visualize the gradient descent.\n\nplot(nll_guess ~ slope_guess, \n     pch = 19, type = \"b\", \n     ylab = \"Neg. Log. Likelihood\",\n     xlab = expression(hat(beta)[1]),\n     xlim = c(0.1, 0.6), ylim = c(15, 40))"
  },
  {
    "objectID": "model-select.html#lecture-material",
    "href": "model-select.html#lecture-material",
    "title": "7  Model selection",
    "section": "7.1 Lecture material",
    "text": "7.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "model-select.html#sec-data",
    "href": "model-select.html#sec-data",
    "title": "7  Model selection",
    "section": "7.2 Generate the data",
    "text": "7.2 Generate the data\nHere we will demonstrate two approaches to model comparison. But first let’s generate data, in the same way we did for multiple linear regression in (Chapter 4). Note that in this case, we will specify that two of the input variables have zero slope (i.e., no linear association with the outcome variable).\n\nn = 40\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.0\nbetas[3] = -0.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n# Create a data.frame\nmy_df = data.frame(y, xmat[,2:5])\nhead(my_df)\n\n          y         X1        X2       X3 X4\n1 28.492690 -1.7268438 18.788730 38.39431 18\n2 14.221411 16.0748747 16.474910 60.76146 10\n3  9.064956 -5.0439349  4.223082 33.40577  5\n4  9.366421  5.5611421  1.832589 41.76465  5\n5 18.874673 18.6915270  9.405498 40.26706 11\n6 17.706978  0.1767361  1.001228 46.05881 10\n\n# Run the model, report the summary\nm1 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\nm1_summary = summary(m1)\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3257 -1.4053 -0.4331  1.3299  4.3178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.41757    1.82632   1.871  0.06968 .  \nX1           0.03245    0.03810   0.852  0.40016    \nX2          -0.26989    0.07297  -3.698  0.00074 ***\nX3          -0.01823    0.03267  -0.558  0.58050    \nX4           1.68543    0.11083  15.207  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.94 on 35 degrees of freedom\nMultiple R-squared:  0.8901,    Adjusted R-squared:  0.8775 \nF-statistic: 70.86 on 4 and 35 DF,  p-value: 2.741e-16"
  },
  {
    "objectID": "model-select.html#parsimony-via-model-simplification",
    "href": "model-select.html#parsimony-via-model-simplification",
    "title": "7  Model selection",
    "section": "7.3 Parsimony via model simplification",
    "text": "7.3 Parsimony via model simplification\nWe will successively simplify the model until we find a “minimally acceptable” model that explains the most variability in the outcome variable.\nThere are several built-in functions in R that can help us make quantitatively justified decisions about which input variables can be dropped from the full model to determine our minimally acceptable model. First, we can use the \\(F\\)-test as described in lecture. This can be implemented by the anova() function.\nBased on the summary() output, we see that input variable 3 (\\(x_3\\)) has the least significant effect on \\(y\\), so we will drop that first and proceed from there.\n\n# The full model lives in object m1\nformula(m1)\n\ny ~ 1 + X1 + X2 + X3 + X4\n\n# Create a new model with the update() function\n# This function has a strange notation, but so it goes...\nm2 = update(m1, .~. -X3)\nformula(m2)\n\ny ~ X1 + X2 + X4\n\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ X1 + X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4492 -1.3713 -0.3323  1.2450  4.3718 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.54371    0.92986   2.736 0.009605 ** \nX1           0.02863    0.03712   0.771 0.445496    \nX2          -0.26530    0.07181  -3.694 0.000728 ***\nX4           1.68217    0.10962  15.346  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.921 on 36 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8799 \nF-statistic: 96.22 on 3 and 36 DF,  p-value: < 2.2e-16\n\n# Use anova() to test if the drop of X3 is justified\nanova(m2, m1)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X1 + X2 + X4\nModel 2: y ~ 1 + X1 + X2 + X3 + X4\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     36 132.91                           \n2     35 131.73  1    1.1712 0.3112 0.5805\n\n\nRemember that the hypothesis tested is: \\[H_0:\\text{simple model}\\] \\[H_A:\\text{complex model}\\] So if the \\(p \\ge 0.05\\), as usual, we cannot reject the null hypothesis. In this case, it means that the simple model is just as good as the more complex model. Therefore, we are justified in dropping \\(x_3\\). From the data, we could not detect that \\(x_3\\) has a statistically meaningful linear relationship with the outcome data \\(y\\).\nLet’s manually calculate that \\(F\\) test statistic and associated \\(p\\)-value to verify that we understand how the test works.\n\n# Extract the residuals (errors)\nresid_null = m2$residuals\nresid_full = m1$residuals\n\n# Sum of Square Errors (SSE)\nsse_null = crossprod(resid_null)\nsse_full = crossprod(resid_full)\n\n# degrees of freedom\ndf_null = n-(p-1) # we dropped one input variable\ndf_full = n-p\n\n# Calculate F_stat\nf_test = ((sse_null - sse_full)/(df_null - df_full)) / (sse_full/df_full)\n\n# Degrees of freedom for the F distribution:\ndf_numerator = df_null - df_full\ndf_denominator = df_full\n\np_m1vm2 = 1 - pf(f_test,\n                df1 = df_numerator,\n                df2 = df_denominator)\n\n# Compare to anova()\nf_test; p_m1vm2\n\n          [,1]\n[1,] 0.3111883\n\n\n          [,1]\n[1,] 0.5805028\n\nanova_m1vm2 = anova(m2,m1)\nanova_m1vm2$`F`; anova_m1vm2$`Pr(>F)`\n\n[1]        NA 0.3111883\n\n\n[1]        NA 0.5805028\n\n\nLet’s continue with the simplification process, using the anova() function.\n\n# model 2 is the current best.\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ X1 + X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4492 -1.3713 -0.3323  1.2450  4.3718 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.54371    0.92986   2.736 0.009605 ** \nX1           0.02863    0.03712   0.771 0.445496    \nX2          -0.26530    0.07181  -3.694 0.000728 ***\nX4           1.68217    0.10962  15.346  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.921 on 36 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8799 \nF-statistic: 96.22 on 3 and 36 DF,  p-value: < 2.2e-16\n\n# Now, drop x1 and check\nm3 = update(m2, .~. -X1)\n\n# Check:\nanova(m3, m2)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X2 + X4\nModel 2: y ~ X1 + X2 + X4\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     37 135.10                           \n2     36 132.91  1    2.1969 0.5951 0.4455\n\n# The p-value is not significant, so we\n# can accept the null (simpler model)\n\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4900 -1.4021 -0.1473  1.3871  4.3194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.46667    0.91941   2.683 0.010847 *  \nX2          -0.25788    0.07077  -3.644 0.000819 ***\nX4           1.69901    0.10683  15.904  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.911 on 37 degrees of freedom\nMultiple R-squared:  0.8873,    Adjusted R-squared:  0.8812 \nF-statistic: 145.6 on 2 and 37 DF,  p-value: < 2.2e-16\n\n# Remove X2 and check\nm4 = update(m3, .~. -X2)\nanova(m4, m3)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X4\nModel 2: y ~ X2 + X4\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     38 183.58                                  \n2     37 135.10  1    48.478 13.277 0.0008195 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Ok now the p-value is significant\n# We need to reject the null (simpler model)\n# We *cannot* reliably remove X2\n\n# Try X4 just in case:\n\nm5 = update(m3, .~. -X4)\nanova(m3, m5)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X2 + X4\nModel 2: y ~ X2\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     37  135.1                                  \n2     38 1058.6 -1   -923.53 252.92 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# p-value is significant again\n# need to reject the null\n# we cannot drop X4\n\n# Therefore, m3 is most parsimonious\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4900 -1.4021 -0.1473  1.3871  4.3194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.46667    0.91941   2.683 0.010847 *  \nX2          -0.25788    0.07077  -3.644 0.000819 ***\nX4           1.69901    0.10683  15.904  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.911 on 37 degrees of freedom\nMultiple R-squared:  0.8873,    Adjusted R-squared:  0.8812 \nF-statistic: 145.6 on 2 and 37 DF,  p-value: < 2.2e-16\n\n\nTherefore, model 3, is the minimum acceptable model: \\[y_i = \\beta_0 + \\beta_2 x_{2,i} + \\beta_4 x_{4,i} + \\epsilon_i\\]\nWe could actually come to the same result, using a different, more automated function, step(). However, this function uses a different metric to test the null vs. full model hypothesis, the Akaike nformation criterion (AIC), which is calculated as: \\[\\text{AIC} = - 2ln(\\text{Model Likelihood}) + 2k\\] And \\(k\\) is the number of estimated parameters in the model. We can then compare the AIC values to decide which models are “best”. We will learn more about AIC later.\nLet’s use the step() function and verify it gives us the same final outcome.\n\nm1_step = step(m1)\n\nStart:  AIC=57.68\ny ~ 1 + X1 + X2 + X3 + X4\n\n       Df Sum of Sq     RSS     AIC\n- X3    1      1.17  132.90  56.030\n- X1    1      2.73  134.46  56.497\n<none>               131.73  57.676\n- X2    1     51.48  183.22  68.871\n- X4    1    870.38 1002.11 136.839\n\nStep:  AIC=56.03\ny ~ X1 + X2 + X4\n\n       Df Sum of Sq     RSS     AIC\n- X1    1      2.20  135.10  54.686\n<none>               132.90  56.030\n- X2    1     50.39  183.29  66.888\n- X4    1    869.43 1002.34 134.848\n\nStep:  AIC=54.69\ny ~ X2 + X4\n\n       Df Sum of Sq     RSS     AIC\n<none>               135.10  54.686\n- X2    1     48.48  183.58  64.951\n- X4    1    923.53 1058.63 135.034\n\nsummary(m1_step)\n\n\nCall:\nlm(formula = y ~ X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4900 -1.4021 -0.1473  1.3871  4.3194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.46667    0.91941   2.683 0.010847 *  \nX2          -0.25788    0.07077  -3.644 0.000819 ***\nX4           1.69901    0.10683  15.904  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.911 on 37 degrees of freedom\nMultiple R-squared:  0.8873,    Adjusted R-squared:  0.8812 \nF-statistic: 145.6 on 2 and 37 DF,  p-value: < 2.2e-16\n\n\nWe can see the selected model only includes \\(x_2\\) and \\(x_4\\), just like our decision based on the \\(F\\)-test."
  },
  {
    "objectID": "model-select.html#model-averaging",
    "href": "model-select.html#model-averaging",
    "title": "7  Model selection",
    "section": "7.4 Model averaging",
    "text": "7.4 Model averaging\nRecall from lecture that model averaging represents another philosophical approach to model selection and model comparison. In this case, the idea is that we cannot know with certainty which model of a nested sub-set of models is “true”. Therefore, instead of reporting the slopes and intercepts from the single “best” model that based on parsimony, we should report “averaged” values of slopes and intercepts. These averages will take into account all of the possible nested subset of models in which those slopes and intercepts could have been calculated. This averaging procedure can produce slope and intercept estimates (as well as estimates of their uncertainty) that are less biased, and can perhaps yield better predictions of future data.\nWe will use the MuMIn package (Multimodel Inference) to do model averaging later, but first we will do it manually.\n\n7.4.1 Required calculations\nRecall that to come up with averaged estimates of model parameters (e.g., model-averaged slopes) we need to calculate weighted averages. These averages are weighted by how well sub-models explain the data. Following lecture, we will use the corrected \\(AIC\\), noted as \\(AIC_c\\), to calculate how well a model explains the data.\n\\[\\text{AIC}_c = - 2ln(\\text{Model Likelihood}) + 2k + \\frac{2K(K+1)}{n-K-1}\\] where \\(n\\) is the number of data observations. Then, to calculate the weights we need to see how much each sub-model deviates from the best model. For this deviation we calculate, for sub-model \\(i\\):\n\\[\\Delta \\text{AIC}_{c,i} = \\text{AIC}_{c,\\text{best}} - \\text{AIC}_{c,i}\\] The weight of sub-model \\(i\\) is:\n\\[w_i = \\frac{\\text{exp}(-\\Delta \\text{AIC}_{c,i} / 2)}{\\sum_{r=1}^{R} \\text{exp}(-\\Delta \\text{AIC}_{c,r} / 2)} \\] And \\(R\\) is the number of submodels being examined.\nWe are now ready to calculate the weighted average of any parameter of interest in the full model, \\(\\theta\\):\n\\[\\hat{\\bar{\\theta}} = \\sum_{r=1}^{R} w_r \\hat{\\theta}_{r}\\] Here, \\(\\hat{\\bar{\\theta}}\\) is the model-averaged estimate of parameter \\(\\theta\\), \\(w_r\\) is the weight of sub-model \\(r\\), and \\(\\hat{\\theta}_r\\) is the parameter estimate derived from sub-model \\(r\\).\nWe can also calculate the new averaged uncertainty in the parameter estimate:\n\\[\\hat{\\text{var}}(\\hat{\\bar{\\theta}}) =  \\sum_{r=1}^{R} w_r \\left( \\hat{\\text{var}}(\\hat{\\theta})_r + (\\hat{\\theta} - \\hat{\\bar{\\theta}})^2 \\right) \\] Here, \\(\\hat{\\text{var}}(\\hat{\\bar{\\theta}})\\) is the standard error of the averaged model parameter, whereas \\(\\hat{\\text{var}}(\\hat{\\theta})_r\\) is the standard error of model parameter \\(\\theta\\) estimated from sub-model \\(r\\).\n\n\n7.4.2 Manual calculation\nLet’s see if we can manually calculate all of this from a less complex example model. Imagine our full model is a model that only has two input variables. We’ll use our simulated data set from above. (Of course, we know this is a poor model, but we’re just doing a case-study here.)\n\n# Full model:\nfull_mod = lm(y~1+X1+X2, data = my_df)\n\nIf this full model has two inputs, then the number of sub-models is \\(2^2 = 4\\), which iteratively drop one or both input variables. Now, run each sub-model. I know, this is tedious.\n\nsub_m2 = update(full_mod, .~. -X2)\nsub_m3 = update(full_mod, .~. -X1)\nsub_m4 = update(full_mod, .~. -X2-X1) # Intercept only\n\n# Store all models in a list for easy looping later:\nmodel_list = list(\n    full_mod, sub_m2, sub_m3, sub_m4\n)\n\n# how many models?\nn_mod = 4\n\nTo get the model-averaged slopes of inputs \\(x_1\\) and \\(x_2\\), we’ll need to calculate \\(AIC_c\\) values and model weights. We’ll store calculations in arrays as much as possible, so we can loop through.\nLet’s start with \\(AIC_c\\). Fortunately, there’s a built-in function for this in the MuMIn package, but we’ll do one manually first.\n\n# Extract neg-log-likelihood from full model:\nnll_full = -1*logLik(full_mod)\n# This is in a weird format, so we'll convert:\nnll_full = as.numeric(nll_full)\nk_full = 4 # two slopes + 1 intercept + residual sigma\n\n# Calculate AIC_c\naic_c_full = 2*nll_full + 2*k_full + (2*k_full*(k_full + 1))/(n - k_full - 1)\naic_c_full\n\n[1] 251.5063\n\n# Check with built-in\nlibrary(MuMIn)\nAICc(full_mod)\n\n[1] 251.5063\n\n# Now calculate all:\nAICc_vec = NULL\nfor(i in 1:n_mod){\n    AICc_vec[i] = AICc(model_list[[i]])\n}\nAICc_vec\n\n[1] 251.5063 252.3124 251.2157 253.8381\n\n\nWe can see that the ‘best’ model, according to \\(AIC_c\\) is the sub_m3, which includes the intercept and only input \\(x_2\\). This makes sense, because we know that the slope of \\(x_1\\) was simulated as zero.\nLet’s now calculate the \\(\\Delta \\text{AIC}_c\\).\n\n# Best AICc\nAICc_best = min(AICc_vec)\n\n#\\Delta AIC_c\nDelta_AICc_vec = AICc_vec - AICc_best\nDelta_AICc_vec\n\n[1] 0.290611 1.096742 0.000000 2.622409\n\n\nNow we can calculate the model weights (i.e., the value representing how “good” each model is, relative to the best model).\n\n# Calculate the denominator of the weight calculation\nweight_denom = 0\nfor(i in 1:n_mod){\n    weight_denom = \n        weight_denom +\n        exp( -Delta_AICc_vec[i] / 2 )\n}\nweight_denom\n\n[1] 2.712144\n\n# Now the individual weights:\nweight = NULL\nfor(i in 1:n_mod){\n    weight[i] = \n        exp( -Delta_AICc_vec[i] / 2) / weight_denom\n}\nweight\n\n[1] 0.31884668 0.21307516 0.36871201 0.09936614\n\n# Sum to 1?\nsum(weight)\n\n[1] 1\n\n\nWe can see the “better” models, based on \\(AIC_c\\) have higher weights, and the weight vector should add to 1.\nLet’s calculate the model-averaged slope estimate for input \\(x_2\\). To do this, we’ll first need to extract the estimate from each sub-model. This is a little tedious, because we need to know which coefficient refers to \\(x_2\\) in each sub-model object (or if the coefficient is absent and therefore equal to zero).\n\ncoef_x2 = NULL\ncoef_x2[1] = coef(model_list[[1]])[3]\ncoef_x2[2] = 0 # Absent from this sub-model\ncoef_x2[3] = coef(model_list[[3]])[2]\ncoef_x2[4] = 0 # Absent from this sub-model\ncoef_x2\n\n[1] 0.2975017 0.0000000 0.3649074 0.0000000\n\n# Averaged, based on model weight:\navg_coef_x2 = 0\nfor(i in 1:n_mod){\n    avg_coef_x2 = \n        avg_coef_x2 +\n        weight[i] * coef_x2[i]\n}\n\navg_coef_x2\n\n[1] 0.2294032\n\n\nWe can see the model-averaged slope estimate for input \\(x_2\\) is slightly less than the estimate from the full model.\n\nsummary(full_mod)\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9147 -3.6749  0.6359  3.4050 10.5608 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.58750    1.78926   7.035 2.55e-08 ***\nX1           0.14204    0.09854   1.441   0.1579    \nX2           0.29750    0.16725   1.779   0.0835 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.205 on 37 degrees of freedom\nMultiple R-squared:  0.1637,    Adjusted R-squared:  0.1185 \nF-statistic: 3.621 on 2 and 37 DF,  p-value: 0.03662\n\ncoef(full_mod)[3]\n\n       X2 \n0.2975017 \n\n\nI’ll leave calculating the model-averaged standard error of the slopes as an exercise for you as a student.\nAs I mentioned above, fortunately someone created a package to do this model averaging for us and remove a lot of the tedium.\nFirst, run all sub-models using the MuMIn::dredge() function.\n\n# Required for MuMIn::dredge functionality\noptions(na.action = \"na.fail\")\n\n# Fit all sub-models:\ndredge_test = dredge(full_mod)\n\nFixed term is \"(Intercept)\"\n\ndredge_test\n\nGlobal model call: lm(formula = y ~ 1 + X1 + X2, data = my_df)\n---\nModel selection table \n  (Intrc)    X1     X2 df   logLik  AICc delta weight\n3   12.71       0.3649  3 -122.275 251.2  0.00  0.369\n4   12.59 0.142 0.2975  4 -121.182 251.5  0.29  0.319\n2   15.26 0.191         3 -122.823 252.3  1.10  0.213\n1   16.31               2 -124.757 253.8  2.62  0.099\nModels ranked by AICc(x) \n\n\nSee how this output has run all sub-models, calculated the likelihoods, the \\(AIC_c\\), the \\(\\Delta AIC_c\\), and the model weights.\nNow, we can average all of the models.\n\n# Average the models:\ntest_average = model.avg(dredge_test, fit = TRUE)\nsummary(test_average)\n\n\nCall:\nmodel.avg(object = get.models(object = dredge_test, subset = NA))\n\nComponent model call: \nlm(formula = y ~ <4 unique rhs>, data = my_df)\n\nComponent models: \n       df  logLik   AICc delta weight\n2       3 -122.27 251.22  0.00   0.37\n12      4 -121.18 251.51  0.29   0.32\n1       3 -122.82 252.31  1.10   0.21\n(Null)  2 -124.76 253.84  2.62   0.10\n\nTerm codes: \nX1 X2 \n 1  2 \n\nModel-averaged coefficients:  \n(full average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept)  13.5710     2.1113      2.1512   6.308   <2e-16 ***\nX2            0.2294     0.2083      0.2113   1.086    0.278    \nX1            0.0860     0.1092      0.1108   0.776    0.438    \n \n(conditional average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept)  13.5710     2.1113      2.1512   6.308   <2e-16 ***\nX2            0.3336     0.1683      0.1737   1.921   0.0547 .  \nX1            0.1617     0.1009      0.1041   1.553   0.1205    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis summary() statement shows the model-averaged values of slopes of \\(x_1\\) and \\(x_2\\), and the intercept. We care about the “full average”. You can see the averaged estimate of the slope for \\(x_2\\) matches our manual calculation. For emphasis:\n\ntest_average$coefficients[1,2];\n\n[1] 0.2294032\n\navg_coef_x2\n\n[1] 0.2294032\n\n\n\n\n7.4.3 Back to more complex model\nOk, but our full model had four input variables, which means the number of sub-models is \\(4^2 = 16\\). Let’s not do that manually, but instead use the MuMIn::model.avg() function.\n\n# Reminder, m1 was our full model:\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3257 -1.4053 -0.4331  1.3299  4.3178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.41757    1.82632   1.871  0.06968 .  \nX1           0.03245    0.03810   0.852  0.40016    \nX2          -0.26989    0.07297  -3.698  0.00074 ***\nX3          -0.01823    0.03267  -0.558  0.58050    \nX4           1.68543    0.11083  15.207  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.94 on 35 degrees of freedom\nMultiple R-squared:  0.8901,    Adjusted R-squared:  0.8775 \nF-statistic: 70.86 on 4 and 35 DF,  p-value: 2.741e-16\n\n# Fit all sub-models:\ndredge_m1 = dredge(m1)\n\nFixed term is \"(Intercept)\"\n\n# Average the models:\nm1_average = model.avg(dredge_m1, fit = TRUE)\nsummary(m1_average)\n\n\nCall:\nmodel.avg(object = get.models(object = dredge_m1, subset = NA))\n\nComponent model call: \nlm(formula = y ~ <16 unique rhs>, data = my_df)\n\nComponent models: \n       df  logLik   AICc delta weight\n24      4  -81.10 171.34  0.00   0.56\n124     5  -80.77 173.31  1.97   0.21\n234     5  -81.01 173.78  2.43   0.17\n1234    6  -80.60 175.74  4.39   0.06\n4       3  -87.23 181.13  9.79   0.00\n14      4  -87.20 183.55 12.20   0.00\n34      4  -87.23 183.60 12.26   0.00\n134     5  -87.19 186.15 14.81   0.00\n2       3 -122.27 251.22 79.87   0.00\n12      4 -121.18 251.51 80.16   0.00\n1       3 -122.82 252.31 80.97   0.00\n23      4 -122.21 253.55 82.21   0.00\n(Null)  2 -124.76 253.84 82.49   0.00\n123     5 -121.18 254.12 82.78   0.00\n13      4 -122.82 254.78 83.44   0.00\n3       3 -124.73 256.12 84.77   0.00\n\nTerm codes: \nX1 X2 X3 X4 \n 1  2  3  4 \n\nModel-averaged coefficients:  \n(full average) \n             Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept)  2.642223   1.219137    1.258700   2.099 0.035802 *  \nX2          -0.258803   0.074396    0.076727   3.373 0.000743 ***\nX4           1.693799   0.109648    0.113293  14.951  < 2e-16 ***\nX1           0.007999   0.023509    0.024078   0.332 0.739716    \nX3          -0.003320   0.016617    0.017119   0.194 0.846219    \n \n(conditional average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept)  2.64222    1.21914     1.25870   2.099 0.035802 *  \nX2          -0.26062    0.07141     0.07385   3.529 0.000417 ***\nX4           1.69380    0.10965     0.11329  14.951  < 2e-16 ***\nX1           0.02940    0.03744     0.03875   0.759 0.448057    \nX3          -0.01452    0.03232     0.03345   0.434 0.664272    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice how we see the model with inputs \\(x_2\\) and \\(x_4\\) is the best, based on \\(AIC_c\\) (note this is the model labeled as 24 meaning it inclues inputs 2 and 4).\nWe can also plot the model parameter estimates with their confidence intervals:\n\n# Plot the coefficient estimates (from the averaged model)\nplot(m1_average)\n\n\n\n\nFinally, we can use the predict() function as we have before to visualize the effect of each input variable on the outcome. Here, we will show the independent, model-averaged effect of \\(x_2\\), when all other input variables are held at their average values. Then, we’ll do the same for \\(x_4\\).\n\n# Predict from the average model:\n# How does y change as a function of x2, while \n# other inputs held at their average?\nnew_df = data.frame(\n    X1 = rep(mean(my_df$X1), 100),\n    X2 = seq(0, 19, length.out = 100),\n    X3 = rep(mean(my_df$X3), 100),\n    X4 = rep(mean(my_df$X4), 100)\n)\n\npred_m1_avg_x2 = \n    predict(m1_average,\n            newdata = new_df,\n            se.fit = TRUE)\n\nplot(my_df$y ~ my_df$X2,\n     xlab = \"input x2\", ylab = \"y\", pch = 19)\nlines(pred_m1_avg_x2$fit ~ new_df$X2)\nlines(pred_m1_avg_x2$fit-2*pred_m1_avg_x2$se.fit ~ new_df$X2, lty = 2)\nlines(pred_m1_avg_x2$fit+2*pred_m1_avg_x2$se.fit ~ new_df$X2, lty = 2)\n\n\n\n# Predict from the average model:\n# How does y change as a function of x4, while \n# other inputs held at their average?\nnew_df = data.frame(\n    X1 = rep(mean(my_df$X1), 100),\n    X2 = rep(mean(my_df$X2), 100),\n    X3 = rep(mean(my_df$X3), 100),\n    X4 = seq(0, 19, length.out = 100)\n)\n\npred_m1_avg_x4 = \n    predict(m1_average,\n            newdata = new_df,\n            se.fit = TRUE)\n\nplot(my_df$y ~ my_df$X4,\n     xlab = \"input x4\", ylab = \"y\", pch = 19)\nlines(pred_m1_avg_x4$fit ~ new_df$X4)\nlines(pred_m1_avg_x4$fit-2*pred_m1_avg_x4$se.fit ~ new_df$X4, lty = 2)\nlines(pred_m1_avg_x4$fit+2*pred_m1_avg_x4$se.fit ~ new_df$X4, lty = 2)\n\n\n\n\nAnother, perhaps simpler way to vizualize how well a model matches the data is to plot the model predictions of the data versus the observed data. We can even compare this to the non-averaged model.\n\nraw_predict_avg = predict(m1_average)\nraw_predict_nonavg = predict(m1)\n\nplot(my_df$y ~ raw_predict_avg,\n     xlab = \"Model Prediction\", ylab = \"Data, y\", pch = 19, col = \"red\")\npoints(my_df$y ~ raw_predict_nonavg, pch = 19, col = \"black\")\n# 1-to-1 line\nabline(a = 0, b = 1)\n\n\n\n\nIt is hard to see, and likely not significant in this case, but the red points (model-averaged) tend to be closer to the 1:1 line, meaning the averaged model makes slightly better predictions of the observed data."
  }
]